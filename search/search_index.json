{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HANNAH - Hardware Accelerator and Neural network searcH Getting Started Note For more information, visit the documentation . Installing dependencies Dependencies and virtual environments are managed using poetry . python (>=3.9 <3.12) and development headers libsndfile and development headers libsox and development headers a blas implementation and development headers Ubuntu 20.04+ Install dependencies: sudo apt update sudo apt -y install python3-dev libblas-dev liblapack-dev libsndfile1-dev libsox-dev git-lfs Centos / RHEL / Scientific Linux: 7+ Install dependencies: sudo yum install portaudio-devel libsndfile1-devel libsox-devel -y Install a python 3.9 or python 3.10 version using pyenv . Install poetry curl -sSL https://install.python-poetry.org/ | python3 For alternative installation methods see: https://python-poetry.org/docs/#installation Caution : this usually installs poetry to ~/.local/bin if this folder is not in your path you might need to run poetry as: ~/.local/bin/poetry Software installation In the root directory of the project run: git submodule update --init --recursive poetry install -E vision This creates a virtual environment under ~/.cache/pypoetry/virtualenvs. The environment can be activated using: poetry shell Optional Dependencies We support installation of optional dependencies using poetry's -E commandline flag We currently have the following optional dependencies: Vision-Models Vision models require additional dependencies, these can be installed using: poetry install -E vision These dependencies include kornia and albumentations for image augmentations and timm (torch image models) for baseline neural network models. Onnx-Runtime Backend poetry install -E onnxrt-backend Backend support for running models on onnx-runtime. Tflite-Backend poetry install -E onnx-tf Onnx based conversion of trained models to tensorflow/tensorflow-lite for external inference backends. DVC based experiment management (experimental) poetry install -E dvc This installs dvc based model, data and experiment management. DVC support is highly experimental and subject to change. Installation Tips 1.) venv location poetry installs the dependencies to a virtual environment in ~/.cache/pypoetry/virtualenvs You can change the location of this directory using: poetry config virtualenvs.path <desired virtual environment path> Or move it to a subdirectory of the project directory using: poetry config virtualenvs.in-project true Installing the datasets Datasets are downloaded automatically to the datasets data folder by default this is a subfolder of the dataset's data folder. For the VAD Dataset the following Flag is needed to Download/Override the Dataset dataset.override=True Training - Keyword Spotting Training is invoked by hannah-train If available the first GPU of the system will be used by default. Selecting another GPU is possible using the argument trainer. devices=[number] e.g. for GPU 2 use: hannah-train trainer.devices=[2] Trained models are saved under trained_models/<experiment_id>/<model_name> . Training - VAD Training of VAD is invoked by hannah-train dataset=vad model.n_labels=2 Training of VAD_Extended is invoked by hannah-train dataset=vad_extended model.n_labels=2 VAD dataset variants Selection of other Voice Dataset use dataset.variants=\"[UWNU, de, en, it, fr, es]\" hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU, de, en, it, fr, es]\" Selection of other Noise Datasets use dataset.noise_dataset=\"[TUT]\" hannah - train dataset = vad model . n_labels = 2 dataset . noise_dataset = \"[TUT]\" Selection of dataset Split use dataset.data_split=\"vad_balanced\" hannah-train dataset=vad model.n_labels=2 dataset.data_split=\"vad_balanced\" Create Vad_small Dataset hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU]\" dataset . noise_dataset = \"[TUT]\" dataset . data_split = \"vad_balanced\" Create VAD_big Dataset hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU, en, de, fr, es, it]\" dataset . noise_dataset = \"[TUT, FSD50K]\" dataset . data_split = \"vad_balanced\" Training - PAMAP2 Training of PAMAP2 human activity detection dataset is invoked by: hannah-train -cn config_activity Training - Emergency Siren Dataset Training of emergency siren detection dataset is invoked by: hannah-train -cn config_siren_detection Parallel Launchers To launch multiple optimizations in parallel you can use a hydra launcher. Joblib launcher is installed by default: hannah-train --multirun hydra/sweeper=optuna hydra/launcher=joblib optimizer.lr='interval(0.0001,0.1)' optimizer.weight_decay='interval(0, 0.1)' hydra.launcher.n_jobs=5 Launches optimizer hyperparameter optimization with 5 parallel jobs. Early stopping To stop training early when a validation metric does not improve, you can use lightning's early stopping callback: hannah-train early_stopping=default Showing graphical results All experiments are logged to tensorboard: To visualize the results use: tensorboard --logdir trained_models or a subdirectory of trained models if only one experiment or model is of interest. Pre commit hooks This project uses precommit hooks for auto formatting and static code analysis. To enable precommit hooks run the following command in a poetry shell . pre-commit install Try to follow pep8 naming conventions and the rest of pep8 to the best of your abilities. Automatic Mirroring This project automatically mirrors its main branch and all branches prefixed with pub/ to its public github repository. These branches are configured as protected branches by default.","title":"Home"},{"location":"#hannah-hardware-accelerator-and-neural-network-search","text":"","title":"HANNAH - Hardware Accelerator and Neural network searcH"},{"location":"#getting-started","text":"Note For more information, visit the documentation .","title":"Getting Started"},{"location":"#installing-dependencies","text":"Dependencies and virtual environments are managed using poetry . python (>=3.9 <3.12) and development headers libsndfile and development headers libsox and development headers a blas implementation and development headers","title":"Installing dependencies"},{"location":"#ubuntu-2004","text":"Install dependencies: sudo apt update sudo apt -y install python3-dev libblas-dev liblapack-dev libsndfile1-dev libsox-dev git-lfs","title":"Ubuntu 20.04+"},{"location":"#centos-rhel-scientific-linux-7","text":"Install dependencies: sudo yum install portaudio-devel libsndfile1-devel libsox-devel -y Install a python 3.9 or python 3.10 version using pyenv .","title":"Centos / RHEL / Scientific Linux: 7+"},{"location":"#install-poetry","text":"curl -sSL https://install.python-poetry.org/ | python3 For alternative installation methods see: https://python-poetry.org/docs/#installation Caution : this usually installs poetry to ~/.local/bin if this folder is not in your path you might need to run poetry as: ~/.local/bin/poetry","title":"Install poetry"},{"location":"#software-installation","text":"In the root directory of the project run: git submodule update --init --recursive poetry install -E vision This creates a virtual environment under ~/.cache/pypoetry/virtualenvs. The environment can be activated using: poetry shell","title":"Software installation"},{"location":"#optional-dependencies","text":"We support installation of optional dependencies using poetry's -E commandline flag We currently have the following optional dependencies:","title":"Optional Dependencies"},{"location":"#vision-models","text":"Vision models require additional dependencies, these can be installed using: poetry install -E vision These dependencies include kornia and albumentations for image augmentations and timm (torch image models) for baseline neural network models.","title":"Vision-Models"},{"location":"#onnx-runtime-backend","text":"poetry install -E onnxrt-backend Backend support for running models on onnx-runtime.","title":"Onnx-Runtime Backend"},{"location":"#tflite-backend","text":"poetry install -E onnx-tf Onnx based conversion of trained models to tensorflow/tensorflow-lite for external inference backends.","title":"Tflite-Backend"},{"location":"#dvc-based-experiment-management-experimental","text":"poetry install -E dvc This installs dvc based model, data and experiment management. DVC support is highly experimental and subject to change.","title":"DVC based experiment management (experimental)"},{"location":"#installation-tips","text":"1.) venv location poetry installs the dependencies to a virtual environment in ~/.cache/pypoetry/virtualenvs You can change the location of this directory using: poetry config virtualenvs.path <desired virtual environment path> Or move it to a subdirectory of the project directory using: poetry config virtualenvs.in-project true","title":"Installation Tips"},{"location":"#installing-the-datasets","text":"Datasets are downloaded automatically to the datasets data folder by default this is a subfolder of the dataset's data folder. For the VAD Dataset the following Flag is needed to Download/Override the Dataset dataset.override=True","title":"Installing the datasets"},{"location":"#training-keyword-spotting","text":"Training is invoked by hannah-train If available the first GPU of the system will be used by default. Selecting another GPU is possible using the argument trainer. devices=[number] e.g. for GPU 2 use: hannah-train trainer.devices=[2] Trained models are saved under trained_models/<experiment_id>/<model_name> .","title":"Training - Keyword Spotting"},{"location":"#training-vad","text":"Training of VAD is invoked by hannah-train dataset=vad model.n_labels=2 Training of VAD_Extended is invoked by hannah-train dataset=vad_extended model.n_labels=2","title":"Training - VAD"},{"location":"#vad-dataset-variants","text":"Selection of other Voice Dataset use dataset.variants=\"[UWNU, de, en, it, fr, es]\" hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU, de, en, it, fr, es]\" Selection of other Noise Datasets use dataset.noise_dataset=\"[TUT]\" hannah - train dataset = vad model . n_labels = 2 dataset . noise_dataset = \"[TUT]\" Selection of dataset Split use dataset.data_split=\"vad_balanced\" hannah-train dataset=vad model.n_labels=2 dataset.data_split=\"vad_balanced\" Create Vad_small Dataset hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU]\" dataset . noise_dataset = \"[TUT]\" dataset . data_split = \"vad_balanced\" Create VAD_big Dataset hannah - train dataset = vad model . n_labels = 2 dataset . variants = \"[UWNU, en, de, fr, es, it]\" dataset . noise_dataset = \"[TUT, FSD50K]\" dataset . data_split = \"vad_balanced\"","title":"VAD dataset variants"},{"location":"#training-pamap2","text":"Training of PAMAP2 human activity detection dataset is invoked by: hannah-train -cn config_activity","title":"Training - PAMAP2"},{"location":"#training-emergency-siren-dataset","text":"Training of emergency siren detection dataset is invoked by: hannah-train -cn config_siren_detection","title":"Training - Emergency Siren Dataset"},{"location":"#parallel-launchers","text":"To launch multiple optimizations in parallel you can use a hydra launcher. Joblib launcher is installed by default: hannah-train --multirun hydra/sweeper=optuna hydra/launcher=joblib optimizer.lr='interval(0.0001,0.1)' optimizer.weight_decay='interval(0, 0.1)' hydra.launcher.n_jobs=5 Launches optimizer hyperparameter optimization with 5 parallel jobs.","title":"Parallel Launchers"},{"location":"#early-stopping","text":"To stop training early when a validation metric does not improve, you can use lightning's early stopping callback: hannah-train early_stopping=default","title":"Early stopping"},{"location":"#showing-graphical-results","text":"All experiments are logged to tensorboard: To visualize the results use: tensorboard --logdir trained_models or a subdirectory of trained models if only one experiment or model is of interest.","title":"Showing graphical results"},{"location":"#pre-commit-hooks","text":"This project uses precommit hooks for auto formatting and static code analysis. To enable precommit hooks run the following command in a poetry shell . pre-commit install Try to follow pep8 naming conventions and the rest of pep8 to the best of your abilities.","title":"Pre commit hooks"},{"location":"#automatic-mirroring","text":"This project automatically mirrors its main branch and all branches prefixed with pub/ to its public github repository. These branches are configured as protected branches by default.","title":"Automatic Mirroring"},{"location":"experiments/","text":"Experiment Mangement It is common to create a new directory for each group of experiments, usually these are group around a specific publication goal or project. Hydra configuration options are taken from one of the following possibilities. A local config.yml taken from the directory in which the command is run. Configuration group overrides from any subdirectory of the current working directory named after a configuration group Overrides from a special configuration group usually called experiments for an example on experiment management have a look at experiments/cifar10 . It has the following directory structure: . \u251c\u2500\u2500 augmentation \u2502 \u2514\u2500\u2500 cifar_augment.yaml \u251c\u2500\u2500 config.yaml \u251c\u2500\u2500 datasets \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 sweep_lr.yaml \u2502 \u2514\u2500\u2500 sweep_models.yaml \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 train_slurm.sh \u2514\u2500\u2500 trained_models \u251c\u2500\u2500 sweep_lr | \u251c\u2500\u2500 0.0001 | \u251c\u2500\u2500 ... \u2514\u2500\u2500 sweep_models \u251c\u2500\u2500 multirun.yaml \u2514\u2500\u2500 resnet18 The main configuration is found in config.yaml . defaults : - base_config - override dataset : cifar10 # Dataset configuration name - override features : identity # Feature extractor configuration name (use identity for vision datasets) - override model : timm_resnet18 #timm_mobilenetv3_small_100 # Neural network name (for now timm_resnet50 or timm_efficientnet_lite1) - override scheduler : 1cycle # learning rate scheduler config name - override optimizer : sgd # Optimizer config name - override normalizer : null # Feature normalizer (used for quantized neural networks) - override module : image_classifier # Lightning module config for the training loop (image classifier for image classification tasks) - override augmentation : cifar_augment - _self_ monitor : metric : val_f1_micro direction : maximize module : batch_size : 64 trainer : max_epochs : 50 scheduler : max_lr : 0.1 The configuration is composed from the standard presets in base_config the defaults are then using different config group presets, using the - override <groupname>: <presetname> syntax. Most of the presets are taken from the package wide configuration in hannah/conf . The configuration for data augmentation is defined in: augmentation/cifar_augment.yaml : batch_augment : pipeline : null transforms : RandomVerticalFlip : p : 0.5 RandomCrop : size : [ 32 , 32 ] padding : 4 This file specifies the presets for data augmentation shared among the experiments in this group of experiments. The experiments are then defined in for example experiment/sweep_lr.yaml and experiment/sweep_model.yaml . These experiments can be started with: hannah-train +experiment=<experiment_name> . Please note the + in front of the commandline argument. This is needed as experiment is not part of default configuration hierarchy. As an example have a look at experiment/sweep_lr.yaml . # @package _global_ experiment_id : sweep_lr hydra : mode : MULTIRUN sweep : subdir : lr=${scheduler.max_lr} sweeper : params : scheduler.max_lr : 0.0001,0.001,0.01,0.1 Experiments must start with # @package _global_ this means that overrides defined here change the global configuration and not some values under experiment . The next line sets the experiment_id this is used to identify the experiment and set a different subfolder for the output artifacts (normally: trained_models/<experiment_id> ). The final part of the configuration then configures a sweep over multiple parameters. In this case we configure a sweep over the max_lr parameter of the used 1cycle learning rate scheduler, and configure the output directory to contain a separate subdirector lr=0.0001 , lr=0.001 for each setting of the max_lr parameter. The final outputs can then be found in trained_models/sweep_lr/lr\\=0.0001/ and so on.","title":"Experiment Management"},{"location":"experiments/#experiment-mangement","text":"It is common to create a new directory for each group of experiments, usually these are group around a specific publication goal or project. Hydra configuration options are taken from one of the following possibilities. A local config.yml taken from the directory in which the command is run. Configuration group overrides from any subdirectory of the current working directory named after a configuration group Overrides from a special configuration group usually called experiments for an example on experiment management have a look at experiments/cifar10 . It has the following directory structure: . \u251c\u2500\u2500 augmentation \u2502 \u2514\u2500\u2500 cifar_augment.yaml \u251c\u2500\u2500 config.yaml \u251c\u2500\u2500 datasets \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 experiment \u2502 \u251c\u2500\u2500 sweep_lr.yaml \u2502 \u2514\u2500\u2500 sweep_models.yaml \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 train_slurm.sh \u2514\u2500\u2500 trained_models \u251c\u2500\u2500 sweep_lr | \u251c\u2500\u2500 0.0001 | \u251c\u2500\u2500 ... \u2514\u2500\u2500 sweep_models \u251c\u2500\u2500 multirun.yaml \u2514\u2500\u2500 resnet18 The main configuration is found in config.yaml . defaults : - base_config - override dataset : cifar10 # Dataset configuration name - override features : identity # Feature extractor configuration name (use identity for vision datasets) - override model : timm_resnet18 #timm_mobilenetv3_small_100 # Neural network name (for now timm_resnet50 or timm_efficientnet_lite1) - override scheduler : 1cycle # learning rate scheduler config name - override optimizer : sgd # Optimizer config name - override normalizer : null # Feature normalizer (used for quantized neural networks) - override module : image_classifier # Lightning module config for the training loop (image classifier for image classification tasks) - override augmentation : cifar_augment - _self_ monitor : metric : val_f1_micro direction : maximize module : batch_size : 64 trainer : max_epochs : 50 scheduler : max_lr : 0.1 The configuration is composed from the standard presets in base_config the defaults are then using different config group presets, using the - override <groupname>: <presetname> syntax. Most of the presets are taken from the package wide configuration in hannah/conf . The configuration for data augmentation is defined in: augmentation/cifar_augment.yaml : batch_augment : pipeline : null transforms : RandomVerticalFlip : p : 0.5 RandomCrop : size : [ 32 , 32 ] padding : 4 This file specifies the presets for data augmentation shared among the experiments in this group of experiments. The experiments are then defined in for example experiment/sweep_lr.yaml and experiment/sweep_model.yaml . These experiments can be started with: hannah-train +experiment=<experiment_name> . Please note the + in front of the commandline argument. This is needed as experiment is not part of default configuration hierarchy. As an example have a look at experiment/sweep_lr.yaml . # @package _global_ experiment_id : sweep_lr hydra : mode : MULTIRUN sweep : subdir : lr=${scheduler.max_lr} sweeper : params : scheduler.max_lr : 0.0001,0.001,0.01,0.1 Experiments must start with # @package _global_ this means that overrides defined here change the global configuration and not some values under experiment . The next line sets the experiment_id this is used to identify the experiment and set a different subfolder for the output artifacts (normally: trained_models/<experiment_id> ). The final part of the configuration then configures a sweep over multiple parameters. In this case we configure a sweep over the max_lr parameter of the used 1cycle learning rate scheduler, and configure the output directory to contain a separate subdirector lr=0.0001 , lr=0.001 for each setting of the max_lr parameter. The final outputs can then be found in trained_models/sweep_lr/lr\\=0.0001/ and so on.","title":"Experiment Mangement"},{"location":"fallacies/","text":"Fallacies and Pitfalls Changing OneCycleLR learningrate By default the OneCycleLR learning rate scheduler will ignore the setting for \u00f2ptimizer.lr and instead will use the setting scheduler.max_lr to calculate each steps learning rate. To avoid problems when sweeping the the learning rate, in hannah the default setting for scheduler.max_lr is using the same as optimizer.lr but this can be overridden in project specific configuration files. No SecretStorage or DBUS running During poetry install the system will try to unlock secret storage / keyring. If this fails you will see a Error Message like: [org.freedesktop.DBus.Error.UnknownMethod] ('Object does not exist on path \"/org/freedesktop/secrets/collection/login\"') You can disable this error message by one of the following means: Install and Configure a local DBus and Keyring support. This might work. Setting the default backend for keyring: export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring Disabling keyring for the whole system: keyring --disable -b keyring.backends.SecretService.Keyring","title":"Fallacies and Pitfalls"},{"location":"fallacies/#fallacies-and-pitfalls","text":"","title":"Fallacies and Pitfalls"},{"location":"fallacies/#changing-onecyclelr-learningrate","text":"By default the OneCycleLR learning rate scheduler will ignore the setting for \u00f2ptimizer.lr and instead will use the setting scheduler.max_lr to calculate each steps learning rate. To avoid problems when sweeping the the learning rate, in hannah the default setting for scheduler.max_lr is using the same as optimizer.lr but this can be overridden in project specific configuration files.","title":"Changing OneCycleLR learningrate"},{"location":"fallacies/#no-secretstorage-or-dbus-running","text":"During poetry install the system will try to unlock secret storage / keyring. If this fails you will see a Error Message like: [org.freedesktop.DBus.Error.UnknownMethod] ('Object does not exist on path \"/org/freedesktop/secrets/collection/login\"') You can disable this error message by one of the following means: Install and Configure a local DBus and Keyring support. This might work. Setting the default backend for keyring: export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring Disabling keyring for the whole system: keyring --disable -b keyring.backends.SecretService.Keyring","title":"No SecretStorage or DBUS running"},{"location":"publications/","text":"Related Publications Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs. by Julia Werner, Christoph Gerum, Moritz Reiber, J\u00f6rg Nick, Oliver Bringmann In 2023 14th International Workshop on Machine Learning in Medical Imaging (MLMI 2023) Hardware Accelerator and Neural Network Co-Optimization for Ultra-Low-Power Audio Processing Devices by Christoph Gerum, Adrian Frischknecht, Paul Palomero Bernardo, Tobias Hald, Konstantin L\u00fcbeck, and Oliver Bringmann In 2022 25th Euromicro Conference on Digital System Design (DSD), pages 1-8, 2022. Keywords: Machine Learning, Neural Networks, AutoML, Neural Architecture Search Behavior of Keyword Spotting Networks Under Noisy Conditions by Anwesh Mohanty, Adrian Frischknecht, Christoph Gerum, and Oliver Bringmann In International Conference on Artificial Neural Networks (ICANN), 2021. Keywords: Keyword spotting, High noise conditions, Adaptive batch normalization, Sinc convolution network, Temporal convolution ResNet. Dynamic Range and Complexity Optimization of Mixed-Signal Machine Learning Systems by Naci Pekcokguler, Dominique Morche, Adrian Frischknecht, Christoph Gerum, Andreas Burg, and Catherine Dehollain In 2021 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1-5, 2021. Keywords: keyword spotting, analog feature extraction, machine learning classi\ufb01er, dynamic range reduction, system complexity optimization Automated HW/SW Co-Design for Edge AI: State, Challenges and Steps Ahead by Oliver Bringmann, Wolfgang Ecker, Ingo Feldner, Adrian Frischknecht, Christoph Gerum, Timo H\u00e4m\u00e4l\u00e4inen, Muhammad Abdullah Hanif, Michael J. Klaiber, Daniel Mueller-Gritschneder, Paul Palomero Bernardo, Sebastian Prebeck, and Muhammad Shafique In Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis, pages 11\u201320. Association for Computing Machinery, 2021. UltraTrail: A Configurable Ultra-Low Power TC-ResNet AI Accelerator for Efficient Keyword Spotting by Paul Palomero Bernardo, Christoph Gerum, Adrian Frischknecht, Konstantin L\u00fcbeck, and Oliver Bringmann In 2020 IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), pages 1-12, 2020.","title":"Publications"},{"location":"publications/#related-publications","text":"Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs. by Julia Werner, Christoph Gerum, Moritz Reiber, J\u00f6rg Nick, Oliver Bringmann In 2023 14th International Workshop on Machine Learning in Medical Imaging (MLMI 2023) Hardware Accelerator and Neural Network Co-Optimization for Ultra-Low-Power Audio Processing Devices by Christoph Gerum, Adrian Frischknecht, Paul Palomero Bernardo, Tobias Hald, Konstantin L\u00fcbeck, and Oliver Bringmann In 2022 25th Euromicro Conference on Digital System Design (DSD), pages 1-8, 2022. Keywords: Machine Learning, Neural Networks, AutoML, Neural Architecture Search Behavior of Keyword Spotting Networks Under Noisy Conditions by Anwesh Mohanty, Adrian Frischknecht, Christoph Gerum, and Oliver Bringmann In International Conference on Artificial Neural Networks (ICANN), 2021. Keywords: Keyword spotting, High noise conditions, Adaptive batch normalization, Sinc convolution network, Temporal convolution ResNet. Dynamic Range and Complexity Optimization of Mixed-Signal Machine Learning Systems by Naci Pekcokguler, Dominique Morche, Adrian Frischknecht, Christoph Gerum, Andreas Burg, and Catherine Dehollain In 2021 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1-5, 2021. Keywords: keyword spotting, analog feature extraction, machine learning classi\ufb01er, dynamic range reduction, system complexity optimization Automated HW/SW Co-Design for Edge AI: State, Challenges and Steps Ahead by Oliver Bringmann, Wolfgang Ecker, Ingo Feldner, Adrian Frischknecht, Christoph Gerum, Timo H\u00e4m\u00e4l\u00e4inen, Muhammad Abdullah Hanif, Michael J. Klaiber, Daniel Mueller-Gritschneder, Paul Palomero Bernardo, Sebastian Prebeck, and Muhammad Shafique In Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis, pages 11\u201320. Association for Computing Machinery, 2021. UltraTrail: A Configurable Ultra-Low Power TC-ResNet AI Accelerator for Efficient Keyword Spotting by Paul Palomero Bernardo, Christoph Gerum, Adrian Frischknecht, Konstantin L\u00fcbeck, and Oliver Bringmann In 2020 IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), pages 1-12, 2020.","title":"Related Publications"},{"location":"compression/compression/clustering/","text":"Clustering k-means \\(k\\) -means clustering can be applied to the weights of all layers at the end of a training epoch. Each time, for a defined number of \\(k\\) clusters per layer the centroids are identified per layer. Next, each weight is replaced with the value of the closest cluster center. The zero is not clustered to preserve the amount of zeros induced by pruning. This results in a total of \\(k+1\\) distinct values. Additionally, at the end of the fitting process, clustering is applied. Afterwards, the weights are not further altered. After completing the training process, every value of each layer of the trained neural network should be equal to one cluster center or zero. Clustering (in combination with pruning and SVD-based compression) can be invoked by: hannah-train compression=full The default amount of clusters is 15 If clustering or SVD was applied, the last checkpoint of the model during training was used for validation and testing the neural network, instead of the best checkpoint as previously used. Fixing the amount of epochs with which the final model is trained enables better comparability of the models for different compression parameters and ensures that the compression is sufficiently integrated into the training process. In particular the choice of a final model with high accuracy, which has not yet been compressed in the training process (via SVD or clustering) is avoided by ensuring that the last checkpoint is used. Consequently, fixing the amount of training epochs for the final model illuminates the effect that the compression during the training has on the model. For applying clustering to quantized models, the batch norm fusion must be considered. This is not yet implemented.","title":"Clustering"},{"location":"compression/compression/clustering/#clustering","text":"","title":"Clustering"},{"location":"compression/compression/clustering/#k-means","text":"\\(k\\) -means clustering can be applied to the weights of all layers at the end of a training epoch. Each time, for a defined number of \\(k\\) clusters per layer the centroids are identified per layer. Next, each weight is replaced with the value of the closest cluster center. The zero is not clustered to preserve the amount of zeros induced by pruning. This results in a total of \\(k+1\\) distinct values. Additionally, at the end of the fitting process, clustering is applied. Afterwards, the weights are not further altered. After completing the training process, every value of each layer of the trained neural network should be equal to one cluster center or zero. Clustering (in combination with pruning and SVD-based compression) can be invoked by: hannah-train compression=full The default amount of clusters is 15 If clustering or SVD was applied, the last checkpoint of the model during training was used for validation and testing the neural network, instead of the best checkpoint as previously used. Fixing the amount of epochs with which the final model is trained enables better comparability of the models for different compression parameters and ensures that the compression is sufficiently integrated into the training process. In particular the choice of a final model with high accuracy, which has not yet been compressed in the training process (via SVD or clustering) is avoided by ensuring that the last checkpoint is used. Consequently, fixing the amount of training epochs for the final model illuminates the effect that the compression during the training has on the model. For applying clustering to quantized models, the batch norm fusion must be considered. This is not yet implemented.","title":"k-means"},{"location":"compression/compression/decomposition/","text":"Decomposition Singular value decomposition (SVD) In a CNN, the SVD can be applied to a fully connected layer, corresponding to a matrix \\(A\\) , resulting in the composition of two fully connected layers of lower dimension associated to the matrices \\(U'\\) and \\(\\Sigma' V^{T'}\\) . If \\(\\Sigma\\) is truncated to a rank \\(r=k\\) , dimensions change to \\(U'\\in \\mathbb{R}^{m \\times k}\\) , \\(\\Sigma' \\in \\mathbb{R}^{k \\times k}\\) and \\(V^{T'}\\in \\mathbb{R}^{k \\times n}\\) . Therefore, the SVD is applied to all weights of the linear layers, resulting in the decomposition of matrices. Subsequently, the weight matrices of the fully connected layers are compressed with a fixed rank \\(r\\) . Afterwards, the model is restructured, resulting in two smaller fully connected layers instead of one larger fully connected layer. SVD-based compression (in combination with pruning and clustering) can be invoked by: hannah-train compression=full The default rank_compression of SVD is 4 This has been implemented for TC-ResNets, which consist of one linear layer as the last layer of the network. Thus, the resulting network comprises of a sequential layer at the end, including one fully connected layer with \\(\\Sigma'*V^{T'}\\) and one fully connected layer with \\(U'\\) . If those partial matrices are multiplied, they ideally yield a close approximation to the original matrix \\(A\\) . Finally, fine-tuning is performed, by continuing the training process with this restructured, compressed network while the same optimizer and parameters are used as for the first training section. With this approach, the SVD-based compression is easily combined with QAT and pruning. Both methods are also applied to the restructured, compressed model during retraining. The layer's name can be adjusted to add other models.","title":"Tensor Decompositions"},{"location":"compression/compression/decomposition/#decomposition","text":"","title":"Decomposition"},{"location":"compression/compression/decomposition/#singular-value-decomposition-svd","text":"In a CNN, the SVD can be applied to a fully connected layer, corresponding to a matrix \\(A\\) , resulting in the composition of two fully connected layers of lower dimension associated to the matrices \\(U'\\) and \\(\\Sigma' V^{T'}\\) . If \\(\\Sigma\\) is truncated to a rank \\(r=k\\) , dimensions change to \\(U'\\in \\mathbb{R}^{m \\times k}\\) , \\(\\Sigma' \\in \\mathbb{R}^{k \\times k}\\) and \\(V^{T'}\\in \\mathbb{R}^{k \\times n}\\) . Therefore, the SVD is applied to all weights of the linear layers, resulting in the decomposition of matrices. Subsequently, the weight matrices of the fully connected layers are compressed with a fixed rank \\(r\\) . Afterwards, the model is restructured, resulting in two smaller fully connected layers instead of one larger fully connected layer. SVD-based compression (in combination with pruning and clustering) can be invoked by: hannah-train compression=full The default rank_compression of SVD is 4 This has been implemented for TC-ResNets, which consist of one linear layer as the last layer of the network. Thus, the resulting network comprises of a sequential layer at the end, including one fully connected layer with \\(\\Sigma'*V^{T'}\\) and one fully connected layer with \\(U'\\) . If those partial matrices are multiplied, they ideally yield a close approximation to the original matrix \\(A\\) . Finally, fine-tuning is performed, by continuing the training process with this restructured, compressed network while the same optimizer and parameters are used as for the first training section. With this approach, the SVD-based compression is easily combined with QAT and pruning. Both methods are also applied to the restructured, compressed model during retraining. The layer's name can be adjusted to add other models.","title":"Singular value decomposition (SVD)"},{"location":"compression/compression/knowledge_distillation/","text":"Pruning TBD","title":"Knowledge Distillation"},{"location":"compression/compression/knowledge_distillation/#pruning","text":"TBD","title":"Pruning"},{"location":"compression/compression/pruning/","text":"Pruning TBD","title":"Pruning"},{"location":"compression/compression/pruning/#pruning","text":"TBD","title":"Pruning"},{"location":"compression/compression/quantization/","text":"Quantization The training framework provides quantization aware training support for model factory based models. We support the following quantization schemes. Symmetric linear Quantization: Symmetric linear quantization applies the following transformation on a floating point value \\(v\\) : \\(\\mbox{quantized}(v)\\ =\\ \\mbox{clamp}(\\mbox{round}(v\\cdot2^{\\mbox{bits}-1}), 2^{-\\mbox{bits}-1}, 2^{\\mbox{bits}-1}-1)\\) Power of 2 Quantization with Zero: Power of 2 quantization applies the following transformation to a floating point value v: \\( \\mbox{quantized}(v)\\ =\\ \\begin{cases} \\mbox{sign}(v)\\ \\cdot\\ 2^{\\mbox{round}(\\log2(\\mbox{abs}(v)))} & \\text{if}\\ v\\ \\ge\\ 2^{2^{\\text{bits}}\\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\) Power of 2 Quantization can currently only be enabled for weights. Noisy Quantization As described in https://openreview.net/pdf?id=dV19Yyi1fS3 quantizing only a subset of model weights during training can improve accuracies of quantized networks compared to full quantization aware training. The probability of quantizing a weight can be given as parameter noise_prob in in the qconfig. Unfortunately this introduces an additional hyperparameter in the quantization space, good values for noise prob seem to be in the range of 0.7 to 0.9. Even when using noisy quantization eval and test are always run on fully quantized networks. Configuration Quantization can be configured as the qconfig attribute of factory based models: qconfig: target : speech_recognition.models.factory.qconfig.get_trax_qat_qconfig config: bw_b: 8 # Bitwidth for bias bw_w: 6 # Bitwidth for weights bw_f: 8 # Bitwidth for activations power_of_2: false # Use power of two quantization for weights noise_prob: 0.7 # Probability of quantizing a value during training Aditionally standard pytorch quantization aware training is supported by using a standard pytorch qconfig. qconfig : _target_ : torch . quantization . get_default_qat_qconfig backend : fbgemm In this case no quantization noise is supported.","title":"Quantization"},{"location":"compression/compression/quantization/#quantization","text":"The training framework provides quantization aware training support for model factory based models. We support the following quantization schemes. Symmetric linear Quantization: Symmetric linear quantization applies the following transformation on a floating point value \\(v\\) : \\(\\mbox{quantized}(v)\\ =\\ \\mbox{clamp}(\\mbox{round}(v\\cdot2^{\\mbox{bits}-1}), 2^{-\\mbox{bits}-1}, 2^{\\mbox{bits}-1}-1)\\) Power of 2 Quantization with Zero: Power of 2 quantization applies the following transformation to a floating point value v: \\( \\mbox{quantized}(v)\\ =\\ \\begin{cases} \\mbox{sign}(v)\\ \\cdot\\ 2^{\\mbox{round}(\\log2(\\mbox{abs}(v)))} & \\text{if}\\ v\\ \\ge\\ 2^{2^{\\text{bits}}\\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\) Power of 2 Quantization can currently only be enabled for weights.","title":"Quantization"},{"location":"compression/compression/quantization/#noisy-quantization","text":"As described in https://openreview.net/pdf?id=dV19Yyi1fS3 quantizing only a subset of model weights during training can improve accuracies of quantized networks compared to full quantization aware training. The probability of quantizing a weight can be given as parameter noise_prob in in the qconfig. Unfortunately this introduces an additional hyperparameter in the quantization space, good values for noise prob seem to be in the range of 0.7 to 0.9. Even when using noisy quantization eval and test are always run on fully quantized networks.","title":"Noisy Quantization"},{"location":"compression/compression/quantization/#configuration","text":"Quantization can be configured as the qconfig attribute of factory based models: qconfig: target : speech_recognition.models.factory.qconfig.get_trax_qat_qconfig config: bw_b: 8 # Bitwidth for bias bw_w: 6 # Bitwidth for weights bw_f: 8 # Bitwidth for activations power_of_2: false # Use power of two quantization for weights noise_prob: 0.7 # Probability of quantizing a value during training Aditionally standard pytorch quantization aware training is supported by using a standard pytorch qconfig. qconfig : _target_ : torch . quantization . get_default_qat_qconfig backend : fbgemm In this case no quantization noise is supported.","title":"Configuration"},{"location":"configuration/configuration/multi_gpu/","text":"Multi GPU training Hannah supports multi GPU-Training using the lightning distributed APIs: We provide preset trainer configs for distributed data parallel training: hannah-train trainer=ddp trainer.devices=[0,1] And for sharded training using fairscale: hannah-train trainer=sharded trainer.devices=[0,1] Sharded training distributes some of the model parameters across multiple devices and allows fitting bigger models in the same amount of GPU memory.","title":"Multi-GPU support"},{"location":"configuration/configuration/multi_gpu/#multi-gpu-training","text":"Hannah supports multi GPU-Training using the lightning distributed APIs: We provide preset trainer configs for distributed data parallel training: hannah-train trainer=ddp trainer.devices=[0,1] And for sharded training using fairscale: hannah-train trainer=sharded trainer.devices=[0,1] Sharded training distributes some of the model parameters across multiple devices and allows fitting bigger models in the same amount of GPU memory.","title":"Multi GPU training"},{"location":"configuration/configuration/overview/","text":"Configuration Configurations are managed by hydra . And follow a structured configuration. The currently used configuration can be shown with: hannah-train -c job The default configuration is located under speech_recognition/conf/ . Configuration Structure There are two main locations for configuration: Framework configuration files are located under hannah/conf configuration files located in this directory should be useful for more than one project. Project specific configuration files are located in configs/<project_name> these usually contain a project specific configuration file and one or more configured experiments in the subfolder experiment make use of composition using defaults lists as much as possible Guideline for writing configuration files Consider avoiding project specific main configuration files, and try to configure your project using experiments only. Use composition defaults list composition as much as possible. Configuration Groups We currently have the following configuration groups: backend Configures the app to run a subset of validation and test on configured backend Choices are: null (no backend, default), torchmobile (translates model to backend) checkpoint Choices are: null (no checkpoint), all (default, creates checkpoints for all training epoch) best (checkpoint, best 5 values, default) compression Configuration for model compression. dataset Choices are: kws (For Google Keyword Spotting), snips (For hey snips dataset), vad (For Voice Activity Detection) Common configuration options for datasets are: data_folder base folder for data (actual data will usually be stored in $data_folder/dataset_name) cls speech_recognition.dataset.SpeechCommandsDataset dataset name of the dataset group_speakers_by_id true group the data by user id before splitting if available use_default_split false if the dataset has a predefined split, use it (ignores *_pct, and group by id parameters) test_pct Percentage of dataset used for test dev_pct Percentage of dataset used for validation train_pct Percentage of dataset used for training wanted_words Wanted words for keyword datasets input_length Length of input in number of samples samplingrate Sampling rate of data in Hz clear_download \"Remove downloaded archive after dataset has been extracted variants Variant of dataset to use for multilanguage datasets timeshift_ms \"Timeshift the input data by +- given ms\" extract loudest silence_prob % dataset samples that are silence unknown_prob % dataset samples that are unknown test_snr SNR used for test train_snr_high minimal SNR for training data train_snr_low maximal SNR for test data noise_dataset [\"TUT\", \"FSDKaggle\", \"FSDnoisy\"] Downloads all the specified datasets. Use TUT + one other data_split initial split after downloading the datasets(Possibilities: \"vad\", \"vad_speech\", \"vad_balanced\", \"getrennt\") downsample samplerate DESTRUCTIVE! change the samplerate of the real files to the target samplerate. Use better parameter samplingrate sensor resolution can be adjusted for resizing of images (currently only for vision capsule datasets). variants variants for kws - v1, v2 variants for snips - snips variants for vad - UWNU - Mozilla has the following language options: - en: Englisch - de: Detusch - fr: Franz\u00f6sisch - it: Italienisch - es: Spanisch - kab: Kabylisch - ca: Katalanisch - nl: Niderl\u00e4ndisch - eo: Esperanto - fa: Persisch - eu: Baskisch - rw: Kinyarwanda - ru: Russisch - pt: Portugiesisch - pl: Polnisch features Feature extractor to use, choices are: melspec, mfcc, raw, sinc, spectrogram . All feature extractors apart from sinc (Sinc Convolutions) currently use torchaudio feature extractors and use the same input parameters. Sinc Convolutions have the following parameters: normalizer Choices: null (default), fixedpoint (normalize feature values to [-1.0, 1.0]) Feature Normalizer run between feature extraction and the neural network model. model Neural network to train: choices are gds (Old sinc1 with sinc convolutions removed), lstm, tc-res8, branchy-tc-res8 module The toplevel module implementing the training and eval loops. We curently support the following Tasks: stream_classifier : Classification on multichannel 1D data streams image_classifier : Classification on Image Data optimizer Choices are: adadelta, adam, adamax, adamw, asgd, lbfgs, rmsprop, rprop, sgd, sparse_adam Directly instantiate the corresponding pytorch classes and take the same options. profiler Choices are: null , advanced, simple Run profiling for different phases. scheduler Choices are: null , 1cycle, cosine, cosine_warm, cyclic, exponential, multistep, plateau, step Learning rate scheduler to use for scheduling, default is null. trainer Choices: default, dds Capsules the options to the lightning trainer. Currently it sets the following defaults: Default options are: devices 1 auto_select_gpus True limit_train_batches 1.0 limit_val_batches 1.0 limit_test_batches 1.0 max_epochs 80 default_root_dir . fast_dev_run false overfit_batches 0.0 benchmark True deterministic True Environment Variables The default configurations interpolate the following environment variables: HANNAH_CACHE_DIR Location of a directory used to cache file loading in some datasets HANNAH_DATA_FOLDER Change default location of dataset folders by default we will use subdirectory datasets of current working directory","title":"Overview"},{"location":"configuration/configuration/overview/#configuration","text":"Configurations are managed by hydra . And follow a structured configuration. The currently used configuration can be shown with: hannah-train -c job The default configuration is located under speech_recognition/conf/ .","title":"Configuration"},{"location":"configuration/configuration/overview/#configuration-structure","text":"There are two main locations for configuration: Framework configuration files are located under hannah/conf configuration files located in this directory should be useful for more than one project. Project specific configuration files are located in configs/<project_name> these usually contain a project specific configuration file and one or more configured experiments in the subfolder experiment make use of composition using defaults lists as much as possible","title":"Configuration Structure"},{"location":"configuration/configuration/overview/#guideline-for-writing-configuration-files","text":"Consider avoiding project specific main configuration files, and try to configure your project using experiments only. Use composition defaults list composition as much as possible.","title":"Guideline for writing configuration files"},{"location":"configuration/configuration/overview/#configuration-groups","text":"We currently have the following configuration groups:","title":"Configuration Groups"},{"location":"configuration/configuration/overview/#backend","text":"Configures the app to run a subset of validation and test on configured backend Choices are: null (no backend, default), torchmobile (translates model to backend)","title":"backend"},{"location":"configuration/configuration/overview/#checkpoint","text":"Choices are: null (no checkpoint), all (default, creates checkpoints for all training epoch) best (checkpoint, best 5 values, default)","title":"checkpoint"},{"location":"configuration/configuration/overview/#compression","text":"Configuration for model compression.","title":"compression"},{"location":"configuration/configuration/overview/#dataset","text":"Choices are: kws (For Google Keyword Spotting), snips (For hey snips dataset), vad (For Voice Activity Detection) Common configuration options for datasets are: data_folder base folder for data (actual data will usually be stored in $data_folder/dataset_name) cls speech_recognition.dataset.SpeechCommandsDataset dataset name of the dataset group_speakers_by_id true group the data by user id before splitting if available use_default_split false if the dataset has a predefined split, use it (ignores *_pct, and group by id parameters) test_pct Percentage of dataset used for test dev_pct Percentage of dataset used for validation train_pct Percentage of dataset used for training wanted_words Wanted words for keyword datasets input_length Length of input in number of samples samplingrate Sampling rate of data in Hz clear_download \"Remove downloaded archive after dataset has been extracted variants Variant of dataset to use for multilanguage datasets timeshift_ms \"Timeshift the input data by +- given ms\" extract loudest silence_prob % dataset samples that are silence unknown_prob % dataset samples that are unknown test_snr SNR used for test train_snr_high minimal SNR for training data train_snr_low maximal SNR for test data noise_dataset [\"TUT\", \"FSDKaggle\", \"FSDnoisy\"] Downloads all the specified datasets. Use TUT + one other data_split initial split after downloading the datasets(Possibilities: \"vad\", \"vad_speech\", \"vad_balanced\", \"getrennt\") downsample samplerate DESTRUCTIVE! change the samplerate of the real files to the target samplerate. Use better parameter samplingrate sensor resolution can be adjusted for resizing of images (currently only for vision capsule datasets).","title":"dataset"},{"location":"configuration/configuration/overview/#variants","text":"variants for kws - v1, v2 variants for snips - snips variants for vad - UWNU - Mozilla has the following language options: - en: Englisch - de: Detusch - fr: Franz\u00f6sisch - it: Italienisch - es: Spanisch - kab: Kabylisch - ca: Katalanisch - nl: Niderl\u00e4ndisch - eo: Esperanto - fa: Persisch - eu: Baskisch - rw: Kinyarwanda - ru: Russisch - pt: Portugiesisch - pl: Polnisch","title":"variants"},{"location":"configuration/configuration/overview/#features","text":"Feature extractor to use, choices are: melspec, mfcc, raw, sinc, spectrogram . All feature extractors apart from sinc (Sinc Convolutions) currently use torchaudio feature extractors and use the same input parameters. Sinc Convolutions have the following parameters:","title":"features"},{"location":"configuration/configuration/overview/#normalizer","text":"Choices: null (default), fixedpoint (normalize feature values to [-1.0, 1.0]) Feature Normalizer run between feature extraction and the neural network model.","title":"normalizer"},{"location":"configuration/configuration/overview/#model","text":"Neural network to train: choices are gds (Old sinc1 with sinc convolutions removed), lstm, tc-res8, branchy-tc-res8","title":"model"},{"location":"configuration/configuration/overview/#module","text":"The toplevel module implementing the training and eval loops. We curently support the following Tasks: stream_classifier : Classification on multichannel 1D data streams image_classifier : Classification on Image Data","title":"module"},{"location":"configuration/configuration/overview/#optimizer","text":"Choices are: adadelta, adam, adamax, adamw, asgd, lbfgs, rmsprop, rprop, sgd, sparse_adam Directly instantiate the corresponding pytorch classes and take the same options.","title":"optimizer"},{"location":"configuration/configuration/overview/#profiler","text":"Choices are: null , advanced, simple Run profiling for different phases.","title":"profiler"},{"location":"configuration/configuration/overview/#scheduler","text":"Choices are: null , 1cycle, cosine, cosine_warm, cyclic, exponential, multistep, plateau, step Learning rate scheduler to use for scheduling, default is null.","title":"scheduler"},{"location":"configuration/configuration/overview/#trainer","text":"Choices: default, dds Capsules the options to the lightning trainer. Currently it sets the following defaults: Default options are: devices 1 auto_select_gpus True limit_train_batches 1.0 limit_val_batches 1.0 limit_test_batches 1.0 max_epochs 80 default_root_dir . fast_dev_run false overfit_batches 0.0 benchmark True deterministic True","title":"trainer"},{"location":"configuration/configuration/overview/#environment-variables","text":"The default configurations interpolate the following environment variables: HANNAH_CACHE_DIR Location of a directory used to cache file loading in some datasets HANNAH_DATA_FOLDER Change default location of dataset folders by default we will use subdirectory datasets of current working directory","title":"Environment Variables"},{"location":"deployment/deployment/tensorrt/","text":"TensorRT Backend For deployment on NVIDIA targets we support TensorRT backends. Currently the TensorRT backend always compiles for the first GPU of the local system. Installation The tensorrt module in the poetry shell needs to be installed seperately via pip: poetry shell pip install tensorrt Configuration The backend supports the following configuration options. val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs) TODO: [ ] remote execution support [ ] profiling and feedback support","title":"TensorRT"},{"location":"deployment/deployment/tensorrt/#tensorrt-backend","text":"For deployment on NVIDIA targets we support TensorRT backends. Currently the TensorRT backend always compiles for the first GPU of the local system.","title":"TensorRT Backend"},{"location":"deployment/deployment/tensorrt/#installation","text":"The tensorrt module in the poetry shell needs to be installed seperately via pip: poetry shell pip install tensorrt","title":"Installation"},{"location":"deployment/deployment/tensorrt/#configuration","text":"The backend supports the following configuration options. val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs)","title":"Configuration"},{"location":"deployment/deployment/tensorrt/#todo","text":"[ ] remote execution support [ ] profiling and feedback support","title":"TODO:"},{"location":"deployment/deployment/torchmobile/","text":"Torchmobile backend A backend that runs the trained neural network through torchmobile on the local cpu. This is an example implementation of backend that allows testing of backend integrations without installation of further packages. It should not be used for serious optimizations, as measurements are much to noisy. Configuration val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs)","title":"Torch Mobile"},{"location":"deployment/deployment/torchmobile/#torchmobile-backend","text":"A backend that runs the trained neural network through torchmobile on the local cpu. This is an example implementation of backend that allows testing of backend integrations without installation of further packages. It should not be used for serious optimizations, as measurements are much to noisy.","title":"Torchmobile backend"},{"location":"deployment/deployment/torchmobile/#configuration","text":"val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs)","title":"Configuration"},{"location":"deployment/deployment/trax-Ultratrail/","text":"Ultratrail backend To use the ultratrail backend use Install teda: https://atreus.informatik.uni-tuebingen.de/ties/t-rax/teda Set environment variable TEDA_HOME to path of teda checkout To run it use: hannah-train compress=fp_8_8_8 backend=trax_ut Configuration teda_dir ${env:TEDA_HOME,/local/frischkn/neural_networks/teda} backend_dir \"${backend.teda_dir}/rtl/tc-resnet8-accelerator\" standalone False rtl_simulation False synthesis False postsyn_simulation False power_estimation False num_inferences 1 bw_w 8 bw_b 8 bw_f 8 cols 8 rows 8 period 1000 macro_type \"RTL\" use_acc_statistic_model True use_acc_analytical_model False","title":"T-Rax Ultratrail"},{"location":"deployment/deployment/trax-Ultratrail/#ultratrail-backend","text":"To use the ultratrail backend use Install teda: https://atreus.informatik.uni-tuebingen.de/ties/t-rax/teda Set environment variable TEDA_HOME to path of teda checkout To run it use: hannah-train compress=fp_8_8_8 backend=trax_ut","title":"Ultratrail backend"},{"location":"deployment/deployment/trax-Ultratrail/#configuration","text":"teda_dir ${env:TEDA_HOME,/local/frischkn/neural_networks/teda} backend_dir \"${backend.teda_dir}/rtl/tc-resnet8-accelerator\" standalone False rtl_simulation False synthesis False postsyn_simulation False power_estimation False num_inferences 1 bw_w 8 bw_b 8 bw_f 8 cols 8 rows 8 period 1000 macro_type \"RTL\" use_acc_statistic_model True use_acc_analytical_model False","title":"Configuration"},{"location":"deployment/deployment/tvm/","text":"TVM backend A backend that runs the trained neural network through tvm with remote target support through automate. Configuration val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs) board Target board configuration tuner optional tuner configuration Board Configuration MicroTVM connected boards or simulators Automate connected Remote Boards Tuner Autotuning is configure using the configuration group backend/tuner . There are 3 predefined tuner configurations: \u00e0utotvm Uses the standard configuration for autotuning on the target board, it is generallly the recommended setting for 8-bit quantized models on CPU like devices. As it allows the targets to use hand defined tuning templates, which generally include tvm tuning templates. auto_scheduler Uses the standard tvm auto_scheduler. This scheduler does not support tensorization and is therefore not recommended on targets that support tensorized instructions. This is mostly the case for tensorized instructions on the target boards. baseline This does not use any tuning but uses the predefined schedules directly. All tuner configurations are parametrizable using the following tuner configurations: All autotuning can be deactivated when setting backend.tune = false Tuner Configuration","title":"TVM"},{"location":"deployment/deployment/tvm/#tvm-backend","text":"A backend that runs the trained neural network through tvm with remote target support through automate.","title":"TVM backend"},{"location":"deployment/deployment/tvm/#configuration","text":"val_batches 1 (number of batches used for validation) test_batches 1 (number of batches used for test) val_frequency 10 (run backend every n validation epochs) board Target board configuration tuner optional tuner configuration","title":"Configuration"},{"location":"deployment/deployment/tvm/#board-configuration","text":"","title":"Board Configuration"},{"location":"deployment/deployment/tvm/#microtvm-connected-boards-or-simulators","text":"","title":"MicroTVM connected boards or simulators"},{"location":"deployment/deployment/tvm/#automate-connected-remote-boards","text":"","title":"Automate connected Remote Boards"},{"location":"deployment/deployment/tvm/#tuner","text":"Autotuning is configure using the configuration group backend/tuner . There are 3 predefined tuner configurations: \u00e0utotvm Uses the standard configuration for autotuning on the target board, it is generallly the recommended setting for 8-bit quantized models on CPU like devices. As it allows the targets to use hand defined tuning templates, which generally include tvm tuning templates. auto_scheduler Uses the standard tvm auto_scheduler. This scheduler does not support tensorization and is therefore not recommended on targets that support tensorized instructions. This is mostly the case for tensorized instructions on the target boards. baseline This does not use any tuning but uses the predefined schedules directly. All tuner configurations are parametrizable using the following tuner configurations: All autotuning can be deactivated when setting backend.tune = false","title":"Tuner"},{"location":"deployment/deployment/tvm/#tuner-configuration","text":"","title":"Tuner Configuration"},{"location":"development/devel/api/","text":"hannah.conf.scheduler OneCycleLRConf Objects @dataclass class OneCycleLRConf () Config for one cycle lr total steps are configured from module hannah.conf hannah.conf.optimizer SGDConf Objects @dataclass class SGDConf () lr _RequiredParameter MADGRADConf Objects @dataclass class MADGRADConf () lr _RequiredParameter hannah.conf.nas hannah.trainer.cross_validation hannah.trainer hannah.tools.train hannah.tools hannah.tools.characterize main @hydra . main ( config_name = \"characterize\" , config_path = \"../conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig: hannah.tools.eval eval_checkpoint def eval_checkpoint ( config : DictConfig , checkpoint ) -> None Arguments : config - DictConfig: checkpoint: config - DictConfig: config - DictConfig: eval def eval ( config : DictConfig ) -> Optional [ bool ] Arguments : config - DictConfig: config - DictConfig: config - DictConfig: main @hydra . main ( config_name = \"eval\" , config_path = \"conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig: hannah.tools.objectdetection_eval eval_train def eval_train ( config , module , test = True ) Arguments : config - param module: test - Default value = True) module: eval_steps def eval_steps ( config , module , hparams , checkpoint ) Arguments : config - param module: hparams - param checkpoint: module: checkpoint: eval_checkpoint def eval_checkpoint ( config : DictConfig , checkpoint ) Arguments : config - DictConfig: checkpoint: config - DictConfig: config - DictConfig: eval def eval ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig: main @hydra . main ( config_name = \"objectdetection_eval\" , config_path = \"../conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig: hannah.modules.vision.base hannah.modules.vision.anomaly_detection AnomalyDetectionModule Objects class AnomalyDetectionModule ( VisionBaseModule ) on_test_end def on_test_end () wd_dir = os.getcwd() score, largest_train_error = self.compute_anomaly_score() train_errors = self.normalized_train_errors plt.hist(train_errors.detach().cpu().numpy(), bins=100) plt.axvline(score, linestyle=\"dashed\") plt.title(\"Normalized train reconstruction errors\") plt.savefig(wd_dir + \"/normalized_train_errors.png\") test = ( torch.tensor(self.test_losses, device=self.device) / torch.max(torch.stack(self.train_losses), dim=0).values ) plt.hist(test.detach().cpu().numpy(), bins=100) plt.title(\"Normalized test reconstruction errors\") plt.savefig(wd_dir + \"/normalized_test_errors.png\") print(\"Anomaly score\", score) print( \"Largest train error\", torch.max(torch.stack(self.train_losses), dim=0).values, ) hannah.modules.vision.image_classifier hannah.modules.vision.loss hannah.modules.vision.anomaly_score class AnomalyScore(CatMetric): def init (self, percentile, nan_strategy=\"warn\", kwargs): super(). init (nan_strategy=nan_strategy, kwargs) self.percentile = percentile def compute(self): anomaly_score = None train_losses = super().compute() if train_losses: normalized_train_errors = torch.stack(train_losses) / ( torch.max(torch.stack(train_losses), dim=0).values ) anomaly_score = np.percentile( normalized_train_errors.cpu().numpy(), self.percentile ) return anomaly_score hannah.modules.vision hannah.modules.base ClassifierModule Objects class ClassifierModule ( LightningModule , ABC ) total_training_steps def total_training_steps () -> int Total training steps inferred from datamodule and devices. hannah.modules.augmentation.augmentation hannah.modules.augmentation.transforms.kornia_transforms hannah.modules.augmentation.transforms.registry hannah.modules.augmentation.transforms hannah.modules.augmentation.bordersearch hannah.modules.augmentation.batch_augmentation BatchAugmentationPipeline Objects class BatchAugmentationPipeline ( nn . Module ) __init__ def __init__ ( transforms = {}) Augmentation pipeline especially for self supervised learning Arguments : replica int - number of replicated different random augmentations transforms dict - configuration of transforms forward @torch . no_grad () def forward ( x ) -> torch . Tensor Perform Augmentations Arguments : x torch.Tensor - a torch.Tensor representing the augementation pipeline Returns : Tuple[torch.Tensor, torch.Tensor]; Batch augmented with replica different random augmentations hannah.modules.augmentation hannah.modules.config_utils dump_config def dump_config ( output_dir , config ) Dumps the configuration to json format Creates file config.json in output_dir Parameters output_dir : str Output directory config : dict Configuration to dump save_model def save_model ( output_dir , model ) Creates serialization of the model for later inference, evaluation Creates the following files: model.pt: Serialized version of network parameters in pytorch model.json: Serialized version of network parameters in json format model.onnx: full model including paramters in onnx format Parameters output_dir : str Directory to put serialized models model : LightningModule Model to serialize hannah.modules.classifier hannah.modules.angle_classifier hannah.modules hannah.modules.object_detection hannah.modules.metrics Error Objects class Error () Computes Error = 1 - Accuracy_ .. math:: \\text{Error} = 1 - \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. This module is a simple wrapper to get the task specific versions of this metric, which is done by setting the task argument to either 'binary' , 'multiclass' or multilabel . See the documentation of :mod: BinaryError , :mod: MulticlassError and :mod: MultilabelError for the specific details of each argument influence and examples. plot_confusion_matrix def plot_confusion_matrix ( cf , group_names = None , categories = \"auto\" , count = True , percent = True , cbar = True , xyticks = True , xyplotlabels = True , sum_stats = True , figsize = None , cmap = \"Blues\" , title = None ) This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization. Arguments cf: confusion matrix to be passed in group_names: List of strings that represent the labels row by row to be shown in each square. categories: List of strings containing the categories to be displayed on the x,y axis. Default is 'auto' count: If True, show the raw number in the confusion matrix. Default is True. normalize: If True, show the proportions for each category. Default is True. cbar: If True, show the color bar. The cbar values are based off the values in the confusion matrix. Default is True. xyticks: If True, show x and y ticks. Default is True. xyplotlabels: If True, show 'True Label' and 'Predicted Label' on the figure. Default is True. sum_stats: If True, display summary statistics below the figure. Default is True. figsize: Tuple representing the figure size. Default will be the matplotlib rcParams value. cmap: Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues' See http://matplotlib.org/examples/color/colormaps_reference.html title: Title for the heatmap. Default is None. hannah.ssl.hard_labeling HardLabeling Objects class HardLabeling () training_step def training_step ( unlabeled_data : torch . Tensor , trainer : pl . Trainer , pl_module : pl . LightningModule , batch_idx : int = - 1 ) -> torch . Tensor Calculate pseudo label loss from unlabeled data. get_dropout_layers def get_dropout_layers () Returns all model layers of class dropout or dropblock. compute_loss def compute_loss ( inputs , logits , targets , loss_fn = None ) Helper function to compute loss, possibly with consistency regularization by augmentations (FixMatch). negative_cre_loss def negative_cre_loss ( logits , targets ) Cross Entropy Loss for negative learning which requires a mutli- class and multi-label loss function. hannah.ssl hannah.normalizer FixedPointNormalizer Objects class FixedPointNormalizer ( nn . Module ) Simple feature normalizer for fixed point models AdaptiveFixedPointNormalizer Objects class AdaptiveFixedPointNormalizer ( nn . Module ) Simple feature normalizer for fixed point models hannah.quantization.rounding round_downward def round_downward ( x : Tensor ) -> Tensor Round to nearest upward round_upward def round_upward ( x : Tensor ) -> Tensor Round to nearest downward round_odd def round_odd ( x : Tensor ) -> Tensor Round to nearest odd round_even def round_even ( x : Tensor ) -> Tensor Round to nearest even round_zero def round_zero ( x : Tensor ) -> Tensor Round towards zero round_infinity def round_infinity ( x : Tensor ) -> Tensor Round toward infinity truncate_up def truncate_up ( x : Tensor ) -> Tensor Always round up to next integer truncate_down def truncate_down ( x : Tensor ) -> Tensor Always round down to next integer truncate_infinity def truncate_infinity ( x : Tensor ) -> Tensor Always round to next integer in direction infinity truncate_zero def truncate_zero ( x : Tensor ) -> Tensor Always round to next integer in direction of Zero round_stochastic def round_stochastic ( x : Tensor ) -> Tensor Round stochastically hannah.quantization.qconfig hannah.quantization.callback hannah.train hannah.nn.qat Implementations of torch.nn.intrinsics qat with an optional quantize bias parameter. Qconfigs can support an optional bias quantization funciton which should be returned by qconfig.bias() else biases will be quantized with qconfig.activation() _ConvBnNd Objects class _ConvBnNd ( nn . modules . conv . _ConvNd , _ConvForwardMixin ) train def train ( mode : bool = True ) -> Any Batchnorm's training behavior is using the self.training flag. Prevent changing it if BN is frozen. This makes sure that calling model.train() on a model with a frozen BN will behave properly. from_float @classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user ConvBn1d Objects class ConvBn1d ( _ConvBnNd ) A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv1d and :class: torch.nn.BatchNorm1d . Similar to :class: torch.nn.Conv1d , with FakeQuantize modules initialized to default. Attributes : freeze_bn: - weight_fake_quant - fake quant module for weight ConvBnReLU1d Objects class ConvBnReLU1d ( ConvBn1d ) A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv1d and :class: torch.nn.BatchNorm1d and :class: torch.nn.ReLU . Similar to torch.nn.Conv1d , with FakeQuantize modules initialized to default. Attributes : weight_fake_quant - fake quant module for weight ConvBn2d Objects class ConvBn2d ( _ConvBnNd ) A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv2d and :class: torch.nn.BatchNorm2d . Similar to :class: torch.nn.Conv2d , with FakeQuantize modules initialized to default. Attributes : freeze_bn: - weight_fake_quant - fake quant module for weight ConvBnReLU2d Objects class ConvBnReLU2d ( ConvBn2d ) A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv2d and :class: torch.nn.BatchNorm2d and :class: torch.nn.ReLU . Similar to torch.nn.Conv2d , with FakeQuantize modules initialized to default. Attributes : weight_fake_quant - fake quant module for weight ConvReLU2d Objects class ConvReLU2d ( nn . Conv2d , _ConvForwardMixin ) A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. We combined the interface of :class: ~torch.nn.Conv2d and :class: ~torch.nn.BatchNorm2d . Attributes : weight_fake_quant - fake quant module for weight ConvReLU1d Objects class ConvReLU1d ( nn . Conv1d , _ConvForwardMixin ) A ConvReLU1d module is fused module of Conv1d and ReLU, attached with FakeQuantize modules for quantization aware training Conv1d Objects class Conv1d ( nn . Conv1d , _ConvForwardMixin ) A Conv1d module is a Conv1d module , attached with FakeQuantize modules for weight for quantization aware training. Attributes : weight_fake_quant - fake quant module for weight bias_fake_quant - fake quant module for bias activation_post_process - fake_quant_module for activations Conv2d Objects class Conv2d ( nn . Conv2d , _ConvForwardMixin ) A Conv2d module is a Conv2d module , attached with FakeQuantize modules for weight for quantization aware training. Attributes : weight_fake_quant - fake quant module for weight bias_fake_quant - fake quant module for bias activation_post_process - fake_quant_module for activations Linear Objects class Linear ( nn . Linear ) A linear module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear , with FakeQuantize modules initialized to default. Attributes : weight - fake quant module for weight from_float @classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user LinearReLU Objects class LinearReLU ( nn . Linear ) A linear module attached with FakeQuantize modules and ReLU for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear , with FakeQuantize modules initialized to default. Attributes : weight - fake quant module for weight from_float @classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user Identity Objects class Identity ( nn . Identity ) A identity module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Identity , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Identity for documentation. Similar to torch.nn.Identity , with FakeQuantize modules initialized to default. from_float @classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user hannah.nn.quantized hannah.backends.tensorrt TensorRTBackend Objects class TensorRTBackend ( InferenceBackendBase ) output_spec def output_spec () Get the specs for the output tensor of the network. Useful to prepare memory allocations. Returns : Two items, the shape of the output tensor and its (numpy) datatype. hannah.backends.base ProfilingResult Objects class ProfilingResult ( NamedTuple ) Result of a profiling run Attributes : outputs - the outputs of the model on the given input batch metrics - a dictionary containing the combined metrics obtained from the profiling run profile - the raw profile in a backend-specific format AbstractBackend Objects class AbstractBackend ( ABC ) prepare @abstractmethod def prepare ( module : ClassifierModule ) Prepare the model for execution on the target device Arguments : module - the classifier module to be exported run @abstractmethod def run ( * inputs ) -> Union [ torch . Tensor , Sequence [ torch . Tensor ]] Run a batch on the target device Arguments : inputs - a list of torch tensors representing the inputs to be run on the target device, each tensor represents a whole batched input, so for models taking 1 parameter, the list will contain 1 tensor of shape (batch_size, *input_shape) Returns - the output(s) of the model as a torch tensor or a Sequence of torch tensors for models producing multiple outputs profile @abstractmethod def profile ( * inputs : torch . Tensor ) -> ProfilingResult Do a profiling run on the target device Arguments : inputs - a list of torch tensors representing the inputs to be run on the target device, each tensor represents a whole batched input, so for models taking 1 parameter, the list will contain 1 tensor of shape (batch_size, *input_shape) Returns - a ProfilingResult object containing the outputs of the model, the metrics obtained from the profiling run and the raw profile in a backend-specific format available @classmethod @abstractmethod def available ( cls ) -> bool Check if the backend is available Returns: True if the backend is available, False otherwise export def export () -> None Export the model through the target backend InferenceBackendBase Objects class InferenceBackendBase ( AbstractBackend ) Base class for backends, it is only here for backwards compatibility reasons, use AbstractBackend instead hannah.backends.onnxrt OnnxruntimeBackend Objects class OnnxruntimeBackend ( InferenceBackendBase ) Inference Backend for tensorflow hannah.backends hannah.backends.utils symbolic_batch_dim def symbolic_batch_dim ( model ) -> None make the batch dimension symbolic for onnx models Arguments : model - onnx model hannah.backends.torch_mobile TorchMobileBackend Objects class TorchMobileBackend ( InferenceBackendBase ) Inference backend for torch mobile hannah.utils.logger JSONLogger Objects class JSONLogger ( Logger ) name @property def name () -> str Gets the name of the experiment. Returns : The name of the experiment. version @property def version () -> Union [ int , str ] Gets the version of the experiment. Returns : The version of the experiment if it is specified, else the next version. root_dir @property def root_dir () -> str Gets the save directory where the versioned JSON experiments are saved. log_dir @property def log_dir () -> str The log directory for this run. By default, it is named 'version_${self.version}' but it can be overridden by passing a string value for the constructor's version parameter instead of None or an int. experiment @property @rank_zero_experiment def experiment () -> \"_ExperimentWriter\" Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following. Example:: self.logger.experiment.some_experiment_writer_function() _ExperimentWriter Objects class _ExperimentWriter () Experiment writer for CSVLogger. Arguments : log_dir - Directory for the experiment logs log_metrics def log_metrics ( metrics_dict : Dict [ str , float ], step : Optional [ int ] = None ) -> None Record metrics. save def save () -> None Save recorded metrics into files. hannah.utils.imports lazy_import def lazy_import ( module_name , callback = None ) Returns a proxy module object that will lazily import the given module the first time it is used. Example usage:: Lazy version of import tensorflow as tf tf = lazy_import(\"tensorflow\") Other commands Now the module is loaded tf. version Arguments : module_name - the fully-qualified module name to import callback None - a callback function to call before importing the module Returns : a proxy module object that will be lazily imported when first used LazyModule Objects class LazyModule ( types . ModuleType ) Proxy module that lazily imports the underlying module the first time it is actually used. Arguments : module_name - the fully-qualified module name to import callback None - a callback function to call before importing the module hannah.utils.dvclive hannah.utils hannah.utils.utils log_execution_env_state def log_execution_env_state () -> None Log information about the execution environment. git_version def git_version ( short = True ) Return the current git sha Arguments : short bool - If True, return the short (7 character) version of the SHA Returns : str - The current git SHA extract_from_download_cache def extract_from_download_cache ( filename , url , cached_files , target_cache , target_folder , target_test_folder = \"\" , clear_download = False , no_exist_check = False ) -> None extracts given file from cache or donwloads first from url Arguments : filename str - name of the file to download or extract url str - possible url to download the file cached_files (list(str)): cached files in download cache target_cache str - path to the folder to cache file if download necessary target_folder str - path where to extract file target_test_folder str, optional - folder to check if data are already there clear_download bool - clear download after usage no_exist_check bool - disables the check if folder exists fullname def fullname ( o ) -> Any Get the full classname of an object including surrounding packages/modules/namespaces set_deterministic @contextmanager def set_deterministic ( mode , warn_only = False ) A contextmanager to set deterministic algorithms hannah.utils.tuple hannah.logo hannah.callbacks.summaries walk_model def walk_model ( model , dummy_input ) Adapted from IntelLabs Distiller Arguments : model: dummy_input: MacSummaryCallback Objects class MacSummaryCallback ( Callback ) predict def predict ( pl_module , input = input ) Arguments : pl_module: on_train_start @rank_zero_only def on_train_start ( trainer , pl_module ) Arguments : trainer: pl_module: on_test_end @rank_zero_only def on_test_end ( trainer , pl_module ) Arguments : trainer: pl_module: on_validation_epoch_end @rank_zero_only def on_validation_epoch_end ( trainer , pl_module ) Arguments : trainer: pl_module: estimate def estimate ( pl_module , input = None ) Generate Summary Metrics for neural network Arguments : pl_module(pytorch_lightning.LightningModule) - pytorch lightning module to summarize Returns : dict[str, float]: Dict of MetricName => Metric Value prod def prod ( seq ) Arguments : seq: hannah.callbacks.clustering clustering def clustering ( params , inertia , cluster ) Arguments : params: inertia: cluster: kMeans Objects class kMeans ( Callback ) on_test_epoch_start def on_test_epoch_start ( trainer , pl_module ) Arguments : trainer: pl_module: on_train_epoch_end def on_train_epoch_end ( trainer , pl_module ) Arguments : trainer: pl_module: hannah.callbacks.svd_compress SVD Objects class SVD ( Callback ) on_train_epoch_start def on_train_epoch_start ( trainer , pl_module ) Arguments : trainer: pl_module: hannah.callbacks.dump_layers TestDumperCallback Objects class TestDumperCallback ( Callback ) on_test_start def on_test_start ( pl_trainer , pl_model ) Arguments : pl_trainer: pl_model: hannah.callbacks.prediction_logger hannah.callbacks hannah.callbacks.pruning PruningAmountScheduler Objects class PruningAmountScheduler () FilteredPruning Objects class FilteredPruning ( ModelPruning ) setup def setup ( trainer : Trainer , pl_module : LightningModule , stage : str ) Arguments : trainer: pl_module: filter_parameters_to_prune def filter_parameters_to_prune ( parameters_to_prune = None ) Filter out unprunable parameters Arguments : parameters_to_prune - (Default value = None) on_test_end def on_test_end ( trainer , pl_module ) -> None Arguments : trainer: pl_module: hannah.callbacks.backbone_finetuning hannah.callbacks.fine_tuning hannah.callbacks.optimization HydraOptCallback Objects class HydraOptCallback ( Callback ) on_test_end def on_test_end ( trainer , pl_module ) Arguments : trainer: pl_module: on_validation_end def on_validation_end ( trainer , pl_module ) Arguments : trainer: pl_module: test_result def test_result () val_result def val_result () result def result ( dict = False ) Arguments : dict - (Default value = False) curves def curves ( dict = False ) Arguments : dict - (Default value = False) hannah.callbacks.backends hannah.datasets.vision.fake hannah.datasets.vision.base TorchvisionDatasetBase Objects class TorchvisionDatasetBase ( VisionDatasetBase ) Wrapper around torchvision classification datasets ImageDatasetBase Objects class ImageDatasetBase ( VisionDatasetBase ) __init__ def __init__ ( X , y , classes , bbox = None , transform = None ) Initialize vision dataset Arguments : X List[str] - List of paths to image files y List[str] - Class id of corresponding image classes List[str] - List of class names, names are ordered by numeric class id bbox Dict[str] - Dict with filename as keys, bbox coordinates as numpy arrays transform Callable[image,image], optional - Optional transformation/augmentation of input images. Defaults to None. hannah.datasets.vision.cifar hannah.datasets.vision.kvasir hannah.datasets.vision hannah.datasets.vision.dresden_capsule Rhode island gastroenterology video capsule endoscopy dataset https://www.nature.com/articles/s41597-022-01726-3 https://github.com/acharoen/Rhode-Island-GI-VCE-Technical-Validation hannah.datasets.vision.utils hannah.datasets.vision.utils.naneye read_naneye def read_naneye ( data_file : Union [ str , Path ]) Read a naneye raw aimage and decode bayer pattern Arguments : data_file Union[str, Path] - path to the datafile Returns : np.ndarray - uint8 array of decoded image data hannah.datasets.vision.kvasir_unlabeled KvasirCapsuleUnlabeled Objects class KvasirCapsuleUnlabeled ( AbstractDataset ) Dataset representing unlabelled videos sequential @property def sequential () -> bool Returns true if this dataset should only be iterated sequentially max_workers @property def max_workers () -> int Returns the maximum number of workers useable for this dataset hannah.datasets.vision.ri_capsule Rhode island gastroenterology video capsule endoscopy dataset https://www.nature.com/articles/s41597-022-01726-3 https://github.com/acharoen/Rhode-Island-GI-VCE-Technical-Validation split_train_set def split_train_set ( csv_file : pathlib . Path , drop_rate : float ) Split train set in two and save as separate csv files. hannah.datasets.Downsample hannah.datasets.activity Data3D Objects class Data3D () 3D-Data PAMPAP2_IMUData Objects class PAMPAP2_IMUData () A IMU set defined by temperature (\u00b0C) 3D-acceleration data (ms -2 ), scale: \u00b116g, resolution: 13-bit 3D-acceleration data (ms -2 ), scale: \u00b16g, resolution: 13-bit 3D-gyroscope data (rad/s) 3D-magnetometer data (\u03bcT) orientation (invalid in this data collection) PAMAP2_DataPoint Objects class PAMAP2_DataPoint () A temporal datapoint in the dataset PAMAP2_DataChunk Objects class PAMAP2_DataChunk () A DataChunk is a item of the pytorch dataset PAMAP2_Dataset Objects class PAMAP2_Dataset ( AbstractDataset ) Class for the PAMAP2 activity dataset https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring hannah.datasets.base DatasetType Objects class DatasetType ( Enum ) The type of a dataset partition e.g. train, dev, test AbstractDataset Objects class AbstractDataset ( Dataset , ABC ) prepare @abstractclassmethod def prepare ( cls , config : Dict [ str , Any ]) -> None Prepare the dataset. This method is run at the beginning of the dataset training. If possible it should download the dataset from its original source, if it is available for public download. Arguments : config Dict[Any] - The dataset configuration splits @abstractclassmethod def splits ( cls , config : Dict [ str , Any ] ) -> Tuple [ \"AbstractDataset\" , \"AbstractDataset\" , \"AbstractDataset\" ] Returns the test, validation and train split according to the Dataset config Arguments : config [type] - [description] class_names @abstractproperty def class_names () -> List [ str ] Returns the names of the classes in the classification dataset class_counts @abstractproperty def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segementation, it may return None __getitem__ @abstractmethod def __getitem__ ( index ) -> List [ torch . Tensor ] Returns a torch.Tensor for the data item at the corresponding index The length of the list depends on the dataset item to use Arguments : index int - the index of the data item __len__ @abstractmethod def __len__ () -> int Returns number of samples in dataset size def size () -> List [ int ] Returns dimension of output without batch dimension std @property def std () -> Optional [ Tuple [ int , ... ]] Returns channel-wise standard deviation for dataset if applicable mean @property def mean () -> Optional [ Tuple [ int , ... ]] Returns channel-wise means for dataset if applicable resolution @property def resolution () -> Optional [ Tuple [ int , ... ]] Returns resolution for dataset if applicable weights @property def weights () -> Optional [ List [ float ]] Class weights for weighted sampling sequential @property def sequential () -> bool Returns true if this dataset should only be iterated sequentially max_workers @property def max_workers () -> int Returns the maximum number of workers useable for this dataset hannah.datasets.speech SpeechDataset Objects class SpeechDataset ( AbstractDataset ) Base Class for speech datasets preprocess def preprocess ( example , silence = False , label = 0 ) Run preprocessing and feature extraction SpeechCommandsDataset Objects class SpeechCommandsDataset ( SpeechDataset ) This class implements reading and preprocessing of speech commands like dataset SpeechHotwordDataset Objects class SpeechHotwordDataset ( SpeechDataset ) Dataset Class for Hotword dataset e.g. Hey Snips! splits @classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List VadDataset Objects class VadDataset ( SpeechDataset ) splits @classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List hannah.datasets.eeg_tusz EEGDataset Objects class EEGDataset ( AbstractDataset ) class_names @property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset class_counts @property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None size def size () -> List [ int ] Returns dimension of output without batch dimension hannah.datasets.DatasetSplit hannah.datasets.Kitti Kitti Objects class Kitti ( AbstractDataset ) splits @classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List hannah.datasets.NoiseDataset hannah.datasets.physio AtrialFibrillationDataset Objects class AtrialFibrillationDataset ( PhysioDataset ) Atrial Fibrillation Database (https://physionet.org/content/afdb/1.0.0/) hannah.datasets.eeg_chb EEGDataset Objects class EEGDataset ( AbstractDataset ) class_names @property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset class_counts @property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None size def size () -> List [ int ] Returns dimension of output without batch dimension hannah.datasets.fake1d hannah.datasets hannah.datasets.eeg_chbrt EEGRTDataset Objects class EEGRTDataset ( AbstractDataset ) class_names @property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset class_counts @property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None size def size () -> List [ int ] Returns dimension of output without batch dimension hannah.datasets.utils.cache hannah.datasets.utils.md5 hannah.datasets.utils hannah.datasets.collate vision_collate_fn def vision_collate_fn ( batch ) Function that takes in a batch of data and puts the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a :class: torch.Tensor , a Sequence of :class: torch.Tensor , a Collection of :class: torch.Tensor , or left unchanged, depending on the input type. This is used as the default function for collation for vision tasks batch_size or batch_sampler is defined in :class: ~torch.utils.data.DataLoader . Here is the general input type (based on the type of the element within the batch) to output type mapping: :class: torch.Tensor -> :class: torch.Tensor (with an added outer dimension batch size) NumPy Arrays -> :class: torch.Tensor float -> :class: torch.Tensor int -> :class: torch.Tensor str -> str (unchanged) bytes -> bytes (unchanged) Mapping[K, V_i] -> Mapping[K, vision_collate([V_1, V_2, ...])] NamedTuple[V1_i, V2_i, ...] -> NamedTuple[vision_collate([V1_1, V1_2, ...]), vision_collate([V2_1, V2_2, ...]), ...] Sequence[V1_i, V2_i, ...] -> Sequence[vision_collate([V1_1, V1_2, ...]), vision_collate([V2_1, V2_2, ...]), ...] Arguments : batch - a single batch to be collated Examples : Example with a batch of int s: vision_collate([0, 1, 2, 3]) tensor([0, 1, 2, 3]) Example with a batch of str s: vision_collate(['a', 'b', 'c']) ['a', 'b', 'c'] Example with Map inside the batch: vision_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]) - {'A' - tensor([ 0, 100]), 'B': tensor([ 1, 100])} Example with NamedTuple inside the batch: Point = namedtuple('Point', ['x', 'y']) vision_collate([Point(0, 0), Point(1, 1)]) Point(x=tensor([0, 1]), y=tensor([0, 1])) Example with Tuple inside the batch: vision_collate([(0, 1), (2, 3)]) [tensor([0, 2]), tensor([1, 3])] Example with List inside the batch: vision_collate([[0, 1], [2, 3]]) [tensor([0, 2]), tensor([1, 3])] ctc_collate_fn def ctc_collate_fn ( data ) Creates mini-batch tensors from the list of tuples (src_seq, trg_seq). We should build a custom collate_fn rather than using default collate_fn, because merging sequences (including padding) is not supported in default. Sequences are padded to the maximum length of mini-batch sequences (dynamic padding). Arguments : data - list of tuple (src_seq, src_length, trg_seq, trg_length). src_seq: torch tensor of shape (x,?); variable length. src length: torch tenso of shape 1x1 trg_seq: torch tensor of shape (?); variable length. trg_length: torch_tensor of shape (1x1) Returns - tuple of four torch tensors src_seqs - torch tensor of shape (batch_size, x, padded_length). src_lengths - torch_tensor of shape (batch_size); valid length for each padded source sequence. trg_seqs - torch tensor of shape (batch_size, x, padded_length). trg_lengths - torch tensor of shape (batch_size); valid length for each padded target sequence. hannah.datasets.directional DirectionalDataset Objects class DirectionalDataset ( AbstractDataset ) Directional Dataset hannah.datasets.emergency EmergencySirenDataset Objects class EmergencySirenDataset ( AbstractDataset ) Emergency Dataset hannah.visualization hannah.features MFCC Objects class MFCC ( torchaudio . transforms . MFCC ) A simple wrapper around torchaudio mfcc, but melkwargs are given as direct named arguments instead of a dictionary SincConv Objects class SincConv ( nn . Module ) Sinc convolution: Arguments : in_channels - No. of input channels(must be 1) out_channels - No. of filters(40) sample_rate - sampling rate, default set at 32000 kernel_size - Filter length(101) LogSpectrogram Objects class LogSpectrogram ( torch . nn . Module ) Create a spectrogram from a audio signal. Arguments : n_fft int, optional - Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400 ) win_length int or None, optional - Window size. (Default: n_fft ) hop_length int or None, optional - Length of hop between STFT windows. (Default: win_length // 2 ) pad int, optional - Two sided padding of signal. (Default: 0 ) window_fn Callable[..., Tensor], optional - A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window ) power float or None, optional - Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. If None, then the complex spectrum is returned instead. (Default: 2 ) normalized bool, optional - Whether to normalize by magnitude after stft. (Default: False ) wkwargs dict or None, optional - Arguments for window function. (Default: None ) center bool, optional - whether to pad :attr: waveform on both sides so that the :math: t -th frame is centered at time :math: t \\times \\text{hop\\_length} . Default - True pad_mode string, optional - controls the padding method used when :attr: center is True . Default: \"reflect\" onesided bool, optional - controls whether to return half of results to avoid redundancy Default: True forward def forward ( waveform : torch . Tensor ) -> torch . Tensor Arguments : waveform Tensor - Tensor of audio of dimension (..., time). Returns : Tensor - Dimension (..., freq, time), where freq is n_fft // 2 + 1 where n_fft is the number of Fourier bins, and time is the number of window hops (n_frame). hannah.optim.madgrad MADGRAD Objects class MADGRAD ( torch . optim . Optimizer ) MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. .. _MADGRAD: https://arxiv.org/abs/2101.11075 MADGRAD is a general purpose optimizer that can be used in place of SGD or Adam may converge faster and generalize better. Currently GPU-only. Typically, the same learning rate schedule that is used for SGD or Adam may be used. The overall learning rate is not comparable to either method and should be determined by a hyper-parameter sweep. MADGRAD requires less weight decay than other methods, often as little as zero. Momentum values used for SGD or Adam's beta1 should work here also. On sparse problems both weight_decay and momentum should be set to 0. Arguments : params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate (default: 1e-2). momentum (float): Momentum value in the range [0,1) (default: 0.9). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6). step def step ( closure : Optional [ Callable [[], float ]] = None ) -> Optional [ float ] Performs a single optimization step. Arguments : closure callable, optional - A closure that reevaluates the model and returns the loss. hannah.optim hannah.optim.RAdam hannah.test_linear_classifier hannah.models.kakao_resnet hannah.models.simple1d hannah.models.sinc hannah.models.sinc.models GDSConv Objects class GDSConv ( nn . Module ) forward def forward ( x ) Arguments : x: GDSConvBlock Objects class GDSConvBlock ( nn . Module ) forward def forward ( x ) Arguments : x: SincNet Objects class SincNet ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models.ai8x hannah.models.ai8x.models A search space based on the cifar 10 NASNet search space for ai85x devices from: htt hannah.models.embedded_vision_net.operators hannah.models.embedded_vision_net.blocks hannah.models.embedded_vision_net.models hannah.models.embedded_vision_net.utils hannah.models.embedded_vision_net.expressions hannah.models.embedded_vision_net.parameters hannah.models.hello hannah.models.hello.models DSConv2d Objects class DSConv2d ( nn . Module ) forward def forward ( x ) Arguments : x: DSCNNSpeechModel Objects class DSCNNSpeechModel ( nn . Module ) forward def forward ( x ) Arguments : x: DNNSpeechModel Objects class DNNSpeechModel ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models.resnet.operators hannah.models.resnet.blocks hannah.models.resnet hannah.models.resnet.models hannah.models.resnet.expressions padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.models.resnet.models_lazy padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.models.lstm hannah.models.lstm.models LSTMModel Objects class LSTMModel ( nn . Module ) Simple LSTM model. forward def forward ( x ) Arguments : x: hannah.models._vendor.resnet_mc_dropout PyTorch ResNet This started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with additional dropout and dynamic global avg/max pool. ResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman Copyright 2019, Ross Wightman ResNet Objects class ResNet ( nn . Module ) ResNet / ResNeXt / SE-ResNeXt / SE-Net This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that * have > 1 stride in the 3x3 conv layer of bottleneck * have conv-bn-act ordering This ResNet impl supports a number of stem and downsample options based on the v1c, v1d, v1e, and v1s variants included in the MXNet Gluon ResNetV1b model. The C and D variants are also discussed in the 'Bag of Tricks' paper: https://arxiv.org/pdf/1812.01187. The B variant is equivalent to torchvision default. ResNet variants (the same modifications can be used in SE/ResNeXt models as well): * normal, b - 7x7 stem, stem_width = 64, same as torchvision ResNet, NVIDIA ResNet 'v1.5', Gluon v1b * c - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64) * d - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64), average pool in downsample * e - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128), average pool in downsample * s - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128) * t - 3 layer deep 3x3 stem, stem width = 32 (24, 48, 64), average pool in downsample * tn - 3 layer deep 3x3 stem, stem width = 32 (24, 32, 64), average pool in downsample ResNeXt * normal - 7x7 stem, stem_width = 64, standard cardinality and base widths * same c,d, e, s variants as ResNet can be enabled SE-ResNeXt * normal - 7x7 stem, stem_width = 64 * same c, d, e, s variants as ResNet can be enabled SENet-154 - 3 layer deep 3x3 stem (same as v1c-v1s), stem_width = 64, cardinality=64, reduction by 2 on width of first bottleneck convolution, 3x3 downsample convs after first block __init__ def __init__ ( block , layers , num_classes = 1000 , in_chans = 3 , output_stride = 32 , global_pool = \"avg\" , cardinality = 1 , base_width = 64 , stem_width = 64 , stem_type = \"\" , replace_stem_pool = False , block_reduce_first = 1 , down_kernel_size = 1 , avg_down = False , act_layer = nn . ReLU , norm_layer = nn . BatchNorm2d , aa_layer = None , drop_rate = 0.0 , drop_path_rate = 0.0 , drop_block_rate = 0.0 , zero_init_last = True , block_args = None ) Arguments : block nn.Module - class for the residual block. Options are BasicBlock, Bottleneck. layers (List[int]) : number of layers in each block num_classes int - number of classification classes (default 1000) in_chans int - number of input (color) channels. (default 3) output_stride int - output stride of the network, 32, 16, or 8. (default 32) global_pool str - Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax' (default 'avg') cardinality int - number of convolution groups for 3x3 conv in Bottleneck. (default 1) base_width int - bottleneck channels factor. planes * base_width / 64 * cardinality (default 64) stem_width int - number of channels in stem convolutions (default 64) stem_type str - The type of stem (default ''): '', default - a single 7x7 conv with a width of stem_width 'deep' - three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2 'deep_tiered' - three 3x3 conv layers of widths stem_width//4 * 3, stem_width, stem_width * 2 replace_stem_pool bool - replace stem max-pooling layer with a 3x3 stride-2 convolution block_reduce_first int - Reduction factor for first convolution output width of residual blocks, 1 for all archs except senets, where 2 (default 1) down_kernel_size int - kernel size of residual block downsample path, 1x1 for most, 3x3 for senets (default: 1) avg_down bool - use avg pooling for projection skip connection between stages/downsample (default False) act_layer str, nn.Module - activation layer norm_layer str, nn.Module - normalization layer aa_layer nn.Module - anti-aliasing layer drop_rate float - Dropout probability before classifier, for training (default 0.) drop_path_rate float - Stochastic depth drop-path rate (default 0.) drop_block_rate float - Drop block rate (default 0.) zero_init_last bool - zero-init the last weight in residual path (usually last BN affine weight) block_args dict - Extra kwargs to pass through to block module resnet10t_mc_dropout @register_model def resnet10t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-10-T model. resnet14t_mc_dropout @register_model def resnet14t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-14-T model. resnet18_mc_dropout @register_model def resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18 model. resnet18d_mc_dropout @register_model def resnet18d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18-D model. resnet34_mc_dropout @register_model def resnet34_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34 model. resnet34d_mc_dropout @register_model def resnet34d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34-D model. resnet26_mc_dropout @register_model def resnet26_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26 model. resnet26t_mc_dropout @register_model def resnet26t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26-T model. resnet26d_mc_dropout @register_model def resnet26d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26-D model. resnet50_mc_dropout @register_model def resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model. resnet50d_mc_dropout @register_model def resnet50d_mc_dropout ( pretrained = False , ** kwargs ) -> ResNet Constructs a ResNet-50-D model. resnet50t_mc_dropout @register_model def resnet50t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-T model. resnet101_mc_dropout @register_model def resnet101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101 model. resnet101d_mc_dropout @register_model def resnet101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model. resnet152_mc_dropout @register_model def resnet152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152 model. resnet152d_mc_dropout @register_model def resnet152d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152-D model. resnet200_mc_dropout @register_model def resnet200_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200 model. resnet200d_mc_dropout @register_model def resnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model. tv_resnet34_mc_dropout @register_model def tv_resnet34_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34 model with original Torchvision weights. tv_resnet50_mc_dropout @register_model def tv_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with original Torchvision weights. tv_resnet101_mc_dropout @register_model def tv_resnet101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101 model w/ Torchvision pretrained weights. tv_resnet152_mc_dropout @register_model def tv_resnet152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152 model w/ Torchvision pretrained weights. wide_resnet50_2_mc_dropout @register_model def wide_resnet50_2_mc_dropout ( pretrained = False , ** kwargs ) Constructs a Wide ResNet-50-2 model. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. wide_resnet101_2_mc_dropout @register_model def wide_resnet101_2_mc_dropout ( pretrained = False , ** kwargs ) Constructs a Wide ResNet-101-2 model. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same. resnet50_gn_mc_dropout @register_model def resnet50_gn_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model w/ GroupNorm resnext50_32x4d_mc_dropout @register_model def resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50-32x4d model. resnext50d_32x4d_mc_dropout @register_model def resnext50d_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample resnext101_32x4d_mc_dropout @register_model def resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x4d model. resnext101_32x8d_mc_dropout @register_model def resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x8d model. resnext101_64x4d_mc_dropout @register_model def resnext101_64x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt101-64x4d model. tv_resnext50_32x4d_mc_dropout @register_model def tv_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50-32x4d model with original Torchvision weights. ig_resnext101_32x8d_mc_dropout @register_model def ig_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/ ig_resnext101_32x16d_mc_dropout @register_model def ig_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/ ig_resnext101_32x32d_mc_dropout @register_model def ig_resnext101_32x32d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/ ig_resnext101_32x48d_mc_dropout @register_model def ig_resnext101_32x48d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/ ssl_resnet18_mc_dropout @register_model def ssl_resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNet-18 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ssl_resnet50_mc_dropout @register_model def ssl_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNet-50 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ssl_resnext50_32x4d_mc_dropout @register_model def ssl_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-50 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ssl_resnext101_32x4d_mc_dropout @register_model def ssl_resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ssl_resnext101_32x8d_mc_dropout @register_model def ssl_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x8 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ssl_resnext101_32x16d_mc_dropout @register_model def ssl_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x16 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnet18_mc_dropout @register_model def swsl_resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised Resnet-18 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnet50_mc_dropout @register_model def swsl_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNet-50 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnext50_32x4d_mc_dropout @register_model def swsl_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-50 32x4 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnext101_32x4d_mc_dropout @register_model def swsl_resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x4 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnext101_32x8d_mc_dropout @register_model def swsl_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x8 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ swsl_resnext101_32x16d_mc_dropout @register_model def swsl_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x16 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/ ecaresnet26t_mc_dropout @register_model def ecaresnet26t_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn. ecaresnet50d_mc_dropout @register_model def ecaresnet50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with eca. ecaresnet50d_pruned_mc_dropout @register_model def ecaresnet50d_pruned_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model pruned with eca. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf ecaresnet50t_mc_dropout @register_model def ecaresnet50t_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNet-50-T model. Like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn. ecaresnetlight_mc_dropout @register_model def ecaresnetlight_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D light model with eca. ecaresnet101d_mc_dropout @register_model def ecaresnet101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with eca. ecaresnet101d_pruned_mc_dropout @register_model def ecaresnet101d_pruned_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model pruned with eca. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf ecaresnet200d_mc_dropout @register_model def ecaresnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model with ECA. ecaresnet269d_mc_dropout @register_model def ecaresnet269d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-269-D model with ECA. ecaresnext26t_32x4d_mc_dropout @register_model def ecaresnext26t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem. This model replaces SE module with the ECA module ecaresnext50t_32x4d_mc_dropout @register_model def ecaresnext50t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-50-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem. This model replaces SE module with the ECA module seresnet200d_mc_dropout @register_model def seresnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model with SE attn. seresnet269d_mc_dropout @register_model def seresnet269d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-269-D model with SE attn. seresnext26d_32x4d_mc_dropout @register_model def seresnext26d_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNeXt-26-D model.` This is technically a 28 layer ResNet, using the 'D' modifier from Gluon / bag-of-tricks for combination of deep stem and avg_pool in downsample. seresnext26t_32x4d_mc_dropout @register_model def seresnext26t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNet-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem. seresnext26tn_32x4d_mc_dropout @register_model def seresnext26tn_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNeXt-26-T model. NOTE I deprecated previous 't' model defs and replaced 't' with 'tn', this was the only tn model of note so keeping this def for backwards compat with any uses out there. Old 't' model is lost. resnetblur18_mc_dropout @register_model def resnetblur18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18 model with blur anti-aliasing resnetblur50_mc_dropout @register_model def resnetblur50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with blur anti-aliasing resnetblur50d_mc_dropout @register_model def resnetblur50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with blur anti-aliasing resnetblur101d_mc_dropout @register_model def resnetblur101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with blur anti-aliasing resnetaa34d_mc_dropout @register_model def resnetaa34d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34-D model w/ avgpool anti-aliasing resnetaa50_mc_dropout @register_model def resnetaa50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with avgpool anti-aliasing resnetaa50d_mc_dropout @register_model def resnetaa50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with avgpool anti-aliasing resnetaa101d_mc_dropout @register_model def resnetaa101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with avgpool anti-aliasing seresnetaa50d_mc_dropout @register_model def seresnetaa50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE=ResNet-50-D model with avgpool anti-aliasing seresnextaa101d_32x8d_mc_dropout @register_model def seresnextaa101d_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing resnetrs50_mc_dropout @register_model def resnetrs50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-50 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs101_mc_dropout @register_model def resnetrs101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-101 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs152_mc_dropout @register_model def resnetrs152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-152 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs200_mc_dropout @register_model def resnetrs200_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-200 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs270_mc_dropout @register_model def resnetrs270_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-270 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs350_mc_dropout @register_model def resnetrs350_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-350 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs resnetrs420_mc_dropout @register_model def resnetrs420_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-420 model Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs hannah.models._vendor hannah.models._vendor.focalnet Mlp Objects class Mlp ( nn . Module ) forward def forward ( x ) Arguments : x: FocalModulation Objects class FocalModulation ( nn . Module ) forward def forward ( x ) Arguments : x - input features with shape of (B, H, W, C) extra_repr def extra_repr () -> str flops def flops ( N ) calculate flops for 1 window with token length of N Arguments : N: FocalNetBlock Objects class FocalNetBlock ( nn . Module ) Focal Modulation Network Block. Arguments : dim(int) - Number of input channels. input_resolution(tuple[int]) - Input resulotion. mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. drop(float) - Dropout rate. Default: 0.0 drop_path(float) - Stochastic depth rate. Default: 0.0 act_layer(nn.Module) - Activation layer. Default: nn.GELU norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm focal_level(int) - Number of focal levels. focal_window(int) - Focal window size at first focal level use_layerscale(bool) - Whether use layerscale layerscale_value(float) - Initial layerscale value use_postln(bool) - Whether use layernorm after modulation forward def forward ( x ) Arguments : x: extra_repr def extra_repr () -> str flops def flops () BasicLayer Objects class BasicLayer ( nn . Module ) A basic Focal Transformer layer for one stage. Arguments : dim(int) - Number of input channels. input_resolution(tuple[int]) - Input resolution. depth(int) - Number of blocks. window_size(int) - Local window size. mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. qkv_bias(bool) - If True, add a learnable bias to query, key, value. Default: True qk_scale(float | None): Override default qk scale of head_dim ** -0.5 if set. drop(float) - Dropout rate. Default: 0.0 drop_path(float | tuple[float]): Stochastic depth rate. Default: 0.0 norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm downsample(nn.Module | None): Downsample layer at the end of the layer. Default: None use_checkpoint(bool) - Whether to use checkpointing to save memory. Default: False. focal_level(int) - Number of focal levels focal_window(int) - Focal window size at first focal level use_layerscale(bool) - Whether use layerscale layerscale_value(float) - Initial layerscale value use_postln(bool) - Whether use layernorm after modulation forward def forward ( x , H , W ) Arguments : x: H: W: extra_repr def extra_repr () -> str flops def flops () PatchEmbed Objects class PatchEmbed ( nn . Module ) Image to Patch Embedding Arguments : img_size(int) - Image size. Default: 224. patch_size(int) - Patch token size. Default: 4. in_chans(int) - Number of input image channels. Default: 3. embed_dim(int) - Number of linear projection output channels. Default: 96. norm_layer(nn.Module) - Normalization layer. Default: None forward def forward ( x ) Arguments : x: flops def flops () FocalNet Objects class FocalNet ( nn . Module ) Focal Modulation Networks (FocalNets) Arguments : img_size(int | tuple(int): Input image size. Default 224 patch_size(int | tuple(int): Patch size. Default: 4 - in_chans(int) - Number of input image channels. Default: 3 - num_classes(int) - Number of classes for classification head. Default: 1000 - embed_dim(int) - Patch embedding dimension. Default: 96 - depths(tuple(int) - Depth of each Focal Transformer layer. - mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. Default: 4 - drop_rate(float) - Dropout rate. Default: 0 - drop_path_rate(float) - Stochastic depth rate. Default: 0.1 - norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm. - patch_norm(bool) - If True, add normalization after patch embedding. Default: True - use_checkpoint(bool) - Whether to use checkpointing to save memory. Default: False - focal_levels(list) - How many focal levels at all stages. Note that this excludes the finest-grain level. Default: [1, 1, 1, 1] - focal_windows(list) - The focal window size at all stages. Default: [7, 5, 3, 1] - use_conv_embed(bool) - Whether use convolutional embedding. We noted that using convolutional embedding usually improve the performance, but we do not use it by default. Default: False - use_layerscale(bool) - Whether use layerscale proposed in CaiT. Default: False - layerscale_value(float) - Value for layer scale. Default: 1e-4 - use_postln(bool) - Whether use layernorm after modulation (it helps stablize training of large models) no_weight_decay @torch . jit . ignore def no_weight_decay () no_weight_decay_keywords @torch . jit . ignore def no_weight_decay_keywords () forward_features def forward_features ( x ) Arguments : x: forward def forward ( x ) Arguments : x: flops def flops () build_transforms def build_transforms ( img_size , center_crop = False ) Arguments : img_size: - center_crop - (Default value = False) build_transforms4display def build_transforms4display ( img_size , center_crop = False ) Arguments : img_size: - center_crop - (Default value = False) focalnet_tiny_srf @register_model def focalnet_tiny_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_small_srf @register_model def focalnet_small_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_base_srf @register_model def focalnet_base_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_tiny_lrf @register_model def focalnet_tiny_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_small_lrf @register_model def focalnet_small_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_base_lrf @register_model def focalnet_base_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_tiny_iso_16 @register_model def focalnet_tiny_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_small_iso_16 @register_model def focalnet_small_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_base_iso_16 @register_model def focalnet_base_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_large_fl3 @register_model def focalnet_large_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_large_fl4 @register_model def focalnet_large_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_xlarge_fl3 @register_model def focalnet_xlarge_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_xlarge_fl4 @register_model def focalnet_xlarge_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_huge_fl3 @register_model def focalnet_huge_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: focalnet_huge_fl4 @register_model def focalnet_huge_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs: hannah.models.honk hannah.models.honk.model truncated_normal def truncated_normal ( tensor , std_dev = 0.01 ) Arguments : tensor: - std_dev - (Default value = 0.01) SpeechResModel Objects class SpeechResModel ( nn . Module ) forward def forward ( x ) Arguments : x: SpeechModel Objects class SpeechModel ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models hannah.models.tc hannah.models.tc.models create_act def create_act ( act , clipping_value ) Arguments : act: clipping_value: ApproximateGlobalAveragePooling1D Objects class ApproximateGlobalAveragePooling1D ( nn . Module ) forward def forward ( x ) Arguments : x: TCResidualBlock Objects class TCResidualBlock ( nn . Module ) forward def forward ( x ) Arguments : x: TCResNetModel Objects class TCResNetModel ( nn . Module ) forward def forward ( x ) Arguments : x: ExitWrapperBlock Objects class ExitWrapperBlock ( nn . Module ) forward def forward ( x ) Arguments : x: BranchyTCResNetModel Objects class BranchyTCResNetModel ( TCResNetModel ) on_val def on_val () on_val_end def on_val_end () on_test def on_test () on_test_end def on_test_end () reset_stats def reset_stats () print_stats def print_stats () forward def forward ( x ) Arguments : x: get_loss_function def get_loss_function () hannah.models.utils next_power_of2 def next_power_of2 ( x ) Arguments : x: hannah.models.objectdetection.loss bbox_iou def bbox_iou ( box1 , box2 , x1y1x2y2 = True , GIoU = False , DIoU = False , CIoU = False , eps = 1e-7 ) Arguments : box1: box2: - x1y1x2y2 - (Default value = True) - GIoU - (Default value = False) - DIoU - (Default value = False) - CIoU - (Default value = False) - eps - (Default value = 1e-7) is_parallel def is_parallel ( model ) Arguments : model: BCEBlurWithLogitsLoss Objects class BCEBlurWithLogitsLoss ( nn . Module ) Arguments : eps - (Default value = 0.1) ) - # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441# return positive: negative label smoothing BCE targetsreturn 1.0 - 0.5 * eps: 0.5 * epsclass BCEBlurWithLogitsLoss(nn.Module: ) - # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441# return positive: forward def forward ( pred , true ) Arguments : pred: true: FocalLoss Objects class FocalLoss ( nn . Module ) forward def forward ( pred , true ) Arguments : pred: true: QFocalLoss Objects class QFocalLoss ( nn . Module ) forward def forward ( pred , true ) Arguments : pred: true: ComputeLoss Objects class ComputeLoss () build_targets def build_targets ( p , targets ) Arguments : p: targets: hannah.models.objectdetection hannah.models.objectdetection.models FasterRCNN Objects class FasterRCNN ( torch . nn . Module ) forward def forward ( x , y = None ) Arguments : x: - y - (Default value = None) transformOutput def transformOutput ( cocoGt , output , x , y ) Arguments : cocoGt: output: x: y: UltralyticsYolo Objects class UltralyticsYolo ( torch . nn . Module ) forward def forward ( x , y = None ) Arguments : x: - y - (Default value = None) train def train ( mode = True ) Arguments : mode - (Default value = True) transformOutput def transformOutput ( cocoGt , output , x , y ) Arguments : cocoGt: output: x: y: hannah.models.factory.qat Import from new loacation for backwards compatibility hannah.models.factory.factory A neural network model factory It allows us to construct quantized and unquantized versions of the same network, allows to explore implementation alternatives using a common neural network construction interface. NormConfig Objects @dataclass class NormConfig () BNConfig Objects @dataclass class BNConfig ( NormConfig ) ActConfig Objects @dataclass class ActConfig () ELUConfig Objects @dataclass class ELUConfig ( ActConfig ) HardtanhConfig Objects @dataclass class HardtanhConfig ( ActConfig ) MinorBlockConfig Objects @dataclass class MinorBlockConfig () target target Operation parallel execute block in parallel with preceding block out_channels number of output channels kernel_size kernel size of this Operation (if applicable) stride stride for this operation use padding use padding for this operation (padding will always try to keep input dimensions / stride) dilation dilation factor to use for this operation groups number of groups for this operation norm normalization to use (true uses networks default configs) act activation to use (true uses default configs) upsampling Upsampling factor for mbconv layers bias use bias for this operation out_quant use output quantization for this operation MajorBlockConfig Objects @dataclass class MajorBlockConfig () stride Union[None, int, Tuple[int, ...], Tuple[int, ...]] last Indicates wether this block is the last reduction block LinearConfig Objects @dataclass class LinearConfig () norm Union[bool, NormConfig] act Union[bool, ActConfig] NetworkConfig Objects @dataclass class NetworkConfig () NetworkFactory Objects class NetworkFactory () act def act ( config : ActConfig ) -> nn . Module Arguments : config - ActConfig: config - ActConfig: conv2d def conv2d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : Union [ int , Tuple [ int , ... ]], stride : Union [ int , Tuple [ int , ... ]] = 1 , padding : Union [ int , Tuple [ int , ... ], bool ] = True , dilation : Union [ int , Tuple [ int , ... ]] = 0 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False , bias : bool = False ) -> None Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - Union[int: Tuple[int, ...]]: (Default value = 1) stride - Union[int: padding - Union[int: Tuple[int, ...]: bool] - (Default value = False) dilation - int: (Default value = 0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: bias - bool: (Default value = False) input_shape - Tuple[int: out_channels - int: kernel_size - Union[int: stride - Union[int: padding - Union[int: dilation - int: (Default value = 0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: bias - bool: (Default value = False) mbconv1d def mbconv1d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : int , dilation : int = 1 , stride : int = 1 , padding : Union [ int , bool ] = True , bias = False , upsampling : float = 1.0 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False ) Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - int: dilation - int: (Default value = 1) stride - int: (Default value = 1) padding - Union[int: bool] - (Default value = False) bias - (Default value = False) upsampling - float: (Default value = 1.0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: input_shape - Tuple[int: out_channels - int: kernel_size - int: dilation - int: (Default value = 1) stride - int: (Default value = 1) padding - Union[int: upsampling - float: (Default value = 1.0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: conv1d def conv1d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : int , stride : int = 1 , bias : bool = False , padding : Union [ int , bool ] = True , dilation : int = 1 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False , out_quant : bool = True ) -> Tuple [ Tuple [ int , ... ], nn . Module ] Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - int: stride - int: (Default value = 1) bias - bool: (Default value = False) padding - Union[int: bool] - (Default value = False) dilation - int: (Default value = 1) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: out_quant - bool: (Default value = True) input_shape - Tuple[int: out_channels - int: kernel_size - int: stride - int: (Default value = 1) bias - bool: (Default value = False) padding - Union[int: dilation - int: (Default value = 1) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: out_quant - bool: (Default value = True) minor def minor ( input_shape , config : MinorBlockConfig , major_stride = None ) Arguments : input_shape: - config - MinorBlockConfig: - major_stride - (Default value = None) - config - MinorBlockConfig: forward def forward ( input_shape : Tuple [ int , ... ], config : MajorBlockConfig ) Create a forward neural network block without parallelism If parallel is set to [True, False, True, False] Input: ------->|---> parallel: False ---> parallel: False ---> | --> output Arguments : input_shape - Tuple[int, ...]: config - MajorBlockConfig: input_shape - Tuple[int, ...]: config - MajorBlockConfig: residual def residual ( input_shape : Tuple [ int , ... ], config : MajorBlockConfig ) Create a neural network block with with residual parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---> parallel: True ---> | Input: ------->| +---> |---> parallel: False ---> parallel: False ---> | If the major block does change the output dimensions compared to the input and one of the branches does not contain any layers, we infer 1x1 conv of maximum group size (gcd (input_channels, output_channels)) to do the downsampling. Arguments : input_shape - Tuple[int, ...]: config - MajorBlockConfig: input_shape - Tuple[int, ...]: config - MajorBlockConfig: input def input ( in_channels : int , config : MajorBlockConfig ) Create a neural network block with input parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---> | |---> parallel: True ---> + -----------------> | Input:--------->| +---> |---> parallel: False ---> parallel: False ---> | If there are no parallel branches in the network. The major block is a standard feed forward layer. Arguments : in_channels - int: config - MajorBlockConfig: in_channels - int: config - MajorBlockConfig: full def full ( in_channels : int , config : MajorBlockConfig ) Create a neural network block with full parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---------------------------------- -| Input:--->| +---> | |--> parallel: False --->| | |---> parallel: False ----> | +--->| |--> parallel: True ---->| If there are no parallel blocks the block is a standard feed forward network. Arguments : in_channels - int: config - MajorBlockConfig: in_channels - int: config - MajorBlockConfig: major def major ( input_shape , config : MajorBlockConfig ) Arguments : input_shape: - config - MajorBlockConfig: - config - MajorBlockConfig: linear def linear ( input_shape , config : LinearConfig ) Arguments : input_shape: - config - LinearConfig: - config - LinearConfig: identity def identity () -> Identity network def network ( input_shape , labels : int , network_config : Union [ ListConfig , DictConfig ]) Arguments : input_shape: - labels - int: - network_config - NetworkConfig: - labels - int: - network_config - NetworkConfig: create_cnn def create_cnn ( input_shape : Sequence [ int ], labels : int , name : str , conv : Optional [ List [ MajorBlockConfig ]] = None , linear : Optional [ List [ LinearConfig ]] = None , norm : Optional [ NormConfig ] = None , act : Optional [ ActConfig ] = None , qconfig : Any = None , dropout : float = 0.5 ) Arguments : input_shape - Sequence[int]: labels - int: name - str: conv - Optional[List[MajorBlockConfig]]: (Default value = None) linear - Optional[List[LinearConfig]]: (Default value = None) norm - Optional[NormConfig]: (Default value = None) act - Optional[ActConfig]: (Default value = None) qconfig - Any: (Default value = None) dropout - float: (Default value = 0.5) input_shape - Sequence[int]: labels - int: name - str: conv - Optional[List[MajorBlockConfig]]: (Default value = None) linear - Optional[List[LinearConfig]]: (Default value = None) norm - Optional[NormConfig]: (Default value = None) act - Optional[ActConfig]: (Default value = None) qconfig - Any: (Default value = None) dropout - float: (Default value = 0.5) hannah.models.factory.rounding Import from new loacation for backwards compatibility hannah.models.factory.qconfig Import from new loacation for backwards compatibility hannah.models.factory.network ConvNet Objects class ConvNet ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models.factory hannah.models.factory.reduction ReductionBlockAdd Objects class ReductionBlockAdd ( nn . Module ) Reduction block that sums over its inputs forward def forward ( x : Tensor ) -> Tensor Arguments : x - Tensor: x - Tensor: ReductionBlockConcat Objects class ReductionBlockConcat ( nn . Module ) Reduction block that concatenates its inputs forward def forward ( x : Tensor ) -> Tensor Arguments : x - Tensor: x - Tensor: hannah.models.factory.pooling ApproximateGlobalAveragePooling1D Objects class ApproximateGlobalAveragePooling1D ( nn . Module ) A global average pooling layer, that divides by the next power of 2 instead of true number of elements forward def forward ( x ) Arguments : x: ApproximateGlobalAveragePooling2D Objects class ApproximateGlobalAveragePooling2D ( nn . Module ) A global average pooling layer, that divides by the next power of 2 instead of true number of elements forward def forward ( x ) Arguments : x: hannah.models.factory.act DummyActivation Objects class DummyActivation ( nn . Identity ) Dummy class that instantiated to mark a missing activation. This can be used to mark requantization of activations for convolutional layers without activation functions. Arguments : hannah.models.factory.quantized Import from new loacation for backwards compatibility hannah.models.wavenet hannah.models.wavenet.models Conv Objects class Conv ( nn . Module ) A convolution with the option to be causal and use xavier initialization forward def forward ( signal ) Arguments : signal: WaveNet Objects class WaveNet ( nn . Module ) forward def forward ( input_data ) Arguments : input_data: hannah.models.conv_vit.operators hannah.models.conv_vit.blocks hannah.models.conv_vit.attention hannah.models.conv_vit.models hannah.models.ekut hannah.models.ekut.models conv_bn def conv_bn ( inp , oup , stride ) Arguments : inp: oup: stride: conv_1x1_bn def conv_1x1_bn ( inp , oup ) Arguments : inp: oup: InvertedResidual Objects class InvertedResidual ( nn . Module ) forward def forward ( x ) Arguments : x: RawSpeechModel Objects class RawSpeechModel ( nn . Module ) Speech Recognition on RAW Data using Wolfgang Fuhls Networks forward def forward ( x ) Arguments : x: RawSpeechModelInvertedResidual Objects class RawSpeechModelInvertedResidual ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models.timm DefaultAnomalyDetector Objects class DefaultAnomalyDetector ( nn . Module ) forward def forward ( x ) Simple anomaly detection head for a neural network Arguments : x - Tensor of logits Returns - A single element floating point tensor representing the anomaly score DefaultClassifierHead Objects class DefaultClassifierHead ( nn . Module ) forward def forward ( x : torch . Tensor ) -> torch . Tensor Arguments : x (torch.Tensor): Returns : Resulting torch.Tensor after applying classification DefaultProjectionHead Objects class DefaultProjectionHead ( nn . Module ) Default projection head for semi supervised classification learning forward def forward ( x : torch . Tensor ) -> torch . Tensor Forward function for default Projection Head Arguments : x - Input tensor Returns : output tensor DefaultDecoderHead Objects class DefaultDecoderHead ( nn . Module ) __init__ def __init__ ( latent_shape , input_shape ) Default Decoder Head for autoencoders using TransposedConv2D Arguments : latent_shape(Tuple) - Shape (CxHxW) of the latent representation of the autoencoder input_shape(Tuple) - Shape (CxHxW) of the reconstructed image forward def forward ( x ) Arguments : x: TimmModel Objects class TimmModel ( nn . Module ) forward def forward ( x : torch . Tensor ) -> torch . Tensor Arguments : x - torch.Tensor: x - torch.Tensor: hannah.models.mobilenet.operators hannah.models.mobilenet.models hannah.models.mobilenet.expressions padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.models.vad hannah.models.vad.models BottleneckVad Objects class BottleneckVad ( nn . Module ) forward def forward ( x ) Arguments : x: num_flat_features def num_flat_features ( x ) Arguments : x: SmallVad Objects class SmallVad ( nn . Module ) forward def forward ( x ) Arguments : x: num_flat_features def num_flat_features ( x ) Arguments : x: SimpleVad Objects class SimpleVad ( nn . Module ) forward def forward ( x ) Arguments : x: num_flat_features def num_flat_features ( x ) Arguments : x: BottleneckVadModel Objects class BottleneckVadModel ( nn . Module ) forward def forward ( x ) Arguments : x: SimpleVadModel Objects class SimpleVadModel ( nn . Module ) forward def forward ( x ) Arguments : x: SmallVadModel Objects class SmallVadModel ( nn . Module ) forward def forward ( x ) Arguments : x: hannah.models.functional_net_test.models hannah.models.functional_net_test.expressions padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.models.convnet hannah.models.convnet.models padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.nas.dataflow.tensor_type hannah.nas.dataflow.tensor hannah.nas.dataflow.ops.identity hannah.nas.dataflow.ops.add hannah.nas.dataflow.ops.concat hannah.nas.dataflow.ops.dropout hannah.nas.dataflow.ops hannah.nas.dataflow.ops.pooling hannah.nas.dataflow.ops.linear hannah.nas.dataflow.ops.batch_nom hannah.nas.dataflow.ops.conv2d hannah.nas.dataflow.ops.relu hannah.nas.dataflow.ops.sum hannah.nas.dataflow.quantization_type hannah.nas.dataflow.dataflow_utils hannah.nas.dataflow.register_ops hannah.nas.dataflow.compression_type hannah.nas.dataflow.analysis.dataflow_analysis hannah.nas.dataflow.repeat hannah.nas.dataflow.op_type hannah.nas.dataflow.registry hannah.nas.dataflow hannah.nas.dataflow.data_type hannah.nas.dataflow.tensor_expression hannah.nas.dataflow.dataflow_graph DataFlowGraph Objects @parametrize class DataFlowGraph ( TensorExpression ) link_users def link_users () Link the DFG to its users and the users of the DFG to the DFG collect_users def collect_users ( node ) Traverse graph starting from node and collect all users (including users from subsequent nodes). If a node_b is NOT in collect_users(node_a), this means that node_b is either BEFORE node_a in the graph OR it is in a parallel branch. Parameters node : type description Returns type description find_first_input def find_first_input ( node ) Recusively traverses the graph from the given node back to its first input. NOTE: The traversal is via OPERANDS and not OUTPUT, meaning that e.g. weight Tensors that are included in Ops in a DFG are not returned Parameters node : type description Returns type description hannah.nas.dataflow.axis_type AxisTuple Objects @parametrize class AxisTuple () Used to have the axis dict as a parametrized object hannah.nas.dataflow.transformations.graph_tranformer hannah.nas.dataflow.optional_op hannah.nas.dataflow.scoping_utils hannah.nas.graph_conversion hannah.nas.expressions.choice hannah.nas.expressions.logic hannah.nas.expressions.shapes hannah.nas.expressions.op hannah.nas.expressions hannah.nas.expressions.conditions hannah.nas.expressions.utils hannah.nas.expressions.types hannah.nas.expressions.metrics hannah.nas.expressions.placeholder hannah.nas.expressions.arithmetic hannah.nas.ops hannah.nas.search.presampler.simple hannah.nas.search.search hannah.nas.search.weight_sharing hannah.nas.search.utils hannah.nas.search.model_trainer.simple_model_trainer hannah.nas.search.search_old AgingEvolution Objects class AgingEvolution () Aging Evolution based multi objective optimization next_parameters def next_parameters () Returns a list of current tasks tell_result def tell_result ( parameters , metrics ) Tell the result of a task hannah.nas.search.sampler.random_sampler hannah.nas.search.sampler.base_sampler Sampler Objects class Sampler ( ABC ) tell_result def tell_result ( parameters , metrics ) Tell the result of a task hannah.nas.search.sampler.mutator hannah.nas.search.sampler.aging_evolution AgingEvolutionSampler Objects class AgingEvolutionSampler ( Sampler ) Aging Evolution based multi objective optimization next_parameters def next_parameters () Returns a list of current tasks tell_result def tell_result ( parameters , metrics ) Tell the result of a task hannah.nas.search.sampler.defined_space_sampler hannah.nas.hardware_description.ultratrail hannah.nas.hardware_description.device hannah.nas.hardware_description hannah.nas.hardware_description.memory_type hannah.nas.hardware_description.description hannah.nas.functional_operators.torch_conversion hannah.nas.functional_operators.visualizer hannah.nas.functional_operators.operators self_attention2d @torch . fx . wrap def self_attention2d ( q , k , v , num_heads , d_model , * , id ) Arguments : q - Tensor, shape [B, h*d, H, W] k - Tensor, shape [B, h*d, H, W] v - Tensor, shape [B, h*d, H, W] hannah.nas.functional_operators.lazy hannah.nas.functional_operators.shapes padding_expression def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1. Parameters kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1 Returns Expression hannah.nas.functional_operators.op Bypass Objects class Bypass ( Op ) Alternative Identity() hannah.nas.functional_operators hannah.nas.functional_operators.executor hannah.nas.parameters.lazy hannah.nas.parameters hannah.nas.parameters.iterators hannah.nas.parameters.parametrize hannah.nas.parameters.parameters hannah.nas.eval.plot hannah.nas.eval.prepare prepare_summary def prepare_summary ( data : Dict [ str , str ], base_dir : str = \".\" , force : bool = False ) -> pd . DataFrame Prepare a summary of one or multiple nas runs Arguments : data Dict[str, str] - A mapping from a short name for a nas run \"e.g KWS\" to a folder containing nas history file e.g. \"trained_models/nas_kws/conv_net_trax\" base_dir str - base directory paths in data mapping are interpreted relative to base directory force bool - force reconstructing of cached results (\"data.pkl\") hannah.nas.eval hannah.nas.eval.__main__ hannah.nas.eval.extract hannah.nas.config Scalar Objects @dataclass class Scalar () Representation of all the options to define a scalar. hannah.nas hannah.nas.spaces.darts.darts_space hannah.nas.spaces.mobilenet.mobilenet hannah.nas.test.test_functional_training hannah.nas.test.test_graph_transformer hannah.nas.test.test_description_ultratrail hannah.nas.test.test_op_to_torch_conversion hannah.nas.test.test_onnx_export hannah.nas.test.test_lazy_torch hannah.nas.test.test_add hannah.nas.test.network hannah.nas.test.test_arithmetic test_unimplemeted @pytest . mark . parametrize ( \"x,y\" , [ ( IntScalarParameter ( 0 , 0 ), 2 ), ( IntScalarParameter ( 0 , 0 ), IntScalarParameter ( 0 , 0 )), ( DefaultInt ( 0 ), 2 ), ], ) def test_unimplemeted ( x , y ) Test that unimplemented methods raise unimplemented errors hannah.nas.test.test_constraint_model hannah.nas.test.test_parameters hannah.nas.test.test_parametrize hannah.nas.test.test_adjacency hannah.nas.test.test_conditions hannah.nas.test.test_z3 hannah.nas.test.test_scoping hannah.nas.test.test_functional_ops conv_relu def conv_relu ( input , out_channels , kernel_size , stride ) Example for a functional block containing conv and relu hannah.nas.test.test_conv2d hannah.nas.test.test_dataflow hannah.nas.test.test_operators hannah.nas.test.test_darts_space hannah.nas.test.test_mobilenet hannah.nas.test.test_functional_executor hannah.nas.test.test_dfg_removal hannah.nas.test.test_repeat hannah.nas.utils is_pareto def is_pareto ( costs , maximise = False ) Arguments : costs : An (n_points, n_costs) array Returns : A (n_points, ) boolean array, indicating whether each point is Pareto efficient hannah.nas.__main__ hannah.nas.constraints.random_walk hannah.nas.constraints.constraint_model hannah.nas.constraints.dfg_constraint_model ConstraintModel Objects class ConstraintModel () process_optype def process_optype ( op : OpType ) Extracts the constraints based on the type of op. New variables are added to self.vars and the constraints are added to the solver. Parameters op : OpType process_tensor def process_tensor ( tensor : Tensor ) Goes through all axis and extracts the constraints for the respective axis sizes Parameters tensor : Tensor hannah.nas.parametrization ScalarParameterState Objects @dataclass class ScalarParameterState ( ParameterState ) sigma variance of sampling parameter hannah.nas.core.parametrized hannah.nas.core hannah.nas.core.expression hannah.nas.fx.tracer InliningTracer Objects class InliningTracer ( SearchSpaceTracer ) Inline all search space functions, into the graph. This generates a standard pytorch.fx graph module containing only replacing the search space parametrizable functions with their equivalent form torch.functional` hannah.nas.export.onnx eval def eval ( exp_tree : Any ) -> Any Recursively evaluate expressions organized as a pytree hannah.nas.export hannah.nas.plotter hannah.nas.performance_prediction.examples.gcn_predictor_example hannah.nas.performance_prediction.examples.ri_capsule_performance_predictor hannah.nas.performance_prediction.examples.gaussian_process_predictor_example hannah.nas.performance_prediction.examples.xgb_predictor_example hannah.nas.performance_prediction.examples.gcn_model_example hannah.nas.performance_prediction.gcn.predictor Predictor Objects class Predictor () __init__ def __init__ ( fea_name = \"features\" ) -> None Parent method for different predictor classes. Parameters fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' train def train ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 1 ) Train GCN model Parameters dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch, by default 1 predict def predict ( graph ) predict cost of graph Parameters graph : dgl.Graph Returns torch.Tensor predicted cost of given graph. Retrieve float value with .item() GCNPredictor Objects class GCNPredictor ( Predictor ) __init__ def __init__ ( input_feature_size , hidden_units = [ 128 ], readout = \"mean\" , fea_name = \"features\" ) -> None G(raph)CN based network latency/cost predictor. End-to-end from graph to score. Parameters input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' train_and_fit def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 0 ) Train GCN model Parameters dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch, by default 1 predict def predict ( graph ) predict cost of graph Parameters graph : dgl.Graph Returns torch.Tensor predicted cost of given graph. Retrieve float value with .item() GaussianProcessPredictor Objects class GaussianProcessPredictor ( Predictor ) __init__ def __init__ ( input_feature_size , hidden_units = 128 , embedding_size = 10 , readout = \"mean\" , fea_name = \"features\" , kernel = \"default\" , alpha = 1e-10 ) -> None Predictor that generates a graph embedding that is used as input for a gaussian process predictor. Parameters input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 embedding_size: int, optional size of output embedding readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' kernel : str, sklearn.gaussian_process.kernels.Kernel, optional The gaussian process kernel to use. input shoudl be either \"default\", or a sklearn Kernel() object by default RBF() + DotProduct() + WhiteKernel() train_and_fit def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 1 ) Train GCN model, generate embeddings for training data and fit the predictor with embeddings. Parameters dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch,by default 1 Returns float score of predictor on TRAINING data, see sklearn doc of chosen predictor for more info predict def predict ( X , return_std = True ) Predict cost/latency of graphs. Parameters X : dgl.DGLGraph, list[DGLGraph], dgl.dataloading.GraphDataLoader Input graph(s) return_std : bool, optional if true, return standard dev. else just mean prediction, by default True Returns array (,array) prediction(s) , (if return_std: standard deviation(s)) XGBPredictor Objects class XGBPredictor ( Predictor ) __init__ def __init__ ( input_feature_size , hidden_units = 128 , embedding_size = 10 , readout = \"mean\" , fea_name = \"features\" , xgb_param = \"default\" ) -> None Predictor that generates a graph embedding that is used as input for a xgb based predictor. Parameters input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 embedding_size: int, optional size of output embedding readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' xgb_param : str, dict, optional The xgb_parameter to use. See https://xgboost.readthedocs.io/en/latest/parameter.html train_and_fit def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , num_round = 8000 , validation_dataloader = None , verbose = 1 ) Train GCN model, generate embeddings for training data and fit the predictor with embeddings. Parameters dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional Training epochs for the GCN embedding network, by default 200 num_round : int, optional training rounds for xgb booster, by default 800 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch,by default 1 predict def predict ( X ) Predict cost/latency of graphs. Parameters X : dgl.DGLGraph, list[DGLGraph], dgl.dataloading.GraphDataLoader Input graph(s) Returns array (,array) prediction(s) , (if return_std: standard deviation(s)) prepare_dataloader def prepare_dataloader ( dataset , batch_size = 50 , train_test_split = 1 , subset = 0 , seed = 0 , validation = False ) helper function to construct dataloaders from NASGraphDataset Parameters dataset : NASGraphDataset batch_size : int, optional by default 50 train_test_split : float, optional number between 0 and 1, the proportion of the dataset to be used for training, by default 1 subset : int, optional choose only many samples from the dataset. Set 0 for disabling, i.e. whole dataset. by default 0 seed : int, optional set seed for reproduceability validation : bool, optional also output a validation set e.g. for hyperparam tuning Returns tuple(GraphDataLoader, (GraphDataLoader), GraphDataLoader) training dataloader to be used in CostPredictor.train() and test/validation dataloader if train_test_split > 0, else len(test_dataloader) == 0 hannah.nas.performance_prediction.gcn.model hannah.nas.performance_prediction hannah.nas.performance_prediction.features.graph_conversion hannah.nas.performance_prediction.features.dataset hannah.nas.performance_prediction.simple MACPredictor Objects class MACPredictor () A predictor class that instantiates the model and calculates abstract metrics GCNPredictor Objects class GCNPredictor () A predictor class that instantiates the model and uses the backends predict function to predict performance metrics hannah.nas.performance_prediction.mlonmcu.predictor hannah.nas.performance_prediction.mlonmcu hannah.nas.performance_prediction.protocol Predictor Objects class Predictor ( Protocol ) predict def predict ( model : ClassifierModule , input : Optional [ InputShape ] = None ) -> Mapping [ str , float ] Pedicts performance metrisc of a model. Performance metrics are returned as a dictionary with the metric name as key and the metric value as floating point value. Arguments : model ClassifierModule - The model to predict the performance of. input __type_, optional_ - Input shape of input . Defaults to None.","title":"API Documentation"},{"location":"development/devel/api/#hannahconfscheduler","text":"","title":"hannah.conf.scheduler"},{"location":"development/devel/api/#onecyclelrconf-objects","text":"@dataclass class OneCycleLRConf () Config for one cycle lr total steps are configured from module","title":"OneCycleLRConf Objects"},{"location":"development/devel/api/#hannahconf","text":"","title":"hannah.conf"},{"location":"development/devel/api/#hannahconfoptimizer","text":"","title":"hannah.conf.optimizer"},{"location":"development/devel/api/#sgdconf-objects","text":"@dataclass class SGDConf ()","title":"SGDConf Objects"},{"location":"development/devel/api/#lr","text":"_RequiredParameter","title":"lr"},{"location":"development/devel/api/#madgradconf-objects","text":"@dataclass class MADGRADConf ()","title":"MADGRADConf Objects"},{"location":"development/devel/api/#lr_1","text":"_RequiredParameter","title":"lr"},{"location":"development/devel/api/#hannahconfnas","text":"","title":"hannah.conf.nas"},{"location":"development/devel/api/#hannahtrainercross_validation","text":"","title":"hannah.trainer.cross_validation"},{"location":"development/devel/api/#hannahtrainer","text":"","title":"hannah.trainer"},{"location":"development/devel/api/#hannahtoolstrain","text":"","title":"hannah.tools.train"},{"location":"development/devel/api/#hannahtools","text":"","title":"hannah.tools"},{"location":"development/devel/api/#hannahtoolscharacterize","text":"","title":"hannah.tools.characterize"},{"location":"development/devel/api/#main","text":"@hydra . main ( config_name = \"characterize\" , config_path = \"../conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig:","title":"main"},{"location":"development/devel/api/#hannahtoolseval","text":"","title":"hannah.tools.eval"},{"location":"development/devel/api/#eval_checkpoint","text":"def eval_checkpoint ( config : DictConfig , checkpoint ) -> None Arguments : config - DictConfig: checkpoint: config - DictConfig: config - DictConfig:","title":"eval_checkpoint"},{"location":"development/devel/api/#eval","text":"def eval ( config : DictConfig ) -> Optional [ bool ] Arguments : config - DictConfig: config - DictConfig: config - DictConfig:","title":"eval"},{"location":"development/devel/api/#main_1","text":"@hydra . main ( config_name = \"eval\" , config_path = \"conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig:","title":"main"},{"location":"development/devel/api/#hannahtoolsobjectdetection_eval","text":"","title":"hannah.tools.objectdetection_eval"},{"location":"development/devel/api/#eval_train","text":"def eval_train ( config , module , test = True ) Arguments : config - param module: test - Default value = True) module:","title":"eval_train"},{"location":"development/devel/api/#eval_steps","text":"def eval_steps ( config , module , hparams , checkpoint ) Arguments : config - param module: hparams - param checkpoint: module: checkpoint:","title":"eval_steps"},{"location":"development/devel/api/#eval_checkpoint_1","text":"def eval_checkpoint ( config : DictConfig , checkpoint ) Arguments : config - DictConfig: checkpoint: config - DictConfig: config - DictConfig:","title":"eval_checkpoint"},{"location":"development/devel/api/#eval_1","text":"def eval ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig:","title":"eval"},{"location":"development/devel/api/#main_2","text":"@hydra . main ( config_name = \"objectdetection_eval\" , config_path = \"../conf\" , version_base = \"1.2\" ) def main ( config : DictConfig ) Arguments : config - DictConfig: config - DictConfig: config - DictConfig:","title":"main"},{"location":"development/devel/api/#hannahmodulesvisionbase","text":"","title":"hannah.modules.vision.base"},{"location":"development/devel/api/#hannahmodulesvisionanomaly_detection","text":"","title":"hannah.modules.vision.anomaly_detection"},{"location":"development/devel/api/#anomalydetectionmodule-objects","text":"class AnomalyDetectionModule ( VisionBaseModule )","title":"AnomalyDetectionModule Objects"},{"location":"development/devel/api/#on_test_end","text":"def on_test_end () wd_dir = os.getcwd() score, largest_train_error = self.compute_anomaly_score() train_errors = self.normalized_train_errors plt.hist(train_errors.detach().cpu().numpy(), bins=100) plt.axvline(score, linestyle=\"dashed\") plt.title(\"Normalized train reconstruction errors\") plt.savefig(wd_dir + \"/normalized_train_errors.png\") test = ( torch.tensor(self.test_losses, device=self.device) / torch.max(torch.stack(self.train_losses), dim=0).values ) plt.hist(test.detach().cpu().numpy(), bins=100) plt.title(\"Normalized test reconstruction errors\") plt.savefig(wd_dir + \"/normalized_test_errors.png\") print(\"Anomaly score\", score) print( \"Largest train error\", torch.max(torch.stack(self.train_losses), dim=0).values, )","title":"on_test_end"},{"location":"development/devel/api/#hannahmodulesvisionimage_classifier","text":"","title":"hannah.modules.vision.image_classifier"},{"location":"development/devel/api/#hannahmodulesvisionloss","text":"","title":"hannah.modules.vision.loss"},{"location":"development/devel/api/#hannahmodulesvisionanomaly_score","text":"class AnomalyScore(CatMetric): def init (self, percentile, nan_strategy=\"warn\", kwargs): super(). init (nan_strategy=nan_strategy, kwargs) self.percentile = percentile def compute(self): anomaly_score = None train_losses = super().compute() if train_losses: normalized_train_errors = torch.stack(train_losses) / ( torch.max(torch.stack(train_losses), dim=0).values ) anomaly_score = np.percentile( normalized_train_errors.cpu().numpy(), self.percentile ) return anomaly_score","title":"hannah.modules.vision.anomaly_score"},{"location":"development/devel/api/#hannahmodulesvision","text":"","title":"hannah.modules.vision"},{"location":"development/devel/api/#hannahmodulesbase","text":"","title":"hannah.modules.base"},{"location":"development/devel/api/#classifiermodule-objects","text":"class ClassifierModule ( LightningModule , ABC )","title":"ClassifierModule Objects"},{"location":"development/devel/api/#total_training_steps","text":"def total_training_steps () -> int Total training steps inferred from datamodule and devices.","title":"total_training_steps"},{"location":"development/devel/api/#hannahmodulesaugmentationaugmentation","text":"","title":"hannah.modules.augmentation.augmentation"},{"location":"development/devel/api/#hannahmodulesaugmentationtransformskornia_transforms","text":"","title":"hannah.modules.augmentation.transforms.kornia_transforms"},{"location":"development/devel/api/#hannahmodulesaugmentationtransformsregistry","text":"","title":"hannah.modules.augmentation.transforms.registry"},{"location":"development/devel/api/#hannahmodulesaugmentationtransforms","text":"","title":"hannah.modules.augmentation.transforms"},{"location":"development/devel/api/#hannahmodulesaugmentationbordersearch","text":"","title":"hannah.modules.augmentation.bordersearch"},{"location":"development/devel/api/#hannahmodulesaugmentationbatch_augmentation","text":"","title":"hannah.modules.augmentation.batch_augmentation"},{"location":"development/devel/api/#batchaugmentationpipeline-objects","text":"class BatchAugmentationPipeline ( nn . Module )","title":"BatchAugmentationPipeline Objects"},{"location":"development/devel/api/#__init__","text":"def __init__ ( transforms = {}) Augmentation pipeline especially for self supervised learning Arguments : replica int - number of replicated different random augmentations transforms dict - configuration of transforms","title":"__init__"},{"location":"development/devel/api/#forward","text":"@torch . no_grad () def forward ( x ) -> torch . Tensor Perform Augmentations Arguments : x torch.Tensor - a torch.Tensor representing the augementation pipeline Returns : Tuple[torch.Tensor, torch.Tensor]; Batch augmented with replica different random augmentations","title":"forward"},{"location":"development/devel/api/#hannahmodulesaugmentation","text":"","title":"hannah.modules.augmentation"},{"location":"development/devel/api/#hannahmodulesconfig_utils","text":"","title":"hannah.modules.config_utils"},{"location":"development/devel/api/#dump_config","text":"def dump_config ( output_dir , config ) Dumps the configuration to json format Creates file config.json in output_dir","title":"dump_config"},{"location":"development/devel/api/#parameters","text":"output_dir : str Output directory config : dict Configuration to dump","title":"Parameters"},{"location":"development/devel/api/#save_model","text":"def save_model ( output_dir , model ) Creates serialization of the model for later inference, evaluation Creates the following files: model.pt: Serialized version of network parameters in pytorch model.json: Serialized version of network parameters in json format model.onnx: full model including paramters in onnx format","title":"save_model"},{"location":"development/devel/api/#parameters_1","text":"output_dir : str Directory to put serialized models model : LightningModule Model to serialize","title":"Parameters"},{"location":"development/devel/api/#hannahmodulesclassifier","text":"","title":"hannah.modules.classifier"},{"location":"development/devel/api/#hannahmodulesangle_classifier","text":"","title":"hannah.modules.angle_classifier"},{"location":"development/devel/api/#hannahmodules","text":"","title":"hannah.modules"},{"location":"development/devel/api/#hannahmodulesobject_detection","text":"","title":"hannah.modules.object_detection"},{"location":"development/devel/api/#hannahmodulesmetrics","text":"","title":"hannah.modules.metrics"},{"location":"development/devel/api/#error-objects","text":"class Error () Computes Error = 1 - Accuracy_ .. math:: \\text{Error} = 1 - \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. This module is a simple wrapper to get the task specific versions of this metric, which is done by setting the task argument to either 'binary' , 'multiclass' or multilabel . See the documentation of :mod: BinaryError , :mod: MulticlassError and :mod: MultilabelError for the specific details of each argument influence and examples.","title":"Error Objects"},{"location":"development/devel/api/#plot_confusion_matrix","text":"def plot_confusion_matrix ( cf , group_names = None , categories = \"auto\" , count = True , percent = True , cbar = True , xyticks = True , xyplotlabels = True , sum_stats = True , figsize = None , cmap = \"Blues\" , title = None ) This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.","title":"plot_confusion_matrix"},{"location":"development/devel/api/#arguments","text":"cf: confusion matrix to be passed in group_names: List of strings that represent the labels row by row to be shown in each square. categories: List of strings containing the categories to be displayed on the x,y axis. Default is 'auto' count: If True, show the raw number in the confusion matrix. Default is True. normalize: If True, show the proportions for each category. Default is True. cbar: If True, show the color bar. The cbar values are based off the values in the confusion matrix. Default is True. xyticks: If True, show x and y ticks. Default is True. xyplotlabels: If True, show 'True Label' and 'Predicted Label' on the figure. Default is True. sum_stats: If True, display summary statistics below the figure. Default is True. figsize: Tuple representing the figure size. Default will be the matplotlib rcParams value. cmap: Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues' See http://matplotlib.org/examples/color/colormaps_reference.html title: Title for the heatmap. Default is None.","title":"Arguments"},{"location":"development/devel/api/#hannahsslhard_labeling","text":"","title":"hannah.ssl.hard_labeling"},{"location":"development/devel/api/#hardlabeling-objects","text":"class HardLabeling ()","title":"HardLabeling Objects"},{"location":"development/devel/api/#training_step","text":"def training_step ( unlabeled_data : torch . Tensor , trainer : pl . Trainer , pl_module : pl . LightningModule , batch_idx : int = - 1 ) -> torch . Tensor Calculate pseudo label loss from unlabeled data.","title":"training_step"},{"location":"development/devel/api/#get_dropout_layers","text":"def get_dropout_layers () Returns all model layers of class dropout or dropblock.","title":"get_dropout_layers"},{"location":"development/devel/api/#compute_loss","text":"def compute_loss ( inputs , logits , targets , loss_fn = None ) Helper function to compute loss, possibly with consistency regularization by augmentations (FixMatch).","title":"compute_loss"},{"location":"development/devel/api/#negative_cre_loss","text":"def negative_cre_loss ( logits , targets ) Cross Entropy Loss for negative learning which requires a mutli- class and multi-label loss function.","title":"negative_cre_loss"},{"location":"development/devel/api/#hannahssl","text":"","title":"hannah.ssl"},{"location":"development/devel/api/#hannahnormalizer","text":"","title":"hannah.normalizer"},{"location":"development/devel/api/#fixedpointnormalizer-objects","text":"class FixedPointNormalizer ( nn . Module ) Simple feature normalizer for fixed point models","title":"FixedPointNormalizer Objects"},{"location":"development/devel/api/#adaptivefixedpointnormalizer-objects","text":"class AdaptiveFixedPointNormalizer ( nn . Module ) Simple feature normalizer for fixed point models","title":"AdaptiveFixedPointNormalizer Objects"},{"location":"development/devel/api/#hannahquantizationrounding","text":"","title":"hannah.quantization.rounding"},{"location":"development/devel/api/#round_downward","text":"def round_downward ( x : Tensor ) -> Tensor Round to nearest upward","title":"round_downward"},{"location":"development/devel/api/#round_upward","text":"def round_upward ( x : Tensor ) -> Tensor Round to nearest downward","title":"round_upward"},{"location":"development/devel/api/#round_odd","text":"def round_odd ( x : Tensor ) -> Tensor Round to nearest odd","title":"round_odd"},{"location":"development/devel/api/#round_even","text":"def round_even ( x : Tensor ) -> Tensor Round to nearest even","title":"round_even"},{"location":"development/devel/api/#round_zero","text":"def round_zero ( x : Tensor ) -> Tensor Round towards zero","title":"round_zero"},{"location":"development/devel/api/#round_infinity","text":"def round_infinity ( x : Tensor ) -> Tensor Round toward infinity","title":"round_infinity"},{"location":"development/devel/api/#truncate_up","text":"def truncate_up ( x : Tensor ) -> Tensor Always round up to next integer","title":"truncate_up"},{"location":"development/devel/api/#truncate_down","text":"def truncate_down ( x : Tensor ) -> Tensor Always round down to next integer","title":"truncate_down"},{"location":"development/devel/api/#truncate_infinity","text":"def truncate_infinity ( x : Tensor ) -> Tensor Always round to next integer in direction infinity","title":"truncate_infinity"},{"location":"development/devel/api/#truncate_zero","text":"def truncate_zero ( x : Tensor ) -> Tensor Always round to next integer in direction of Zero","title":"truncate_zero"},{"location":"development/devel/api/#round_stochastic","text":"def round_stochastic ( x : Tensor ) -> Tensor Round stochastically","title":"round_stochastic"},{"location":"development/devel/api/#hannahquantizationqconfig","text":"","title":"hannah.quantization.qconfig"},{"location":"development/devel/api/#hannahquantizationcallback","text":"","title":"hannah.quantization.callback"},{"location":"development/devel/api/#hannahtrain","text":"","title":"hannah.train"},{"location":"development/devel/api/#hannahnnqat","text":"Implementations of torch.nn.intrinsics qat with an optional quantize bias parameter. Qconfigs can support an optional bias quantization funciton which should be returned by qconfig.bias() else biases will be quantized with qconfig.activation()","title":"hannah.nn.qat"},{"location":"development/devel/api/#_convbnnd-objects","text":"class _ConvBnNd ( nn . modules . conv . _ConvNd , _ConvForwardMixin )","title":"_ConvBnNd Objects"},{"location":"development/devel/api/#train","text":"def train ( mode : bool = True ) -> Any Batchnorm's training behavior is using the self.training flag. Prevent changing it if BN is frozen. This makes sure that calling model.train() on a model with a frozen BN will behave properly.","title":"train"},{"location":"development/devel/api/#from_float","text":"@classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user","title":"from_float"},{"location":"development/devel/api/#convbn1d-objects","text":"class ConvBn1d ( _ConvBnNd ) A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv1d and :class: torch.nn.BatchNorm1d . Similar to :class: torch.nn.Conv1d , with FakeQuantize modules initialized to default. Attributes : freeze_bn: - weight_fake_quant - fake quant module for weight","title":"ConvBn1d Objects"},{"location":"development/devel/api/#convbnrelu1d-objects","text":"class ConvBnReLU1d ( ConvBn1d ) A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv1d and :class: torch.nn.BatchNorm1d and :class: torch.nn.ReLU . Similar to torch.nn.Conv1d , with FakeQuantize modules initialized to default. Attributes : weight_fake_quant - fake quant module for weight","title":"ConvBnReLU1d Objects"},{"location":"development/devel/api/#convbn2d-objects","text":"class ConvBn2d ( _ConvBnNd ) A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv2d and :class: torch.nn.BatchNorm2d . Similar to :class: torch.nn.Conv2d , with FakeQuantize modules initialized to default. Attributes : freeze_bn: - weight_fake_quant - fake quant module for weight","title":"ConvBn2d Objects"},{"location":"development/devel/api/#convbnrelu2d-objects","text":"class ConvBnReLU2d ( ConvBn2d ) A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of :class: torch.nn.Conv2d and :class: torch.nn.BatchNorm2d and :class: torch.nn.ReLU . Similar to torch.nn.Conv2d , with FakeQuantize modules initialized to default. Attributes : weight_fake_quant - fake quant module for weight","title":"ConvBnReLU2d Objects"},{"location":"development/devel/api/#convrelu2d-objects","text":"class ConvReLU2d ( nn . Conv2d , _ConvForwardMixin ) A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. We combined the interface of :class: ~torch.nn.Conv2d and :class: ~torch.nn.BatchNorm2d . Attributes : weight_fake_quant - fake quant module for weight","title":"ConvReLU2d Objects"},{"location":"development/devel/api/#convrelu1d-objects","text":"class ConvReLU1d ( nn . Conv1d , _ConvForwardMixin ) A ConvReLU1d module is fused module of Conv1d and ReLU, attached with FakeQuantize modules for quantization aware training","title":"ConvReLU1d Objects"},{"location":"development/devel/api/#conv1d-objects","text":"class Conv1d ( nn . Conv1d , _ConvForwardMixin ) A Conv1d module is a Conv1d module , attached with FakeQuantize modules for weight for quantization aware training. Attributes : weight_fake_quant - fake quant module for weight bias_fake_quant - fake quant module for bias activation_post_process - fake_quant_module for activations","title":"Conv1d Objects"},{"location":"development/devel/api/#conv2d-objects","text":"class Conv2d ( nn . Conv2d , _ConvForwardMixin ) A Conv2d module is a Conv2d module , attached with FakeQuantize modules for weight for quantization aware training. Attributes : weight_fake_quant - fake quant module for weight bias_fake_quant - fake quant module for bias activation_post_process - fake_quant_module for activations","title":"Conv2d Objects"},{"location":"development/devel/api/#linear-objects","text":"class Linear ( nn . Linear ) A linear module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear , with FakeQuantize modules initialized to default. Attributes : weight - fake quant module for weight","title":"Linear Objects"},{"location":"development/devel/api/#from_float_1","text":"@classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user","title":"from_float"},{"location":"development/devel/api/#linearrelu-objects","text":"class LinearReLU ( nn . Linear ) A linear module attached with FakeQuantize modules and ReLU for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear , with FakeQuantize modules initialized to default. Attributes : weight - fake quant module for weight","title":"LinearReLU Objects"},{"location":"development/devel/api/#from_float_2","text":"@classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user","title":"from_float"},{"location":"development/devel/api/#identity-objects","text":"class Identity ( nn . Identity ) A identity module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Identity , please see https://pytorch.org/docs/stable/nn.html#torch.nn.Identity for documentation. Similar to torch.nn.Identity , with FakeQuantize modules initialized to default.","title":"Identity Objects"},{"location":"development/devel/api/#from_float_3","text":"@classmethod def from_float ( cls , mod ) Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user","title":"from_float"},{"location":"development/devel/api/#hannahnnquantized","text":"","title":"hannah.nn.quantized"},{"location":"development/devel/api/#hannahbackendstensorrt","text":"","title":"hannah.backends.tensorrt"},{"location":"development/devel/api/#tensorrtbackend-objects","text":"class TensorRTBackend ( InferenceBackendBase )","title":"TensorRTBackend Objects"},{"location":"development/devel/api/#output_spec","text":"def output_spec () Get the specs for the output tensor of the network. Useful to prepare memory allocations. Returns : Two items, the shape of the output tensor and its (numpy) datatype.","title":"output_spec"},{"location":"development/devel/api/#hannahbackendsbase","text":"","title":"hannah.backends.base"},{"location":"development/devel/api/#profilingresult-objects","text":"class ProfilingResult ( NamedTuple ) Result of a profiling run Attributes : outputs - the outputs of the model on the given input batch metrics - a dictionary containing the combined metrics obtained from the profiling run profile - the raw profile in a backend-specific format","title":"ProfilingResult Objects"},{"location":"development/devel/api/#abstractbackend-objects","text":"class AbstractBackend ( ABC )","title":"AbstractBackend Objects"},{"location":"development/devel/api/#prepare","text":"@abstractmethod def prepare ( module : ClassifierModule ) Prepare the model for execution on the target device Arguments : module - the classifier module to be exported","title":"prepare"},{"location":"development/devel/api/#run","text":"@abstractmethod def run ( * inputs ) -> Union [ torch . Tensor , Sequence [ torch . Tensor ]] Run a batch on the target device Arguments : inputs - a list of torch tensors representing the inputs to be run on the target device, each tensor represents a whole batched input, so for models taking 1 parameter, the list will contain 1 tensor of shape (batch_size, *input_shape) Returns - the output(s) of the model as a torch tensor or a Sequence of torch tensors for models producing multiple outputs","title":"run"},{"location":"development/devel/api/#profile","text":"@abstractmethod def profile ( * inputs : torch . Tensor ) -> ProfilingResult Do a profiling run on the target device Arguments : inputs - a list of torch tensors representing the inputs to be run on the target device, each tensor represents a whole batched input, so for models taking 1 parameter, the list will contain 1 tensor of shape (batch_size, *input_shape) Returns - a ProfilingResult object containing the outputs of the model, the metrics obtained from the profiling run and the raw profile in a backend-specific format","title":"profile"},{"location":"development/devel/api/#available","text":"@classmethod @abstractmethod def available ( cls ) -> bool Check if the backend is available Returns: True if the backend is available, False otherwise","title":"available"},{"location":"development/devel/api/#export","text":"def export () -> None Export the model through the target backend","title":"export"},{"location":"development/devel/api/#inferencebackendbase-objects","text":"class InferenceBackendBase ( AbstractBackend ) Base class for backends, it is only here for backwards compatibility reasons, use AbstractBackend instead","title":"InferenceBackendBase Objects"},{"location":"development/devel/api/#hannahbackendsonnxrt","text":"","title":"hannah.backends.onnxrt"},{"location":"development/devel/api/#onnxruntimebackend-objects","text":"class OnnxruntimeBackend ( InferenceBackendBase ) Inference Backend for tensorflow","title":"OnnxruntimeBackend Objects"},{"location":"development/devel/api/#hannahbackends","text":"","title":"hannah.backends"},{"location":"development/devel/api/#hannahbackendsutils","text":"","title":"hannah.backends.utils"},{"location":"development/devel/api/#symbolic_batch_dim","text":"def symbolic_batch_dim ( model ) -> None make the batch dimension symbolic for onnx models Arguments : model - onnx model","title":"symbolic_batch_dim"},{"location":"development/devel/api/#hannahbackendstorch_mobile","text":"","title":"hannah.backends.torch_mobile"},{"location":"development/devel/api/#torchmobilebackend-objects","text":"class TorchMobileBackend ( InferenceBackendBase ) Inference backend for torch mobile","title":"TorchMobileBackend Objects"},{"location":"development/devel/api/#hannahutilslogger","text":"","title":"hannah.utils.logger"},{"location":"development/devel/api/#jsonlogger-objects","text":"class JSONLogger ( Logger )","title":"JSONLogger Objects"},{"location":"development/devel/api/#name","text":"@property def name () -> str Gets the name of the experiment. Returns : The name of the experiment.","title":"name"},{"location":"development/devel/api/#version","text":"@property def version () -> Union [ int , str ] Gets the version of the experiment. Returns : The version of the experiment if it is specified, else the next version.","title":"version"},{"location":"development/devel/api/#root_dir","text":"@property def root_dir () -> str Gets the save directory where the versioned JSON experiments are saved.","title":"root_dir"},{"location":"development/devel/api/#log_dir","text":"@property def log_dir () -> str The log directory for this run. By default, it is named 'version_${self.version}' but it can be overridden by passing a string value for the constructor's version parameter instead of None or an int.","title":"log_dir"},{"location":"development/devel/api/#experiment","text":"@property @rank_zero_experiment def experiment () -> \"_ExperimentWriter\" Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following. Example:: self.logger.experiment.some_experiment_writer_function()","title":"experiment"},{"location":"development/devel/api/#_experimentwriter-objects","text":"class _ExperimentWriter () Experiment writer for CSVLogger. Arguments : log_dir - Directory for the experiment logs","title":"_ExperimentWriter Objects"},{"location":"development/devel/api/#log_metrics","text":"def log_metrics ( metrics_dict : Dict [ str , float ], step : Optional [ int ] = None ) -> None Record metrics.","title":"log_metrics"},{"location":"development/devel/api/#save","text":"def save () -> None Save recorded metrics into files.","title":"save"},{"location":"development/devel/api/#hannahutilsimports","text":"","title":"hannah.utils.imports"},{"location":"development/devel/api/#lazy_import","text":"def lazy_import ( module_name , callback = None ) Returns a proxy module object that will lazily import the given module the first time it is used. Example usage::","title":"lazy_import"},{"location":"development/devel/api/#lazy-version-of-import-tensorflow-as-tf","text":"tf = lazy_import(\"tensorflow\")","title":"Lazy version of import tensorflow as tf"},{"location":"development/devel/api/#other-commands","text":"","title":"Other commands"},{"location":"development/devel/api/#now-the-module-is-loaded","text":"tf. version Arguments : module_name - the fully-qualified module name to import callback None - a callback function to call before importing the module Returns : a proxy module object that will be lazily imported when first used","title":"Now the module is loaded"},{"location":"development/devel/api/#lazymodule-objects","text":"class LazyModule ( types . ModuleType ) Proxy module that lazily imports the underlying module the first time it is actually used. Arguments : module_name - the fully-qualified module name to import callback None - a callback function to call before importing the module","title":"LazyModule Objects"},{"location":"development/devel/api/#hannahutilsdvclive","text":"","title":"hannah.utils.dvclive"},{"location":"development/devel/api/#hannahutils","text":"","title":"hannah.utils"},{"location":"development/devel/api/#hannahutilsutils","text":"","title":"hannah.utils.utils"},{"location":"development/devel/api/#log_execution_env_state","text":"def log_execution_env_state () -> None Log information about the execution environment.","title":"log_execution_env_state"},{"location":"development/devel/api/#git_version","text":"def git_version ( short = True ) Return the current git sha Arguments : short bool - If True, return the short (7 character) version of the SHA Returns : str - The current git SHA","title":"git_version"},{"location":"development/devel/api/#extract_from_download_cache","text":"def extract_from_download_cache ( filename , url , cached_files , target_cache , target_folder , target_test_folder = \"\" , clear_download = False , no_exist_check = False ) -> None extracts given file from cache or donwloads first from url Arguments : filename str - name of the file to download or extract url str - possible url to download the file cached_files (list(str)): cached files in download cache target_cache str - path to the folder to cache file if download necessary target_folder str - path where to extract file target_test_folder str, optional - folder to check if data are already there clear_download bool - clear download after usage no_exist_check bool - disables the check if folder exists","title":"extract_from_download_cache"},{"location":"development/devel/api/#fullname","text":"def fullname ( o ) -> Any Get the full classname of an object including surrounding packages/modules/namespaces","title":"fullname"},{"location":"development/devel/api/#set_deterministic","text":"@contextmanager def set_deterministic ( mode , warn_only = False ) A contextmanager to set deterministic algorithms","title":"set_deterministic"},{"location":"development/devel/api/#hannahutilstuple","text":"","title":"hannah.utils.tuple"},{"location":"development/devel/api/#hannahlogo","text":"","title":"hannah.logo"},{"location":"development/devel/api/#hannahcallbackssummaries","text":"","title":"hannah.callbacks.summaries"},{"location":"development/devel/api/#walk_model","text":"def walk_model ( model , dummy_input ) Adapted from IntelLabs Distiller Arguments : model: dummy_input:","title":"walk_model"},{"location":"development/devel/api/#macsummarycallback-objects","text":"class MacSummaryCallback ( Callback )","title":"MacSummaryCallback Objects"},{"location":"development/devel/api/#predict","text":"def predict ( pl_module , input = input ) Arguments : pl_module:","title":"predict"},{"location":"development/devel/api/#on_train_start","text":"@rank_zero_only def on_train_start ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_train_start"},{"location":"development/devel/api/#on_test_end_1","text":"@rank_zero_only def on_test_end ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_test_end"},{"location":"development/devel/api/#on_validation_epoch_end","text":"@rank_zero_only def on_validation_epoch_end ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_validation_epoch_end"},{"location":"development/devel/api/#estimate","text":"def estimate ( pl_module , input = None ) Generate Summary Metrics for neural network Arguments : pl_module(pytorch_lightning.LightningModule) - pytorch lightning module to summarize Returns : dict[str, float]: Dict of MetricName => Metric Value","title":"estimate"},{"location":"development/devel/api/#prod","text":"def prod ( seq ) Arguments : seq:","title":"prod"},{"location":"development/devel/api/#hannahcallbacksclustering","text":"","title":"hannah.callbacks.clustering"},{"location":"development/devel/api/#clustering","text":"def clustering ( params , inertia , cluster ) Arguments : params: inertia: cluster:","title":"clustering"},{"location":"development/devel/api/#kmeans-objects","text":"class kMeans ( Callback )","title":"kMeans Objects"},{"location":"development/devel/api/#on_test_epoch_start","text":"def on_test_epoch_start ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_test_epoch_start"},{"location":"development/devel/api/#on_train_epoch_end","text":"def on_train_epoch_end ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_train_epoch_end"},{"location":"development/devel/api/#hannahcallbackssvd_compress","text":"","title":"hannah.callbacks.svd_compress"},{"location":"development/devel/api/#svd-objects","text":"class SVD ( Callback )","title":"SVD Objects"},{"location":"development/devel/api/#on_train_epoch_start","text":"def on_train_epoch_start ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_train_epoch_start"},{"location":"development/devel/api/#hannahcallbacksdump_layers","text":"","title":"hannah.callbacks.dump_layers"},{"location":"development/devel/api/#testdumpercallback-objects","text":"class TestDumperCallback ( Callback )","title":"TestDumperCallback Objects"},{"location":"development/devel/api/#on_test_start","text":"def on_test_start ( pl_trainer , pl_model ) Arguments : pl_trainer: pl_model:","title":"on_test_start"},{"location":"development/devel/api/#hannahcallbacksprediction_logger","text":"","title":"hannah.callbacks.prediction_logger"},{"location":"development/devel/api/#hannahcallbacks","text":"","title":"hannah.callbacks"},{"location":"development/devel/api/#hannahcallbackspruning","text":"","title":"hannah.callbacks.pruning"},{"location":"development/devel/api/#pruningamountscheduler-objects","text":"class PruningAmountScheduler ()","title":"PruningAmountScheduler Objects"},{"location":"development/devel/api/#filteredpruning-objects","text":"class FilteredPruning ( ModelPruning )","title":"FilteredPruning Objects"},{"location":"development/devel/api/#setup","text":"def setup ( trainer : Trainer , pl_module : LightningModule , stage : str ) Arguments : trainer: pl_module:","title":"setup"},{"location":"development/devel/api/#filter_parameters_to_prune","text":"def filter_parameters_to_prune ( parameters_to_prune = None ) Filter out unprunable parameters Arguments : parameters_to_prune - (Default value = None)","title":"filter_parameters_to_prune"},{"location":"development/devel/api/#on_test_end_2","text":"def on_test_end ( trainer , pl_module ) -> None Arguments : trainer: pl_module:","title":"on_test_end"},{"location":"development/devel/api/#hannahcallbacksbackbone_finetuning","text":"","title":"hannah.callbacks.backbone_finetuning"},{"location":"development/devel/api/#hannahcallbacksfine_tuning","text":"","title":"hannah.callbacks.fine_tuning"},{"location":"development/devel/api/#hannahcallbacksoptimization","text":"","title":"hannah.callbacks.optimization"},{"location":"development/devel/api/#hydraoptcallback-objects","text":"class HydraOptCallback ( Callback )","title":"HydraOptCallback Objects"},{"location":"development/devel/api/#on_test_end_3","text":"def on_test_end ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_test_end"},{"location":"development/devel/api/#on_validation_end","text":"def on_validation_end ( trainer , pl_module ) Arguments : trainer: pl_module:","title":"on_validation_end"},{"location":"development/devel/api/#test_result","text":"def test_result ()","title":"test_result"},{"location":"development/devel/api/#val_result","text":"def val_result ()","title":"val_result"},{"location":"development/devel/api/#result","text":"def result ( dict = False ) Arguments : dict - (Default value = False)","title":"result"},{"location":"development/devel/api/#curves","text":"def curves ( dict = False ) Arguments : dict - (Default value = False)","title":"curves"},{"location":"development/devel/api/#hannahcallbacksbackends","text":"","title":"hannah.callbacks.backends"},{"location":"development/devel/api/#hannahdatasetsvisionfake","text":"","title":"hannah.datasets.vision.fake"},{"location":"development/devel/api/#hannahdatasetsvisionbase","text":"","title":"hannah.datasets.vision.base"},{"location":"development/devel/api/#torchvisiondatasetbase-objects","text":"class TorchvisionDatasetBase ( VisionDatasetBase ) Wrapper around torchvision classification datasets","title":"TorchvisionDatasetBase Objects"},{"location":"development/devel/api/#imagedatasetbase-objects","text":"class ImageDatasetBase ( VisionDatasetBase )","title":"ImageDatasetBase Objects"},{"location":"development/devel/api/#__init___1","text":"def __init__ ( X , y , classes , bbox = None , transform = None ) Initialize vision dataset Arguments : X List[str] - List of paths to image files y List[str] - Class id of corresponding image classes List[str] - List of class names, names are ordered by numeric class id bbox Dict[str] - Dict with filename as keys, bbox coordinates as numpy arrays transform Callable[image,image], optional - Optional transformation/augmentation of input images. Defaults to None.","title":"__init__"},{"location":"development/devel/api/#hannahdatasetsvisioncifar","text":"","title":"hannah.datasets.vision.cifar"},{"location":"development/devel/api/#hannahdatasetsvisionkvasir","text":"","title":"hannah.datasets.vision.kvasir"},{"location":"development/devel/api/#hannahdatasetsvision","text":"","title":"hannah.datasets.vision"},{"location":"development/devel/api/#hannahdatasetsvisiondresden_capsule","text":"Rhode island gastroenterology video capsule endoscopy dataset https://www.nature.com/articles/s41597-022-01726-3 https://github.com/acharoen/Rhode-Island-GI-VCE-Technical-Validation","title":"hannah.datasets.vision.dresden_capsule"},{"location":"development/devel/api/#hannahdatasetsvisionutils","text":"","title":"hannah.datasets.vision.utils"},{"location":"development/devel/api/#hannahdatasetsvisionutilsnaneye","text":"","title":"hannah.datasets.vision.utils.naneye"},{"location":"development/devel/api/#read_naneye","text":"def read_naneye ( data_file : Union [ str , Path ]) Read a naneye raw aimage and decode bayer pattern Arguments : data_file Union[str, Path] - path to the datafile Returns : np.ndarray - uint8 array of decoded image data","title":"read_naneye"},{"location":"development/devel/api/#hannahdatasetsvisionkvasir_unlabeled","text":"","title":"hannah.datasets.vision.kvasir_unlabeled"},{"location":"development/devel/api/#kvasircapsuleunlabeled-objects","text":"class KvasirCapsuleUnlabeled ( AbstractDataset ) Dataset representing unlabelled videos","title":"KvasirCapsuleUnlabeled Objects"},{"location":"development/devel/api/#sequential","text":"@property def sequential () -> bool Returns true if this dataset should only be iterated sequentially","title":"sequential"},{"location":"development/devel/api/#max_workers","text":"@property def max_workers () -> int Returns the maximum number of workers useable for this dataset","title":"max_workers"},{"location":"development/devel/api/#hannahdatasetsvisionri_capsule","text":"Rhode island gastroenterology video capsule endoscopy dataset https://www.nature.com/articles/s41597-022-01726-3 https://github.com/acharoen/Rhode-Island-GI-VCE-Technical-Validation","title":"hannah.datasets.vision.ri_capsule"},{"location":"development/devel/api/#split_train_set","text":"def split_train_set ( csv_file : pathlib . Path , drop_rate : float ) Split train set in two and save as separate csv files.","title":"split_train_set"},{"location":"development/devel/api/#hannahdatasetsdownsample","text":"","title":"hannah.datasets.Downsample"},{"location":"development/devel/api/#hannahdatasetsactivity","text":"","title":"hannah.datasets.activity"},{"location":"development/devel/api/#data3d-objects","text":"class Data3D () 3D-Data","title":"Data3D Objects"},{"location":"development/devel/api/#pampap2_imudata-objects","text":"class PAMPAP2_IMUData () A IMU set defined by temperature (\u00b0C) 3D-acceleration data (ms -2 ), scale: \u00b116g, resolution: 13-bit 3D-acceleration data (ms -2 ), scale: \u00b16g, resolution: 13-bit 3D-gyroscope data (rad/s) 3D-magnetometer data (\u03bcT) orientation (invalid in this data collection)","title":"PAMPAP2_IMUData Objects"},{"location":"development/devel/api/#pamap2_datapoint-objects","text":"class PAMAP2_DataPoint () A temporal datapoint in the dataset","title":"PAMAP2_DataPoint Objects"},{"location":"development/devel/api/#pamap2_datachunk-objects","text":"class PAMAP2_DataChunk () A DataChunk is a item of the pytorch dataset","title":"PAMAP2_DataChunk Objects"},{"location":"development/devel/api/#pamap2_dataset-objects","text":"class PAMAP2_Dataset ( AbstractDataset ) Class for the PAMAP2 activity dataset https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring","title":"PAMAP2_Dataset Objects"},{"location":"development/devel/api/#hannahdatasetsbase","text":"","title":"hannah.datasets.base"},{"location":"development/devel/api/#datasettype-objects","text":"class DatasetType ( Enum ) The type of a dataset partition e.g. train, dev, test","title":"DatasetType Objects"},{"location":"development/devel/api/#abstractdataset-objects","text":"class AbstractDataset ( Dataset , ABC )","title":"AbstractDataset Objects"},{"location":"development/devel/api/#prepare_1","text":"@abstractclassmethod def prepare ( cls , config : Dict [ str , Any ]) -> None Prepare the dataset. This method is run at the beginning of the dataset training. If possible it should download the dataset from its original source, if it is available for public download. Arguments : config Dict[Any] - The dataset configuration","title":"prepare"},{"location":"development/devel/api/#splits","text":"@abstractclassmethod def splits ( cls , config : Dict [ str , Any ] ) -> Tuple [ \"AbstractDataset\" , \"AbstractDataset\" , \"AbstractDataset\" ] Returns the test, validation and train split according to the Dataset config Arguments : config [type] - [description]","title":"splits"},{"location":"development/devel/api/#class_names","text":"@abstractproperty def class_names () -> List [ str ] Returns the names of the classes in the classification dataset","title":"class_names"},{"location":"development/devel/api/#class_counts","text":"@abstractproperty def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segementation, it may return None","title":"class_counts"},{"location":"development/devel/api/#__getitem__","text":"@abstractmethod def __getitem__ ( index ) -> List [ torch . Tensor ] Returns a torch.Tensor for the data item at the corresponding index The length of the list depends on the dataset item to use Arguments : index int - the index of the data item","title":"__getitem__"},{"location":"development/devel/api/#__len__","text":"@abstractmethod def __len__ () -> int Returns number of samples in dataset","title":"__len__"},{"location":"development/devel/api/#size","text":"def size () -> List [ int ] Returns dimension of output without batch dimension","title":"size"},{"location":"development/devel/api/#std","text":"@property def std () -> Optional [ Tuple [ int , ... ]] Returns channel-wise standard deviation for dataset if applicable","title":"std"},{"location":"development/devel/api/#mean","text":"@property def mean () -> Optional [ Tuple [ int , ... ]] Returns channel-wise means for dataset if applicable","title":"mean"},{"location":"development/devel/api/#resolution","text":"@property def resolution () -> Optional [ Tuple [ int , ... ]] Returns resolution for dataset if applicable","title":"resolution"},{"location":"development/devel/api/#weights","text":"@property def weights () -> Optional [ List [ float ]] Class weights for weighted sampling","title":"weights"},{"location":"development/devel/api/#sequential_1","text":"@property def sequential () -> bool Returns true if this dataset should only be iterated sequentially","title":"sequential"},{"location":"development/devel/api/#max_workers_1","text":"@property def max_workers () -> int Returns the maximum number of workers useable for this dataset","title":"max_workers"},{"location":"development/devel/api/#hannahdatasetsspeech","text":"","title":"hannah.datasets.speech"},{"location":"development/devel/api/#speechdataset-objects","text":"class SpeechDataset ( AbstractDataset ) Base Class for speech datasets","title":"SpeechDataset Objects"},{"location":"development/devel/api/#preprocess","text":"def preprocess ( example , silence = False , label = 0 ) Run preprocessing and feature extraction","title":"preprocess"},{"location":"development/devel/api/#speechcommandsdataset-objects","text":"class SpeechCommandsDataset ( SpeechDataset ) This class implements reading and preprocessing of speech commands like dataset","title":"SpeechCommandsDataset Objects"},{"location":"development/devel/api/#speechhotworddataset-objects","text":"class SpeechHotwordDataset ( SpeechDataset ) Dataset Class for Hotword dataset e.g. Hey Snips!","title":"SpeechHotwordDataset Objects"},{"location":"development/devel/api/#splits_1","text":"@classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List","title":"splits"},{"location":"development/devel/api/#vaddataset-objects","text":"class VadDataset ( SpeechDataset )","title":"VadDataset Objects"},{"location":"development/devel/api/#splits_2","text":"@classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List","title":"splits"},{"location":"development/devel/api/#hannahdatasetseeg_tusz","text":"","title":"hannah.datasets.eeg_tusz"},{"location":"development/devel/api/#eegdataset-objects","text":"class EEGDataset ( AbstractDataset )","title":"EEGDataset Objects"},{"location":"development/devel/api/#class_names_1","text":"@property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset","title":"class_names"},{"location":"development/devel/api/#class_counts_1","text":"@property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None","title":"class_counts"},{"location":"development/devel/api/#size_1","text":"def size () -> List [ int ] Returns dimension of output without batch dimension","title":"size"},{"location":"development/devel/api/#hannahdatasetsdatasetsplit","text":"","title":"hannah.datasets.DatasetSplit"},{"location":"development/devel/api/#hannahdatasetskitti","text":"","title":"hannah.datasets.Kitti"},{"location":"development/devel/api/#kitti-objects","text":"class Kitti ( AbstractDataset )","title":"Kitti Objects"},{"location":"development/devel/api/#splits_3","text":"@classmethod def splits ( cls , config ) Splits the dataset in training, devlopment and test set and returns the three sets as List","title":"splits"},{"location":"development/devel/api/#hannahdatasetsnoisedataset","text":"","title":"hannah.datasets.NoiseDataset"},{"location":"development/devel/api/#hannahdatasetsphysio","text":"","title":"hannah.datasets.physio"},{"location":"development/devel/api/#atrialfibrillationdataset-objects","text":"class AtrialFibrillationDataset ( PhysioDataset ) Atrial Fibrillation Database (https://physionet.org/content/afdb/1.0.0/)","title":"AtrialFibrillationDataset Objects"},{"location":"development/devel/api/#hannahdatasetseeg_chb","text":"","title":"hannah.datasets.eeg_chb"},{"location":"development/devel/api/#eegdataset-objects_1","text":"class EEGDataset ( AbstractDataset )","title":"EEGDataset Objects"},{"location":"development/devel/api/#class_names_2","text":"@property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset","title":"class_names"},{"location":"development/devel/api/#class_counts_2","text":"@property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None","title":"class_counts"},{"location":"development/devel/api/#size_2","text":"def size () -> List [ int ] Returns dimension of output without batch dimension","title":"size"},{"location":"development/devel/api/#hannahdatasetsfake1d","text":"","title":"hannah.datasets.fake1d"},{"location":"development/devel/api/#hannahdatasets","text":"","title":"hannah.datasets"},{"location":"development/devel/api/#hannahdatasetseeg_chbrt","text":"","title":"hannah.datasets.eeg_chbrt"},{"location":"development/devel/api/#eegrtdataset-objects","text":"class EEGRTDataset ( AbstractDataset )","title":"EEGRTDataset Objects"},{"location":"development/devel/api/#class_names_3","text":"@property def class_names () -> List [ str ] Returns the names of the classes in the classification dataset","title":"class_names"},{"location":"development/devel/api/#class_counts_3","text":"@property def class_counts () -> Optional [ Dict [ int , int ]] Returns the number of items in each class of the dataset If this is not applicable to a dataset type e.g. ASR, Semantic Segmentation, it may return None","title":"class_counts"},{"location":"development/devel/api/#size_3","text":"def size () -> List [ int ] Returns dimension of output without batch dimension","title":"size"},{"location":"development/devel/api/#hannahdatasetsutilscache","text":"","title":"hannah.datasets.utils.cache"},{"location":"development/devel/api/#hannahdatasetsutilsmd5","text":"","title":"hannah.datasets.utils.md5"},{"location":"development/devel/api/#hannahdatasetsutils","text":"","title":"hannah.datasets.utils"},{"location":"development/devel/api/#hannahdatasetscollate","text":"","title":"hannah.datasets.collate"},{"location":"development/devel/api/#vision_collate_fn","text":"def vision_collate_fn ( batch ) Function that takes in a batch of data and puts the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a :class: torch.Tensor , a Sequence of :class: torch.Tensor , a Collection of :class: torch.Tensor , or left unchanged, depending on the input type. This is used as the default function for collation for vision tasks batch_size or batch_sampler is defined in :class: ~torch.utils.data.DataLoader . Here is the general input type (based on the type of the element within the batch) to output type mapping: :class: torch.Tensor -> :class: torch.Tensor (with an added outer dimension batch size) NumPy Arrays -> :class: torch.Tensor float -> :class: torch.Tensor int -> :class: torch.Tensor str -> str (unchanged) bytes -> bytes (unchanged) Mapping[K, V_i] -> Mapping[K, vision_collate([V_1, V_2, ...])] NamedTuple[V1_i, V2_i, ...] -> NamedTuple[vision_collate([V1_1, V1_2, ...]), vision_collate([V2_1, V2_2, ...]), ...] Sequence[V1_i, V2_i, ...] -> Sequence[vision_collate([V1_1, V1_2, ...]), vision_collate([V2_1, V2_2, ...]), ...] Arguments : batch - a single batch to be collated Examples :","title":"vision_collate_fn"},{"location":"development/devel/api/#example-with-a-batch-of-ints","text":"vision_collate([0, 1, 2, 3]) tensor([0, 1, 2, 3])","title":"Example with a batch of ints:"},{"location":"development/devel/api/#example-with-a-batch-of-strs","text":"vision_collate(['a', 'b', 'c']) ['a', 'b', 'c']","title":"Example with a batch of strs:"},{"location":"development/devel/api/#example-with-map-inside-the-batch","text":"vision_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]) - {'A' - tensor([ 0, 100]), 'B': tensor([ 1, 100])}","title":"Example with Map inside the batch:"},{"location":"development/devel/api/#example-with-namedtuple-inside-the-batch","text":"Point = namedtuple('Point', ['x', 'y']) vision_collate([Point(0, 0), Point(1, 1)]) Point(x=tensor([0, 1]), y=tensor([0, 1]))","title":"Example with NamedTuple inside the batch:"},{"location":"development/devel/api/#example-with-tuple-inside-the-batch","text":"vision_collate([(0, 1), (2, 3)]) [tensor([0, 2]), tensor([1, 3])]","title":"Example with Tuple inside the batch:"},{"location":"development/devel/api/#example-with-list-inside-the-batch","text":"vision_collate([[0, 1], [2, 3]]) [tensor([0, 2]), tensor([1, 3])]","title":"Example with List inside the batch:"},{"location":"development/devel/api/#ctc_collate_fn","text":"def ctc_collate_fn ( data ) Creates mini-batch tensors from the list of tuples (src_seq, trg_seq). We should build a custom collate_fn rather than using default collate_fn, because merging sequences (including padding) is not supported in default. Sequences are padded to the maximum length of mini-batch sequences (dynamic padding). Arguments : data - list of tuple (src_seq, src_length, trg_seq, trg_length). src_seq: torch tensor of shape (x,?); variable length. src length: torch tenso of shape 1x1 trg_seq: torch tensor of shape (?); variable length. trg_length: torch_tensor of shape (1x1) Returns - tuple of four torch tensors src_seqs - torch tensor of shape (batch_size, x, padded_length). src_lengths - torch_tensor of shape (batch_size); valid length for each padded source sequence. trg_seqs - torch tensor of shape (batch_size, x, padded_length). trg_lengths - torch tensor of shape (batch_size); valid length for each padded target sequence.","title":"ctc_collate_fn"},{"location":"development/devel/api/#hannahdatasetsdirectional","text":"","title":"hannah.datasets.directional"},{"location":"development/devel/api/#directionaldataset-objects","text":"class DirectionalDataset ( AbstractDataset ) Directional Dataset","title":"DirectionalDataset Objects"},{"location":"development/devel/api/#hannahdatasetsemergency","text":"","title":"hannah.datasets.emergency"},{"location":"development/devel/api/#emergencysirendataset-objects","text":"class EmergencySirenDataset ( AbstractDataset ) Emergency Dataset","title":"EmergencySirenDataset Objects"},{"location":"development/devel/api/#hannahvisualization","text":"","title":"hannah.visualization"},{"location":"development/devel/api/#hannahfeatures","text":"","title":"hannah.features"},{"location":"development/devel/api/#mfcc-objects","text":"class MFCC ( torchaudio . transforms . MFCC ) A simple wrapper around torchaudio mfcc, but melkwargs are given as direct named arguments instead of a dictionary","title":"MFCC Objects"},{"location":"development/devel/api/#sincconv-objects","text":"class SincConv ( nn . Module ) Sinc convolution: Arguments : in_channels - No. of input channels(must be 1) out_channels - No. of filters(40) sample_rate - sampling rate, default set at 32000 kernel_size - Filter length(101)","title":"SincConv Objects"},{"location":"development/devel/api/#logspectrogram-objects","text":"class LogSpectrogram ( torch . nn . Module ) Create a spectrogram from a audio signal. Arguments : n_fft int, optional - Size of FFT, creates n_fft // 2 + 1 bins. (Default: 400 ) win_length int or None, optional - Window size. (Default: n_fft ) hop_length int or None, optional - Length of hop between STFT windows. (Default: win_length // 2 ) pad int, optional - Two sided padding of signal. (Default: 0 ) window_fn Callable[..., Tensor], optional - A function to create a window tensor that is applied/multiplied to each frame/window. (Default: torch.hann_window ) power float or None, optional - Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for energy, 2 for power, etc. If None, then the complex spectrum is returned instead. (Default: 2 ) normalized bool, optional - Whether to normalize by magnitude after stft. (Default: False ) wkwargs dict or None, optional - Arguments for window function. (Default: None ) center bool, optional - whether to pad :attr: waveform on both sides so that the :math: t -th frame is centered at time :math: t \\times \\text{hop\\_length} . Default - True pad_mode string, optional - controls the padding method used when :attr: center is True . Default: \"reflect\" onesided bool, optional - controls whether to return half of results to avoid redundancy Default: True","title":"LogSpectrogram Objects"},{"location":"development/devel/api/#forward_1","text":"def forward ( waveform : torch . Tensor ) -> torch . Tensor Arguments : waveform Tensor - Tensor of audio of dimension (..., time). Returns : Tensor - Dimension (..., freq, time), where freq is n_fft // 2 + 1 where n_fft is the number of Fourier bins, and time is the number of window hops (n_frame).","title":"forward"},{"location":"development/devel/api/#hannahoptimmadgrad","text":"","title":"hannah.optim.madgrad"},{"location":"development/devel/api/#madgrad-objects","text":"class MADGRAD ( torch . optim . Optimizer ) MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. .. _MADGRAD: https://arxiv.org/abs/2101.11075 MADGRAD is a general purpose optimizer that can be used in place of SGD or Adam may converge faster and generalize better. Currently GPU-only. Typically, the same learning rate schedule that is used for SGD or Adam may be used. The overall learning rate is not comparable to either method and should be determined by a hyper-parameter sweep. MADGRAD requires less weight decay than other methods, often as little as zero. Momentum values used for SGD or Adam's beta1 should work here also. On sparse problems both weight_decay and momentum should be set to 0. Arguments : params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate (default: 1e-2). momentum (float): Momentum value in the range [0,1) (default: 0.9). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).","title":"MADGRAD Objects"},{"location":"development/devel/api/#step","text":"def step ( closure : Optional [ Callable [[], float ]] = None ) -> Optional [ float ] Performs a single optimization step. Arguments : closure callable, optional - A closure that reevaluates the model and returns the loss.","title":"step"},{"location":"development/devel/api/#hannahoptim","text":"","title":"hannah.optim"},{"location":"development/devel/api/#hannahoptimradam","text":"","title":"hannah.optim.RAdam"},{"location":"development/devel/api/#hannahtest_linear_classifier","text":"","title":"hannah.test_linear_classifier"},{"location":"development/devel/api/#hannahmodelskakao_resnet","text":"","title":"hannah.models.kakao_resnet"},{"location":"development/devel/api/#hannahmodelssimple1d","text":"","title":"hannah.models.simple1d"},{"location":"development/devel/api/#hannahmodelssinc","text":"","title":"hannah.models.sinc"},{"location":"development/devel/api/#hannahmodelssincmodels","text":"","title":"hannah.models.sinc.models"},{"location":"development/devel/api/#gdsconv-objects","text":"class GDSConv ( nn . Module )","title":"GDSConv Objects"},{"location":"development/devel/api/#forward_2","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#gdsconvblock-objects","text":"class GDSConvBlock ( nn . Module )","title":"GDSConvBlock Objects"},{"location":"development/devel/api/#forward_3","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#sincnet-objects","text":"class SincNet ( nn . Module )","title":"SincNet Objects"},{"location":"development/devel/api/#forward_4","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelsai8x","text":"","title":"hannah.models.ai8x"},{"location":"development/devel/api/#hannahmodelsai8xmodels","text":"A search space based on the cifar 10 NASNet search space for ai85x devices from: htt","title":"hannah.models.ai8x.models"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netoperators","text":"","title":"hannah.models.embedded_vision_net.operators"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netblocks","text":"","title":"hannah.models.embedded_vision_net.blocks"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netmodels","text":"","title":"hannah.models.embedded_vision_net.models"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netutils","text":"","title":"hannah.models.embedded_vision_net.utils"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netexpressions","text":"","title":"hannah.models.embedded_vision_net.expressions"},{"location":"development/devel/api/#hannahmodelsembedded_vision_netparameters","text":"","title":"hannah.models.embedded_vision_net.parameters"},{"location":"development/devel/api/#hannahmodelshello","text":"","title":"hannah.models.hello"},{"location":"development/devel/api/#hannahmodelshellomodels","text":"","title":"hannah.models.hello.models"},{"location":"development/devel/api/#dsconv2d-objects","text":"class DSConv2d ( nn . Module )","title":"DSConv2d Objects"},{"location":"development/devel/api/#forward_5","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#dscnnspeechmodel-objects","text":"class DSCNNSpeechModel ( nn . Module )","title":"DSCNNSpeechModel Objects"},{"location":"development/devel/api/#forward_6","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#dnnspeechmodel-objects","text":"class DNNSpeechModel ( nn . Module )","title":"DNNSpeechModel Objects"},{"location":"development/devel/api/#forward_7","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelsresnetoperators","text":"","title":"hannah.models.resnet.operators"},{"location":"development/devel/api/#hannahmodelsresnetblocks","text":"","title":"hannah.models.resnet.blocks"},{"location":"development/devel/api/#hannahmodelsresnet","text":"","title":"hannah.models.resnet"},{"location":"development/devel/api/#hannahmodelsresnetmodels","text":"","title":"hannah.models.resnet.models"},{"location":"development/devel/api/#hannahmodelsresnetexpressions","text":"","title":"hannah.models.resnet.expressions"},{"location":"development/devel/api/#padding_expression","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_2","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahmodelsresnetmodels_lazy","text":"","title":"hannah.models.resnet.models_lazy"},{"location":"development/devel/api/#padding_expression_1","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_3","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_1","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahmodelslstm","text":"","title":"hannah.models.lstm"},{"location":"development/devel/api/#hannahmodelslstmmodels","text":"","title":"hannah.models.lstm.models"},{"location":"development/devel/api/#lstmmodel-objects","text":"class LSTMModel ( nn . Module ) Simple LSTM model.","title":"LSTMModel Objects"},{"location":"development/devel/api/#forward_8","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodels_vendorresnet_mc_dropout","text":"PyTorch ResNet This started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with additional dropout and dynamic global avg/max pool. ResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman Copyright 2019, Ross Wightman","title":"hannah.models._vendor.resnet_mc_dropout"},{"location":"development/devel/api/#resnet-objects","text":"class ResNet ( nn . Module ) ResNet / ResNeXt / SE-ResNeXt / SE-Net This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that * have > 1 stride in the 3x3 conv layer of bottleneck * have conv-bn-act ordering This ResNet impl supports a number of stem and downsample options based on the v1c, v1d, v1e, and v1s variants included in the MXNet Gluon ResNetV1b model. The C and D variants are also discussed in the 'Bag of Tricks' paper: https://arxiv.org/pdf/1812.01187. The B variant is equivalent to torchvision default. ResNet variants (the same modifications can be used in SE/ResNeXt models as well): * normal, b - 7x7 stem, stem_width = 64, same as torchvision ResNet, NVIDIA ResNet 'v1.5', Gluon v1b * c - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64) * d - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64), average pool in downsample * e - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128), average pool in downsample * s - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128) * t - 3 layer deep 3x3 stem, stem width = 32 (24, 48, 64), average pool in downsample * tn - 3 layer deep 3x3 stem, stem width = 32 (24, 32, 64), average pool in downsample ResNeXt * normal - 7x7 stem, stem_width = 64, standard cardinality and base widths * same c,d, e, s variants as ResNet can be enabled SE-ResNeXt * normal - 7x7 stem, stem_width = 64 * same c, d, e, s variants as ResNet can be enabled SENet-154 - 3 layer deep 3x3 stem (same as v1c-v1s), stem_width = 64, cardinality=64, reduction by 2 on width of first bottleneck convolution, 3x3 downsample convs after first block","title":"ResNet Objects"},{"location":"development/devel/api/#__init___2","text":"def __init__ ( block , layers , num_classes = 1000 , in_chans = 3 , output_stride = 32 , global_pool = \"avg\" , cardinality = 1 , base_width = 64 , stem_width = 64 , stem_type = \"\" , replace_stem_pool = False , block_reduce_first = 1 , down_kernel_size = 1 , avg_down = False , act_layer = nn . ReLU , norm_layer = nn . BatchNorm2d , aa_layer = None , drop_rate = 0.0 , drop_path_rate = 0.0 , drop_block_rate = 0.0 , zero_init_last = True , block_args = None ) Arguments : block nn.Module - class for the residual block. Options are BasicBlock, Bottleneck. layers (List[int]) : number of layers in each block num_classes int - number of classification classes (default 1000) in_chans int - number of input (color) channels. (default 3) output_stride int - output stride of the network, 32, 16, or 8. (default 32) global_pool str - Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax' (default 'avg') cardinality int - number of convolution groups for 3x3 conv in Bottleneck. (default 1) base_width int - bottleneck channels factor. planes * base_width / 64 * cardinality (default 64) stem_width int - number of channels in stem convolutions (default 64) stem_type str - The type of stem (default ''): '', default - a single 7x7 conv with a width of stem_width 'deep' - three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2 'deep_tiered' - three 3x3 conv layers of widths stem_width//4 * 3, stem_width, stem_width * 2 replace_stem_pool bool - replace stem max-pooling layer with a 3x3 stride-2 convolution block_reduce_first int - Reduction factor for first convolution output width of residual blocks, 1 for all archs except senets, where 2 (default 1) down_kernel_size int - kernel size of residual block downsample path, 1x1 for most, 3x3 for senets (default: 1) avg_down bool - use avg pooling for projection skip connection between stages/downsample (default False) act_layer str, nn.Module - activation layer norm_layer str, nn.Module - normalization layer aa_layer nn.Module - anti-aliasing layer drop_rate float - Dropout probability before classifier, for training (default 0.) drop_path_rate float - Stochastic depth drop-path rate (default 0.) drop_block_rate float - Drop block rate (default 0.) zero_init_last bool - zero-init the last weight in residual path (usually last BN affine weight) block_args dict - Extra kwargs to pass through to block module","title":"__init__"},{"location":"development/devel/api/#resnet10t_mc_dropout","text":"@register_model def resnet10t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-10-T model.","title":"resnet10t_mc_dropout"},{"location":"development/devel/api/#resnet14t_mc_dropout","text":"@register_model def resnet14t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-14-T model.","title":"resnet14t_mc_dropout"},{"location":"development/devel/api/#resnet18_mc_dropout","text":"@register_model def resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18 model.","title":"resnet18_mc_dropout"},{"location":"development/devel/api/#resnet18d_mc_dropout","text":"@register_model def resnet18d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18-D model.","title":"resnet18d_mc_dropout"},{"location":"development/devel/api/#resnet34_mc_dropout","text":"@register_model def resnet34_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34 model.","title":"resnet34_mc_dropout"},{"location":"development/devel/api/#resnet34d_mc_dropout","text":"@register_model def resnet34d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34-D model.","title":"resnet34d_mc_dropout"},{"location":"development/devel/api/#resnet26_mc_dropout","text":"@register_model def resnet26_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26 model.","title":"resnet26_mc_dropout"},{"location":"development/devel/api/#resnet26t_mc_dropout","text":"@register_model def resnet26t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26-T model.","title":"resnet26t_mc_dropout"},{"location":"development/devel/api/#resnet26d_mc_dropout","text":"@register_model def resnet26d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-26-D model.","title":"resnet26d_mc_dropout"},{"location":"development/devel/api/#resnet50_mc_dropout","text":"@register_model def resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model.","title":"resnet50_mc_dropout"},{"location":"development/devel/api/#resnet50d_mc_dropout","text":"@register_model def resnet50d_mc_dropout ( pretrained = False , ** kwargs ) -> ResNet Constructs a ResNet-50-D model.","title":"resnet50d_mc_dropout"},{"location":"development/devel/api/#resnet50t_mc_dropout","text":"@register_model def resnet50t_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-T model.","title":"resnet50t_mc_dropout"},{"location":"development/devel/api/#resnet101_mc_dropout","text":"@register_model def resnet101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101 model.","title":"resnet101_mc_dropout"},{"location":"development/devel/api/#resnet101d_mc_dropout","text":"@register_model def resnet101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model.","title":"resnet101d_mc_dropout"},{"location":"development/devel/api/#resnet152_mc_dropout","text":"@register_model def resnet152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152 model.","title":"resnet152_mc_dropout"},{"location":"development/devel/api/#resnet152d_mc_dropout","text":"@register_model def resnet152d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152-D model.","title":"resnet152d_mc_dropout"},{"location":"development/devel/api/#resnet200_mc_dropout","text":"@register_model def resnet200_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200 model.","title":"resnet200_mc_dropout"},{"location":"development/devel/api/#resnet200d_mc_dropout","text":"@register_model def resnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model.","title":"resnet200d_mc_dropout"},{"location":"development/devel/api/#tv_resnet34_mc_dropout","text":"@register_model def tv_resnet34_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34 model with original Torchvision weights.","title":"tv_resnet34_mc_dropout"},{"location":"development/devel/api/#tv_resnet50_mc_dropout","text":"@register_model def tv_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with original Torchvision weights.","title":"tv_resnet50_mc_dropout"},{"location":"development/devel/api/#tv_resnet101_mc_dropout","text":"@register_model def tv_resnet101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101 model w/ Torchvision pretrained weights.","title":"tv_resnet101_mc_dropout"},{"location":"development/devel/api/#tv_resnet152_mc_dropout","text":"@register_model def tv_resnet152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-152 model w/ Torchvision pretrained weights.","title":"tv_resnet152_mc_dropout"},{"location":"development/devel/api/#wide_resnet50_2_mc_dropout","text":"@register_model def wide_resnet50_2_mc_dropout ( pretrained = False , ** kwargs ) Constructs a Wide ResNet-50-2 model. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.","title":"wide_resnet50_2_mc_dropout"},{"location":"development/devel/api/#wide_resnet101_2_mc_dropout","text":"@register_model def wide_resnet101_2_mc_dropout ( pretrained = False , ** kwargs ) Constructs a Wide ResNet-101-2 model. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same.","title":"wide_resnet101_2_mc_dropout"},{"location":"development/devel/api/#resnet50_gn_mc_dropout","text":"@register_model def resnet50_gn_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model w/ GroupNorm","title":"resnet50_gn_mc_dropout"},{"location":"development/devel/api/#resnext50_32x4d_mc_dropout","text":"@register_model def resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50-32x4d model.","title":"resnext50_32x4d_mc_dropout"},{"location":"development/devel/api/#resnext50d_32x4d_mc_dropout","text":"@register_model def resnext50d_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample","title":"resnext50d_32x4d_mc_dropout"},{"location":"development/devel/api/#resnext101_32x4d_mc_dropout","text":"@register_model def resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x4d model.","title":"resnext101_32x4d_mc_dropout"},{"location":"development/devel/api/#resnext101_32x8d_mc_dropout","text":"@register_model def resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x8d model.","title":"resnext101_32x8d_mc_dropout"},{"location":"development/devel/api/#resnext101_64x4d_mc_dropout","text":"@register_model def resnext101_64x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt101-64x4d model.","title":"resnext101_64x4d_mc_dropout"},{"location":"development/devel/api/#tv_resnext50_32x4d_mc_dropout","text":"@register_model def tv_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt50-32x4d model with original Torchvision weights.","title":"tv_resnext50_32x4d_mc_dropout"},{"location":"development/devel/api/#ig_resnext101_32x8d_mc_dropout","text":"@register_model def ig_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/","title":"ig_resnext101_32x8d_mc_dropout"},{"location":"development/devel/api/#ig_resnext101_32x16d_mc_dropout","text":"@register_model def ig_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/","title":"ig_resnext101_32x16d_mc_dropout"},{"location":"development/devel/api/#ig_resnext101_32x32d_mc_dropout","text":"@register_model def ig_resnext101_32x32d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/","title":"ig_resnext101_32x32d_mc_dropout"},{"location":"development/devel/api/#ig_resnext101_32x48d_mc_dropout","text":"@register_model def ig_resnext101_32x48d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data and finetuned on ImageNet from Figure 5 in \"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932> _ Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/","title":"ig_resnext101_32x48d_mc_dropout"},{"location":"development/devel/api/#ssl_resnet18_mc_dropout","text":"@register_model def ssl_resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNet-18 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnet18_mc_dropout"},{"location":"development/devel/api/#ssl_resnet50_mc_dropout","text":"@register_model def ssl_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNet-50 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnet50_mc_dropout"},{"location":"development/devel/api/#ssl_resnext50_32x4d_mc_dropout","text":"@register_model def ssl_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-50 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnext50_32x4d_mc_dropout"},{"location":"development/devel/api/#ssl_resnext101_32x4d_mc_dropout","text":"@register_model def ssl_resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnext101_32x4d_mc_dropout"},{"location":"development/devel/api/#ssl_resnext101_32x8d_mc_dropout","text":"@register_model def ssl_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x8 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnext101_32x8d_mc_dropout"},{"location":"development/devel/api/#ssl_resnext101_32x16d_mc_dropout","text":"@register_model def ssl_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-supervised ResNeXt-101 32x16 model pre-trained on YFCC100M dataset and finetuned on ImageNet \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"ssl_resnext101_32x16d_mc_dropout"},{"location":"development/devel/api/#swsl_resnet18_mc_dropout","text":"@register_model def swsl_resnet18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised Resnet-18 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnet18_mc_dropout"},{"location":"development/devel/api/#swsl_resnet50_mc_dropout","text":"@register_model def swsl_resnet50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNet-50 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnet50_mc_dropout"},{"location":"development/devel/api/#swsl_resnext50_32x4d_mc_dropout","text":"@register_model def swsl_resnext50_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-50 32x4 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnext50_32x4d_mc_dropout"},{"location":"development/devel/api/#swsl_resnext101_32x4d_mc_dropout","text":"@register_model def swsl_resnext101_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x4 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnext101_32x4d_mc_dropout"},{"location":"development/devel/api/#swsl_resnext101_32x8d_mc_dropout","text":"@register_model def swsl_resnext101_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x8 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnext101_32x8d_mc_dropout"},{"location":"development/devel/api/#swsl_resnext101_32x16d_mc_dropout","text":"@register_model def swsl_resnext101_32x16d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a semi-weakly supervised ResNeXt-101 32x16 model pre-trained on 1B weakly supervised image dataset and finetuned on ImageNet. \"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546> _ Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/","title":"swsl_resnext101_32x16d_mc_dropout"},{"location":"development/devel/api/#ecaresnet26t_mc_dropout","text":"@register_model def ecaresnet26t_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn.","title":"ecaresnet26t_mc_dropout"},{"location":"development/devel/api/#ecaresnet50d_mc_dropout","text":"@register_model def ecaresnet50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with eca.","title":"ecaresnet50d_mc_dropout"},{"location":"development/devel/api/#ecaresnet50d_pruned_mc_dropout","text":"@register_model def ecaresnet50d_pruned_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model pruned with eca. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf","title":"ecaresnet50d_pruned_mc_dropout"},{"location":"development/devel/api/#ecaresnet50t_mc_dropout","text":"@register_model def ecaresnet50t_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNet-50-T model. Like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn.","title":"ecaresnet50t_mc_dropout"},{"location":"development/devel/api/#ecaresnetlight_mc_dropout","text":"@register_model def ecaresnetlight_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D light model with eca.","title":"ecaresnetlight_mc_dropout"},{"location":"development/devel/api/#ecaresnet101d_mc_dropout","text":"@register_model def ecaresnet101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with eca.","title":"ecaresnet101d_mc_dropout"},{"location":"development/devel/api/#ecaresnet101d_pruned_mc_dropout","text":"@register_model def ecaresnet101d_pruned_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model pruned with eca. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf","title":"ecaresnet101d_pruned_mc_dropout"},{"location":"development/devel/api/#ecaresnet200d_mc_dropout","text":"@register_model def ecaresnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model with ECA.","title":"ecaresnet200d_mc_dropout"},{"location":"development/devel/api/#ecaresnet269d_mc_dropout","text":"@register_model def ecaresnet269d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-269-D model with ECA.","title":"ecaresnet269d_mc_dropout"},{"location":"development/devel/api/#ecaresnext26t_32x4d_mc_dropout","text":"@register_model def ecaresnext26t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem. This model replaces SE module with the ECA module","title":"ecaresnext26t_32x4d_mc_dropout"},{"location":"development/devel/api/#ecaresnext50t_32x4d_mc_dropout","text":"@register_model def ecaresnext50t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs an ECA-ResNeXt-50-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem. This model replaces SE module with the ECA module","title":"ecaresnext50t_32x4d_mc_dropout"},{"location":"development/devel/api/#seresnet200d_mc_dropout","text":"@register_model def seresnet200d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-200-D model with SE attn.","title":"seresnet200d_mc_dropout"},{"location":"development/devel/api/#seresnet269d_mc_dropout","text":"@register_model def seresnet269d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-269-D model with SE attn.","title":"seresnet269d_mc_dropout"},{"location":"development/devel/api/#seresnext26d_32x4d_mc_dropout","text":"@register_model def seresnext26d_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNeXt-26-D model.` This is technically a 28 layer ResNet, using the 'D' modifier from Gluon / bag-of-tricks for combination of deep stem and avg_pool in downsample.","title":"seresnext26d_32x4d_mc_dropout"},{"location":"development/devel/api/#seresnext26t_32x4d_mc_dropout","text":"@register_model def seresnext26t_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNet-26-T model. This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem.","title":"seresnext26t_32x4d_mc_dropout"},{"location":"development/devel/api/#seresnext26tn_32x4d_mc_dropout","text":"@register_model def seresnext26tn_32x4d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE-ResNeXt-26-T model. NOTE I deprecated previous 't' model defs and replaced 't' with 'tn', this was the only tn model of note so keeping this def for backwards compat with any uses out there. Old 't' model is lost.","title":"seresnext26tn_32x4d_mc_dropout"},{"location":"development/devel/api/#resnetblur18_mc_dropout","text":"@register_model def resnetblur18_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-18 model with blur anti-aliasing","title":"resnetblur18_mc_dropout"},{"location":"development/devel/api/#resnetblur50_mc_dropout","text":"@register_model def resnetblur50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with blur anti-aliasing","title":"resnetblur50_mc_dropout"},{"location":"development/devel/api/#resnetblur50d_mc_dropout","text":"@register_model def resnetblur50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with blur anti-aliasing","title":"resnetblur50d_mc_dropout"},{"location":"development/devel/api/#resnetblur101d_mc_dropout","text":"@register_model def resnetblur101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with blur anti-aliasing","title":"resnetblur101d_mc_dropout"},{"location":"development/devel/api/#resnetaa34d_mc_dropout","text":"@register_model def resnetaa34d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-34-D model w/ avgpool anti-aliasing","title":"resnetaa34d_mc_dropout"},{"location":"development/devel/api/#resnetaa50_mc_dropout","text":"@register_model def resnetaa50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50 model with avgpool anti-aliasing","title":"resnetaa50_mc_dropout"},{"location":"development/devel/api/#resnetaa50d_mc_dropout","text":"@register_model def resnetaa50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-50-D model with avgpool anti-aliasing","title":"resnetaa50d_mc_dropout"},{"location":"development/devel/api/#resnetaa101d_mc_dropout","text":"@register_model def resnetaa101d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-101-D model with avgpool anti-aliasing","title":"resnetaa101d_mc_dropout"},{"location":"development/devel/api/#seresnetaa50d_mc_dropout","text":"@register_model def seresnetaa50d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE=ResNet-50-D model with avgpool anti-aliasing","title":"seresnetaa50d_mc_dropout"},{"location":"development/devel/api/#seresnextaa101d_32x8d_mc_dropout","text":"@register_model def seresnextaa101d_32x8d_mc_dropout ( pretrained = False , ** kwargs ) Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing","title":"seresnextaa101d_32x8d_mc_dropout"},{"location":"development/devel/api/#resnetrs50_mc_dropout","text":"@register_model def resnetrs50_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-50 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs50_mc_dropout"},{"location":"development/devel/api/#resnetrs101_mc_dropout","text":"@register_model def resnetrs101_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-101 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs101_mc_dropout"},{"location":"development/devel/api/#resnetrs152_mc_dropout","text":"@register_model def resnetrs152_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-152 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs152_mc_dropout"},{"location":"development/devel/api/#resnetrs200_mc_dropout","text":"@register_model def resnetrs200_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-200 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs200_mc_dropout"},{"location":"development/devel/api/#resnetrs270_mc_dropout","text":"@register_model def resnetrs270_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-270 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs270_mc_dropout"},{"location":"development/devel/api/#resnetrs350_mc_dropout","text":"@register_model def resnetrs350_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-350 model. Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs350_mc_dropout"},{"location":"development/devel/api/#resnetrs420_mc_dropout","text":"@register_model def resnetrs420_mc_dropout ( pretrained = False , ** kwargs ) Constructs a ResNet-RS-420 model Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579 Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs","title":"resnetrs420_mc_dropout"},{"location":"development/devel/api/#hannahmodels_vendor","text":"","title":"hannah.models._vendor"},{"location":"development/devel/api/#hannahmodels_vendorfocalnet","text":"","title":"hannah.models._vendor.focalnet"},{"location":"development/devel/api/#mlp-objects","text":"class Mlp ( nn . Module )","title":"Mlp Objects"},{"location":"development/devel/api/#forward_9","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#focalmodulation-objects","text":"class FocalModulation ( nn . Module )","title":"FocalModulation Objects"},{"location":"development/devel/api/#forward_10","text":"def forward ( x ) Arguments : x - input features with shape of (B, H, W, C)","title":"forward"},{"location":"development/devel/api/#extra_repr","text":"def extra_repr () -> str","title":"extra_repr"},{"location":"development/devel/api/#flops","text":"def flops ( N ) calculate flops for 1 window with token length of N Arguments : N:","title":"flops"},{"location":"development/devel/api/#focalnetblock-objects","text":"class FocalNetBlock ( nn . Module ) Focal Modulation Network Block. Arguments : dim(int) - Number of input channels. input_resolution(tuple[int]) - Input resulotion. mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. drop(float) - Dropout rate. Default: 0.0 drop_path(float) - Stochastic depth rate. Default: 0.0 act_layer(nn.Module) - Activation layer. Default: nn.GELU norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm focal_level(int) - Number of focal levels. focal_window(int) - Focal window size at first focal level use_layerscale(bool) - Whether use layerscale layerscale_value(float) - Initial layerscale value use_postln(bool) - Whether use layernorm after modulation","title":"FocalNetBlock Objects"},{"location":"development/devel/api/#forward_11","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#extra_repr_1","text":"def extra_repr () -> str","title":"extra_repr"},{"location":"development/devel/api/#flops_1","text":"def flops ()","title":"flops"},{"location":"development/devel/api/#basiclayer-objects","text":"class BasicLayer ( nn . Module ) A basic Focal Transformer layer for one stage. Arguments : dim(int) - Number of input channels. input_resolution(tuple[int]) - Input resolution. depth(int) - Number of blocks. window_size(int) - Local window size. mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. qkv_bias(bool) - If True, add a learnable bias to query, key, value. Default: True qk_scale(float | None): Override default qk scale of head_dim ** -0.5 if set. drop(float) - Dropout rate. Default: 0.0 drop_path(float | tuple[float]): Stochastic depth rate. Default: 0.0 norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm downsample(nn.Module | None): Downsample layer at the end of the layer. Default: None use_checkpoint(bool) - Whether to use checkpointing to save memory. Default: False. focal_level(int) - Number of focal levels focal_window(int) - Focal window size at first focal level use_layerscale(bool) - Whether use layerscale layerscale_value(float) - Initial layerscale value use_postln(bool) - Whether use layernorm after modulation","title":"BasicLayer Objects"},{"location":"development/devel/api/#forward_12","text":"def forward ( x , H , W ) Arguments : x: H: W:","title":"forward"},{"location":"development/devel/api/#extra_repr_2","text":"def extra_repr () -> str","title":"extra_repr"},{"location":"development/devel/api/#flops_2","text":"def flops ()","title":"flops"},{"location":"development/devel/api/#patchembed-objects","text":"class PatchEmbed ( nn . Module ) Image to Patch Embedding Arguments : img_size(int) - Image size. Default: 224. patch_size(int) - Patch token size. Default: 4. in_chans(int) - Number of input image channels. Default: 3. embed_dim(int) - Number of linear projection output channels. Default: 96. norm_layer(nn.Module) - Normalization layer. Default: None","title":"PatchEmbed Objects"},{"location":"development/devel/api/#forward_13","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#flops_3","text":"def flops ()","title":"flops"},{"location":"development/devel/api/#focalnet-objects","text":"class FocalNet ( nn . Module ) Focal Modulation Networks (FocalNets) Arguments : img_size(int | tuple(int): Input image size. Default 224 patch_size(int | tuple(int): Patch size. Default: 4 - in_chans(int) - Number of input image channels. Default: 3 - num_classes(int) - Number of classes for classification head. Default: 1000 - embed_dim(int) - Patch embedding dimension. Default: 96 - depths(tuple(int) - Depth of each Focal Transformer layer. - mlp_ratio(float) - Ratio of mlp hidden dim to embedding dim. Default: 4 - drop_rate(float) - Dropout rate. Default: 0 - drop_path_rate(float) - Stochastic depth rate. Default: 0.1 - norm_layer(nn.Module) - Normalization layer. Default: nn.LayerNorm. - patch_norm(bool) - If True, add normalization after patch embedding. Default: True - use_checkpoint(bool) - Whether to use checkpointing to save memory. Default: False - focal_levels(list) - How many focal levels at all stages. Note that this excludes the finest-grain level. Default: [1, 1, 1, 1] - focal_windows(list) - The focal window size at all stages. Default: [7, 5, 3, 1] - use_conv_embed(bool) - Whether use convolutional embedding. We noted that using convolutional embedding usually improve the performance, but we do not use it by default. Default: False - use_layerscale(bool) - Whether use layerscale proposed in CaiT. Default: False - layerscale_value(float) - Value for layer scale. Default: 1e-4 - use_postln(bool) - Whether use layernorm after modulation (it helps stablize training of large models)","title":"FocalNet Objects"},{"location":"development/devel/api/#no_weight_decay","text":"@torch . jit . ignore def no_weight_decay ()","title":"no_weight_decay"},{"location":"development/devel/api/#no_weight_decay_keywords","text":"@torch . jit . ignore def no_weight_decay_keywords ()","title":"no_weight_decay_keywords"},{"location":"development/devel/api/#forward_features","text":"def forward_features ( x ) Arguments : x:","title":"forward_features"},{"location":"development/devel/api/#forward_14","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#flops_4","text":"def flops ()","title":"flops"},{"location":"development/devel/api/#build_transforms","text":"def build_transforms ( img_size , center_crop = False ) Arguments : img_size: - center_crop - (Default value = False)","title":"build_transforms"},{"location":"development/devel/api/#build_transforms4display","text":"def build_transforms4display ( img_size , center_crop = False ) Arguments : img_size: - center_crop - (Default value = False)","title":"build_transforms4display"},{"location":"development/devel/api/#focalnet_tiny_srf","text":"@register_model def focalnet_tiny_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_tiny_srf"},{"location":"development/devel/api/#focalnet_small_srf","text":"@register_model def focalnet_small_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_small_srf"},{"location":"development/devel/api/#focalnet_base_srf","text":"@register_model def focalnet_base_srf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_base_srf"},{"location":"development/devel/api/#focalnet_tiny_lrf","text":"@register_model def focalnet_tiny_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_tiny_lrf"},{"location":"development/devel/api/#focalnet_small_lrf","text":"@register_model def focalnet_small_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_small_lrf"},{"location":"development/devel/api/#focalnet_base_lrf","text":"@register_model def focalnet_base_lrf ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_base_lrf"},{"location":"development/devel/api/#focalnet_tiny_iso_16","text":"@register_model def focalnet_tiny_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_tiny_iso_16"},{"location":"development/devel/api/#focalnet_small_iso_16","text":"@register_model def focalnet_small_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_small_iso_16"},{"location":"development/devel/api/#focalnet_base_iso_16","text":"@register_model def focalnet_base_iso_16 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_base_iso_16"},{"location":"development/devel/api/#focalnet_large_fl3","text":"@register_model def focalnet_large_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_large_fl3"},{"location":"development/devel/api/#focalnet_large_fl4","text":"@register_model def focalnet_large_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_large_fl4"},{"location":"development/devel/api/#focalnet_xlarge_fl3","text":"@register_model def focalnet_xlarge_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_xlarge_fl3"},{"location":"development/devel/api/#focalnet_xlarge_fl4","text":"@register_model def focalnet_xlarge_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_xlarge_fl4"},{"location":"development/devel/api/#focalnet_huge_fl3","text":"@register_model def focalnet_huge_fl3 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_huge_fl3"},{"location":"development/devel/api/#focalnet_huge_fl4","text":"@register_model def focalnet_huge_fl4 ( pretrained = False , ** kwargs ) Arguments : pretrained - (Default value = False) **kwargs:","title":"focalnet_huge_fl4"},{"location":"development/devel/api/#hannahmodelshonk","text":"","title":"hannah.models.honk"},{"location":"development/devel/api/#hannahmodelshonkmodel","text":"","title":"hannah.models.honk.model"},{"location":"development/devel/api/#truncated_normal","text":"def truncated_normal ( tensor , std_dev = 0.01 ) Arguments : tensor: - std_dev - (Default value = 0.01)","title":"truncated_normal"},{"location":"development/devel/api/#speechresmodel-objects","text":"class SpeechResModel ( nn . Module )","title":"SpeechResModel Objects"},{"location":"development/devel/api/#forward_15","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#speechmodel-objects","text":"class SpeechModel ( nn . Module )","title":"SpeechModel Objects"},{"location":"development/devel/api/#forward_16","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodels","text":"","title":"hannah.models"},{"location":"development/devel/api/#hannahmodelstc","text":"","title":"hannah.models.tc"},{"location":"development/devel/api/#hannahmodelstcmodels","text":"","title":"hannah.models.tc.models"},{"location":"development/devel/api/#create_act","text":"def create_act ( act , clipping_value ) Arguments : act: clipping_value:","title":"create_act"},{"location":"development/devel/api/#approximateglobalaveragepooling1d-objects","text":"class ApproximateGlobalAveragePooling1D ( nn . Module )","title":"ApproximateGlobalAveragePooling1D Objects"},{"location":"development/devel/api/#forward_17","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#tcresidualblock-objects","text":"class TCResidualBlock ( nn . Module )","title":"TCResidualBlock Objects"},{"location":"development/devel/api/#forward_18","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#tcresnetmodel-objects","text":"class TCResNetModel ( nn . Module )","title":"TCResNetModel Objects"},{"location":"development/devel/api/#forward_19","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#exitwrapperblock-objects","text":"class ExitWrapperBlock ( nn . Module )","title":"ExitWrapperBlock Objects"},{"location":"development/devel/api/#forward_20","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#branchytcresnetmodel-objects","text":"class BranchyTCResNetModel ( TCResNetModel )","title":"BranchyTCResNetModel Objects"},{"location":"development/devel/api/#on_val","text":"def on_val ()","title":"on_val"},{"location":"development/devel/api/#on_val_end","text":"def on_val_end ()","title":"on_val_end"},{"location":"development/devel/api/#on_test","text":"def on_test ()","title":"on_test"},{"location":"development/devel/api/#on_test_end_4","text":"def on_test_end ()","title":"on_test_end"},{"location":"development/devel/api/#reset_stats","text":"def reset_stats ()","title":"reset_stats"},{"location":"development/devel/api/#print_stats","text":"def print_stats ()","title":"print_stats"},{"location":"development/devel/api/#forward_21","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#get_loss_function","text":"def get_loss_function ()","title":"get_loss_function"},{"location":"development/devel/api/#hannahmodelsutils","text":"","title":"hannah.models.utils"},{"location":"development/devel/api/#next_power_of2","text":"def next_power_of2 ( x ) Arguments : x:","title":"next_power_of2"},{"location":"development/devel/api/#hannahmodelsobjectdetectionloss","text":"","title":"hannah.models.objectdetection.loss"},{"location":"development/devel/api/#bbox_iou","text":"def bbox_iou ( box1 , box2 , x1y1x2y2 = True , GIoU = False , DIoU = False , CIoU = False , eps = 1e-7 ) Arguments : box1: box2: - x1y1x2y2 - (Default value = True) - GIoU - (Default value = False) - DIoU - (Default value = False) - CIoU - (Default value = False) - eps - (Default value = 1e-7)","title":"bbox_iou"},{"location":"development/devel/api/#is_parallel","text":"def is_parallel ( model ) Arguments : model:","title":"is_parallel"},{"location":"development/devel/api/#bceblurwithlogitsloss-objects","text":"class BCEBlurWithLogitsLoss ( nn . Module ) Arguments : eps - (Default value = 0.1) ) - # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441# return positive: negative label smoothing BCE targetsreturn 1.0 - 0.5 * eps: 0.5 * epsclass BCEBlurWithLogitsLoss(nn.Module: ) - # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441# return positive:","title":"BCEBlurWithLogitsLoss Objects"},{"location":"development/devel/api/#forward_22","text":"def forward ( pred , true ) Arguments : pred: true:","title":"forward"},{"location":"development/devel/api/#focalloss-objects","text":"class FocalLoss ( nn . Module )","title":"FocalLoss Objects"},{"location":"development/devel/api/#forward_23","text":"def forward ( pred , true ) Arguments : pred: true:","title":"forward"},{"location":"development/devel/api/#qfocalloss-objects","text":"class QFocalLoss ( nn . Module )","title":"QFocalLoss Objects"},{"location":"development/devel/api/#forward_24","text":"def forward ( pred , true ) Arguments : pred: true:","title":"forward"},{"location":"development/devel/api/#computeloss-objects","text":"class ComputeLoss ()","title":"ComputeLoss Objects"},{"location":"development/devel/api/#build_targets","text":"def build_targets ( p , targets ) Arguments : p: targets:","title":"build_targets"},{"location":"development/devel/api/#hannahmodelsobjectdetection","text":"","title":"hannah.models.objectdetection"},{"location":"development/devel/api/#hannahmodelsobjectdetectionmodels","text":"","title":"hannah.models.objectdetection.models"},{"location":"development/devel/api/#fasterrcnn-objects","text":"class FasterRCNN ( torch . nn . Module )","title":"FasterRCNN Objects"},{"location":"development/devel/api/#forward_25","text":"def forward ( x , y = None ) Arguments : x: - y - (Default value = None)","title":"forward"},{"location":"development/devel/api/#transformoutput","text":"def transformOutput ( cocoGt , output , x , y ) Arguments : cocoGt: output: x: y:","title":"transformOutput"},{"location":"development/devel/api/#ultralyticsyolo-objects","text":"class UltralyticsYolo ( torch . nn . Module )","title":"UltralyticsYolo Objects"},{"location":"development/devel/api/#forward_26","text":"def forward ( x , y = None ) Arguments : x: - y - (Default value = None)","title":"forward"},{"location":"development/devel/api/#train_1","text":"def train ( mode = True ) Arguments : mode - (Default value = True)","title":"train"},{"location":"development/devel/api/#transformoutput_1","text":"def transformOutput ( cocoGt , output , x , y ) Arguments : cocoGt: output: x: y:","title":"transformOutput"},{"location":"development/devel/api/#hannahmodelsfactoryqat","text":"Import from new loacation for backwards compatibility","title":"hannah.models.factory.qat"},{"location":"development/devel/api/#hannahmodelsfactoryfactory","text":"A neural network model factory It allows us to construct quantized and unquantized versions of the same network, allows to explore implementation alternatives using a common neural network construction interface.","title":"hannah.models.factory.factory"},{"location":"development/devel/api/#normconfig-objects","text":"@dataclass class NormConfig ()","title":"NormConfig Objects"},{"location":"development/devel/api/#bnconfig-objects","text":"@dataclass class BNConfig ( NormConfig )","title":"BNConfig Objects"},{"location":"development/devel/api/#actconfig-objects","text":"@dataclass class ActConfig ()","title":"ActConfig Objects"},{"location":"development/devel/api/#eluconfig-objects","text":"@dataclass class ELUConfig ( ActConfig )","title":"ELUConfig Objects"},{"location":"development/devel/api/#hardtanhconfig-objects","text":"@dataclass class HardtanhConfig ( ActConfig )","title":"HardtanhConfig Objects"},{"location":"development/devel/api/#minorblockconfig-objects","text":"@dataclass class MinorBlockConfig ()","title":"MinorBlockConfig Objects"},{"location":"development/devel/api/#target","text":"target Operation","title":"target"},{"location":"development/devel/api/#parallel","text":"execute block in parallel with preceding block","title":"parallel"},{"location":"development/devel/api/#out_channels","text":"number of output channels","title":"out_channels"},{"location":"development/devel/api/#kernel_size","text":"kernel size of this Operation (if applicable)","title":"kernel_size"},{"location":"development/devel/api/#stride","text":"stride for this operation use","title":"stride"},{"location":"development/devel/api/#padding","text":"use padding for this operation (padding will always try to keep input dimensions / stride)","title":"padding"},{"location":"development/devel/api/#dilation","text":"dilation factor to use for this operation","title":"dilation"},{"location":"development/devel/api/#groups","text":"number of groups for this operation","title":"groups"},{"location":"development/devel/api/#norm","text":"normalization to use (true uses networks default configs)","title":"norm"},{"location":"development/devel/api/#act","text":"activation to use (true uses default configs)","title":"act"},{"location":"development/devel/api/#upsampling","text":"Upsampling factor for mbconv layers","title":"upsampling"},{"location":"development/devel/api/#bias","text":"use bias for this operation","title":"bias"},{"location":"development/devel/api/#out_quant","text":"use output quantization for this operation","title":"out_quant"},{"location":"development/devel/api/#majorblockconfig-objects","text":"@dataclass class MajorBlockConfig ()","title":"MajorBlockConfig Objects"},{"location":"development/devel/api/#stride_1","text":"Union[None, int, Tuple[int, ...], Tuple[int, ...]]","title":"stride"},{"location":"development/devel/api/#last","text":"Indicates wether this block is the last reduction block","title":"last"},{"location":"development/devel/api/#linearconfig-objects","text":"@dataclass class LinearConfig ()","title":"LinearConfig Objects"},{"location":"development/devel/api/#norm_1","text":"Union[bool, NormConfig]","title":"norm"},{"location":"development/devel/api/#act_1","text":"Union[bool, ActConfig]","title":"act"},{"location":"development/devel/api/#networkconfig-objects","text":"@dataclass class NetworkConfig ()","title":"NetworkConfig Objects"},{"location":"development/devel/api/#networkfactory-objects","text":"class NetworkFactory ()","title":"NetworkFactory Objects"},{"location":"development/devel/api/#act_2","text":"def act ( config : ActConfig ) -> nn . Module Arguments : config - ActConfig: config - ActConfig:","title":"act"},{"location":"development/devel/api/#conv2d","text":"def conv2d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : Union [ int , Tuple [ int , ... ]], stride : Union [ int , Tuple [ int , ... ]] = 1 , padding : Union [ int , Tuple [ int , ... ], bool ] = True , dilation : Union [ int , Tuple [ int , ... ]] = 0 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False , bias : bool = False ) -> None Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - Union[int: Tuple[int, ...]]: (Default value = 1) stride - Union[int: padding - Union[int: Tuple[int, ...]: bool] - (Default value = False) dilation - int: (Default value = 0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: bias - bool: (Default value = False) input_shape - Tuple[int: out_channels - int: kernel_size - Union[int: stride - Union[int: padding - Union[int: dilation - int: (Default value = 0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: bias - bool: (Default value = False)","title":"conv2d"},{"location":"development/devel/api/#mbconv1d","text":"def mbconv1d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : int , dilation : int = 1 , stride : int = 1 , padding : Union [ int , bool ] = True , bias = False , upsampling : float = 1.0 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False ) Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - int: dilation - int: (Default value = 1) stride - int: (Default value = 1) padding - Union[int: bool] - (Default value = False) bias - (Default value = False) upsampling - float: (Default value = 1.0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: input_shape - Tuple[int: out_channels - int: kernel_size - int: dilation - int: (Default value = 1) stride - int: (Default value = 1) padding - Union[int: upsampling - float: (Default value = 1.0) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig:","title":"mbconv1d"},{"location":"development/devel/api/#conv1d","text":"def conv1d ( input_shape : Tuple [ int , ... ], out_channels : int , kernel_size : int , stride : int = 1 , bias : bool = False , padding : Union [ int , bool ] = True , dilation : int = 1 , groups : int = 1 , norm : Union [ BNConfig , bool ] = False , act : Union [ ActConfig , bool ] = False , out_quant : bool = True ) -> Tuple [ Tuple [ int , ... ], nn . Module ] Arguments : input_shape - Tuple[int: int: int]: out_channels - int: kernel_size - int: stride - int: (Default value = 1) bias - bool: (Default value = False) padding - Union[int: bool] - (Default value = False) dilation - int: (Default value = 1) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: out_quant - bool: (Default value = True) input_shape - Tuple[int: out_channels - int: kernel_size - int: stride - int: (Default value = 1) bias - bool: (Default value = False) padding - Union[int: dilation - int: (Default value = 1) groups - int: (Default value = 1) norm - Union[BNConfig: act - Union[ActConfig: out_quant - bool: (Default value = True)","title":"conv1d"},{"location":"development/devel/api/#minor","text":"def minor ( input_shape , config : MinorBlockConfig , major_stride = None ) Arguments : input_shape: - config - MinorBlockConfig: - major_stride - (Default value = None) - config - MinorBlockConfig:","title":"minor"},{"location":"development/devel/api/#forward_27","text":"def forward ( input_shape : Tuple [ int , ... ], config : MajorBlockConfig ) Create a forward neural network block without parallelism If parallel is set to [True, False, True, False] Input: ------->|---> parallel: False ---> parallel: False ---> | --> output Arguments : input_shape - Tuple[int, ...]: config - MajorBlockConfig: input_shape - Tuple[int, ...]: config - MajorBlockConfig:","title":"forward"},{"location":"development/devel/api/#residual","text":"def residual ( input_shape : Tuple [ int , ... ], config : MajorBlockConfig ) Create a neural network block with with residual parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---> parallel: True ---> | Input: ------->| +---> |---> parallel: False ---> parallel: False ---> | If the major block does change the output dimensions compared to the input and one of the branches does not contain any layers, we infer 1x1 conv of maximum group size (gcd (input_channels, output_channels)) to do the downsampling. Arguments : input_shape - Tuple[int, ...]: config - MajorBlockConfig: input_shape - Tuple[int, ...]: config - MajorBlockConfig:","title":"residual"},{"location":"development/devel/api/#input","text":"def input ( in_channels : int , config : MajorBlockConfig ) Create a neural network block with input parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---> | |---> parallel: True ---> + -----------------> | Input:--------->| +---> |---> parallel: False ---> parallel: False ---> | If there are no parallel branches in the network. The major block is a standard feed forward layer. Arguments : in_channels - int: config - MajorBlockConfig: in_channels - int: config - MajorBlockConfig:","title":"input"},{"location":"development/devel/api/#full","text":"def full ( in_channels : int , config : MajorBlockConfig ) Create a neural network block with full parallelism If parallel is set to [True, False, True, False] |---> parallel: True ---------------------------------- -| Input:--->| +---> | |--> parallel: False --->| | |---> parallel: False ----> | +--->| |--> parallel: True ---->| If there are no parallel blocks the block is a standard feed forward network. Arguments : in_channels - int: config - MajorBlockConfig: in_channels - int: config - MajorBlockConfig:","title":"full"},{"location":"development/devel/api/#major","text":"def major ( input_shape , config : MajorBlockConfig ) Arguments : input_shape: - config - MajorBlockConfig: - config - MajorBlockConfig:","title":"major"},{"location":"development/devel/api/#linear","text":"def linear ( input_shape , config : LinearConfig ) Arguments : input_shape: - config - LinearConfig: - config - LinearConfig:","title":"linear"},{"location":"development/devel/api/#identity","text":"def identity () -> Identity","title":"identity"},{"location":"development/devel/api/#network","text":"def network ( input_shape , labels : int , network_config : Union [ ListConfig , DictConfig ]) Arguments : input_shape: - labels - int: - network_config - NetworkConfig: - labels - int: - network_config - NetworkConfig:","title":"network"},{"location":"development/devel/api/#create_cnn","text":"def create_cnn ( input_shape : Sequence [ int ], labels : int , name : str , conv : Optional [ List [ MajorBlockConfig ]] = None , linear : Optional [ List [ LinearConfig ]] = None , norm : Optional [ NormConfig ] = None , act : Optional [ ActConfig ] = None , qconfig : Any = None , dropout : float = 0.5 ) Arguments : input_shape - Sequence[int]: labels - int: name - str: conv - Optional[List[MajorBlockConfig]]: (Default value = None) linear - Optional[List[LinearConfig]]: (Default value = None) norm - Optional[NormConfig]: (Default value = None) act - Optional[ActConfig]: (Default value = None) qconfig - Any: (Default value = None) dropout - float: (Default value = 0.5) input_shape - Sequence[int]: labels - int: name - str: conv - Optional[List[MajorBlockConfig]]: (Default value = None) linear - Optional[List[LinearConfig]]: (Default value = None) norm - Optional[NormConfig]: (Default value = None) act - Optional[ActConfig]: (Default value = None) qconfig - Any: (Default value = None) dropout - float: (Default value = 0.5)","title":"create_cnn"},{"location":"development/devel/api/#hannahmodelsfactoryrounding","text":"Import from new loacation for backwards compatibility","title":"hannah.models.factory.rounding"},{"location":"development/devel/api/#hannahmodelsfactoryqconfig","text":"Import from new loacation for backwards compatibility","title":"hannah.models.factory.qconfig"},{"location":"development/devel/api/#hannahmodelsfactorynetwork","text":"","title":"hannah.models.factory.network"},{"location":"development/devel/api/#convnet-objects","text":"class ConvNet ( nn . Module )","title":"ConvNet Objects"},{"location":"development/devel/api/#forward_28","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelsfactory","text":"","title":"hannah.models.factory"},{"location":"development/devel/api/#hannahmodelsfactoryreduction","text":"","title":"hannah.models.factory.reduction"},{"location":"development/devel/api/#reductionblockadd-objects","text":"class ReductionBlockAdd ( nn . Module ) Reduction block that sums over its inputs","title":"ReductionBlockAdd Objects"},{"location":"development/devel/api/#forward_29","text":"def forward ( x : Tensor ) -> Tensor Arguments : x - Tensor: x - Tensor:","title":"forward"},{"location":"development/devel/api/#reductionblockconcat-objects","text":"class ReductionBlockConcat ( nn . Module ) Reduction block that concatenates its inputs","title":"ReductionBlockConcat Objects"},{"location":"development/devel/api/#forward_30","text":"def forward ( x : Tensor ) -> Tensor Arguments : x - Tensor: x - Tensor:","title":"forward"},{"location":"development/devel/api/#hannahmodelsfactorypooling","text":"","title":"hannah.models.factory.pooling"},{"location":"development/devel/api/#approximateglobalaveragepooling1d-objects_1","text":"class ApproximateGlobalAveragePooling1D ( nn . Module ) A global average pooling layer, that divides by the next power of 2 instead of true number of elements","title":"ApproximateGlobalAveragePooling1D Objects"},{"location":"development/devel/api/#forward_31","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#approximateglobalaveragepooling2d-objects","text":"class ApproximateGlobalAveragePooling2D ( nn . Module ) A global average pooling layer, that divides by the next power of 2 instead of true number of elements","title":"ApproximateGlobalAveragePooling2D Objects"},{"location":"development/devel/api/#forward_32","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelsfactoryact","text":"","title":"hannah.models.factory.act"},{"location":"development/devel/api/#dummyactivation-objects","text":"class DummyActivation ( nn . Identity ) Dummy class that instantiated to mark a missing activation. This can be used to mark requantization of activations for convolutional layers without activation functions. Arguments :","title":"DummyActivation Objects"},{"location":"development/devel/api/#hannahmodelsfactoryquantized","text":"Import from new loacation for backwards compatibility","title":"hannah.models.factory.quantized"},{"location":"development/devel/api/#hannahmodelswavenet","text":"","title":"hannah.models.wavenet"},{"location":"development/devel/api/#hannahmodelswavenetmodels","text":"","title":"hannah.models.wavenet.models"},{"location":"development/devel/api/#conv-objects","text":"class Conv ( nn . Module ) A convolution with the option to be causal and use xavier initialization","title":"Conv Objects"},{"location":"development/devel/api/#forward_33","text":"def forward ( signal ) Arguments : signal:","title":"forward"},{"location":"development/devel/api/#wavenet-objects","text":"class WaveNet ( nn . Module )","title":"WaveNet Objects"},{"location":"development/devel/api/#forward_34","text":"def forward ( input_data ) Arguments : input_data:","title":"forward"},{"location":"development/devel/api/#hannahmodelsconv_vitoperators","text":"","title":"hannah.models.conv_vit.operators"},{"location":"development/devel/api/#hannahmodelsconv_vitblocks","text":"","title":"hannah.models.conv_vit.blocks"},{"location":"development/devel/api/#hannahmodelsconv_vitattention","text":"","title":"hannah.models.conv_vit.attention"},{"location":"development/devel/api/#hannahmodelsconv_vitmodels","text":"","title":"hannah.models.conv_vit.models"},{"location":"development/devel/api/#hannahmodelsekut","text":"","title":"hannah.models.ekut"},{"location":"development/devel/api/#hannahmodelsekutmodels","text":"","title":"hannah.models.ekut.models"},{"location":"development/devel/api/#conv_bn","text":"def conv_bn ( inp , oup , stride ) Arguments : inp: oup: stride:","title":"conv_bn"},{"location":"development/devel/api/#conv_1x1_bn","text":"def conv_1x1_bn ( inp , oup ) Arguments : inp: oup:","title":"conv_1x1_bn"},{"location":"development/devel/api/#invertedresidual-objects","text":"class InvertedResidual ( nn . Module )","title":"InvertedResidual Objects"},{"location":"development/devel/api/#forward_35","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#rawspeechmodel-objects","text":"class RawSpeechModel ( nn . Module ) Speech Recognition on RAW Data using Wolfgang Fuhls Networks","title":"RawSpeechModel Objects"},{"location":"development/devel/api/#forward_36","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#rawspeechmodelinvertedresidual-objects","text":"class RawSpeechModelInvertedResidual ( nn . Module )","title":"RawSpeechModelInvertedResidual Objects"},{"location":"development/devel/api/#forward_37","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelstimm","text":"","title":"hannah.models.timm"},{"location":"development/devel/api/#defaultanomalydetector-objects","text":"class DefaultAnomalyDetector ( nn . Module )","title":"DefaultAnomalyDetector Objects"},{"location":"development/devel/api/#forward_38","text":"def forward ( x ) Simple anomaly detection head for a neural network Arguments : x - Tensor of logits Returns - A single element floating point tensor representing the anomaly score","title":"forward"},{"location":"development/devel/api/#defaultclassifierhead-objects","text":"class DefaultClassifierHead ( nn . Module )","title":"DefaultClassifierHead Objects"},{"location":"development/devel/api/#forward_39","text":"def forward ( x : torch . Tensor ) -> torch . Tensor Arguments : x (torch.Tensor): Returns : Resulting torch.Tensor after applying classification","title":"forward"},{"location":"development/devel/api/#defaultprojectionhead-objects","text":"class DefaultProjectionHead ( nn . Module ) Default projection head for semi supervised classification learning","title":"DefaultProjectionHead Objects"},{"location":"development/devel/api/#forward_40","text":"def forward ( x : torch . Tensor ) -> torch . Tensor Forward function for default Projection Head Arguments : x - Input tensor Returns : output tensor","title":"forward"},{"location":"development/devel/api/#defaultdecoderhead-objects","text":"class DefaultDecoderHead ( nn . Module )","title":"DefaultDecoderHead Objects"},{"location":"development/devel/api/#__init___3","text":"def __init__ ( latent_shape , input_shape ) Default Decoder Head for autoencoders using TransposedConv2D Arguments : latent_shape(Tuple) - Shape (CxHxW) of the latent representation of the autoencoder input_shape(Tuple) - Shape (CxHxW) of the reconstructed image","title":"__init__"},{"location":"development/devel/api/#forward_41","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#timmmodel-objects","text":"class TimmModel ( nn . Module )","title":"TimmModel Objects"},{"location":"development/devel/api/#forward_42","text":"def forward ( x : torch . Tensor ) -> torch . Tensor Arguments : x - torch.Tensor: x - torch.Tensor:","title":"forward"},{"location":"development/devel/api/#hannahmodelsmobilenetoperators","text":"","title":"hannah.models.mobilenet.operators"},{"location":"development/devel/api/#hannahmodelsmobilenetmodels","text":"","title":"hannah.models.mobilenet.models"},{"location":"development/devel/api/#hannahmodelsmobilenetexpressions","text":"","title":"hannah.models.mobilenet.expressions"},{"location":"development/devel/api/#padding_expression_2","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_4","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_2","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahmodelsvad","text":"","title":"hannah.models.vad"},{"location":"development/devel/api/#hannahmodelsvadmodels","text":"","title":"hannah.models.vad.models"},{"location":"development/devel/api/#bottleneckvad-objects","text":"class BottleneckVad ( nn . Module )","title":"BottleneckVad Objects"},{"location":"development/devel/api/#forward_43","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#num_flat_features","text":"def num_flat_features ( x ) Arguments : x:","title":"num_flat_features"},{"location":"development/devel/api/#smallvad-objects","text":"class SmallVad ( nn . Module )","title":"SmallVad Objects"},{"location":"development/devel/api/#forward_44","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#num_flat_features_1","text":"def num_flat_features ( x ) Arguments : x:","title":"num_flat_features"},{"location":"development/devel/api/#simplevad-objects","text":"class SimpleVad ( nn . Module )","title":"SimpleVad Objects"},{"location":"development/devel/api/#forward_45","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#num_flat_features_2","text":"def num_flat_features ( x ) Arguments : x:","title":"num_flat_features"},{"location":"development/devel/api/#bottleneckvadmodel-objects","text":"class BottleneckVadModel ( nn . Module )","title":"BottleneckVadModel Objects"},{"location":"development/devel/api/#forward_46","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#simplevadmodel-objects","text":"class SimpleVadModel ( nn . Module )","title":"SimpleVadModel Objects"},{"location":"development/devel/api/#forward_47","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#smallvadmodel-objects","text":"class SmallVadModel ( nn . Module )","title":"SmallVadModel Objects"},{"location":"development/devel/api/#forward_48","text":"def forward ( x ) Arguments : x:","title":"forward"},{"location":"development/devel/api/#hannahmodelsfunctional_net_testmodels","text":"","title":"hannah.models.functional_net_test.models"},{"location":"development/devel/api/#hannahmodelsfunctional_net_testexpressions","text":"","title":"hannah.models.functional_net_test.expressions"},{"location":"development/devel/api/#padding_expression_3","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_5","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_3","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahmodelsconvnet","text":"","title":"hannah.models.convnet"},{"location":"development/devel/api/#hannahmodelsconvnetmodels","text":"","title":"hannah.models.convnet.models"},{"location":"development/devel/api/#padding_expression_4","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_6","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_4","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahnasdataflowtensor_type","text":"","title":"hannah.nas.dataflow.tensor_type"},{"location":"development/devel/api/#hannahnasdataflowtensor","text":"","title":"hannah.nas.dataflow.tensor"},{"location":"development/devel/api/#hannahnasdataflowopsidentity","text":"","title":"hannah.nas.dataflow.ops.identity"},{"location":"development/devel/api/#hannahnasdataflowopsadd","text":"","title":"hannah.nas.dataflow.ops.add"},{"location":"development/devel/api/#hannahnasdataflowopsconcat","text":"","title":"hannah.nas.dataflow.ops.concat"},{"location":"development/devel/api/#hannahnasdataflowopsdropout","text":"","title":"hannah.nas.dataflow.ops.dropout"},{"location":"development/devel/api/#hannahnasdataflowops","text":"","title":"hannah.nas.dataflow.ops"},{"location":"development/devel/api/#hannahnasdataflowopspooling","text":"","title":"hannah.nas.dataflow.ops.pooling"},{"location":"development/devel/api/#hannahnasdataflowopslinear","text":"","title":"hannah.nas.dataflow.ops.linear"},{"location":"development/devel/api/#hannahnasdataflowopsbatch_nom","text":"","title":"hannah.nas.dataflow.ops.batch_nom"},{"location":"development/devel/api/#hannahnasdataflowopsconv2d","text":"","title":"hannah.nas.dataflow.ops.conv2d"},{"location":"development/devel/api/#hannahnasdataflowopsrelu","text":"","title":"hannah.nas.dataflow.ops.relu"},{"location":"development/devel/api/#hannahnasdataflowopssum","text":"","title":"hannah.nas.dataflow.ops.sum"},{"location":"development/devel/api/#hannahnasdataflowquantization_type","text":"","title":"hannah.nas.dataflow.quantization_type"},{"location":"development/devel/api/#hannahnasdataflowdataflow_utils","text":"","title":"hannah.nas.dataflow.dataflow_utils"},{"location":"development/devel/api/#hannahnasdataflowregister_ops","text":"","title":"hannah.nas.dataflow.register_ops"},{"location":"development/devel/api/#hannahnasdataflowcompression_type","text":"","title":"hannah.nas.dataflow.compression_type"},{"location":"development/devel/api/#hannahnasdataflowanalysisdataflow_analysis","text":"","title":"hannah.nas.dataflow.analysis.dataflow_analysis"},{"location":"development/devel/api/#hannahnasdataflowrepeat","text":"","title":"hannah.nas.dataflow.repeat"},{"location":"development/devel/api/#hannahnasdataflowop_type","text":"","title":"hannah.nas.dataflow.op_type"},{"location":"development/devel/api/#hannahnasdataflowregistry","text":"","title":"hannah.nas.dataflow.registry"},{"location":"development/devel/api/#hannahnasdataflow","text":"","title":"hannah.nas.dataflow"},{"location":"development/devel/api/#hannahnasdataflowdata_type","text":"","title":"hannah.nas.dataflow.data_type"},{"location":"development/devel/api/#hannahnasdataflowtensor_expression","text":"","title":"hannah.nas.dataflow.tensor_expression"},{"location":"development/devel/api/#hannahnasdataflowdataflow_graph","text":"","title":"hannah.nas.dataflow.dataflow_graph"},{"location":"development/devel/api/#dataflowgraph-objects","text":"@parametrize class DataFlowGraph ( TensorExpression )","title":"DataFlowGraph Objects"},{"location":"development/devel/api/#link_users","text":"def link_users () Link the DFG to its users and the users of the DFG to the DFG","title":"link_users"},{"location":"development/devel/api/#collect_users","text":"def collect_users ( node ) Traverse graph starting from node and collect all users (including users from subsequent nodes). If a node_b is NOT in collect_users(node_a), this means that node_b is either BEFORE node_a in the graph OR it is in a parallel branch.","title":"collect_users"},{"location":"development/devel/api/#parameters_7","text":"node : type description","title":"Parameters"},{"location":"development/devel/api/#returns_5","text":"type description","title":"Returns"},{"location":"development/devel/api/#find_first_input","text":"def find_first_input ( node ) Recusively traverses the graph from the given node back to its first input. NOTE: The traversal is via OPERANDS and not OUTPUT, meaning that e.g. weight Tensors that are included in Ops in a DFG are not returned","title":"find_first_input"},{"location":"development/devel/api/#parameters_8","text":"node : type description","title":"Parameters"},{"location":"development/devel/api/#returns_6","text":"type description","title":"Returns"},{"location":"development/devel/api/#hannahnasdataflowaxis_type","text":"","title":"hannah.nas.dataflow.axis_type"},{"location":"development/devel/api/#axistuple-objects","text":"@parametrize class AxisTuple () Used to have the axis dict as a parametrized object","title":"AxisTuple Objects"},{"location":"development/devel/api/#hannahnasdataflowtransformationsgraph_tranformer","text":"","title":"hannah.nas.dataflow.transformations.graph_tranformer"},{"location":"development/devel/api/#hannahnasdataflowoptional_op","text":"","title":"hannah.nas.dataflow.optional_op"},{"location":"development/devel/api/#hannahnasdataflowscoping_utils","text":"","title":"hannah.nas.dataflow.scoping_utils"},{"location":"development/devel/api/#hannahnasgraph_conversion","text":"","title":"hannah.nas.graph_conversion"},{"location":"development/devel/api/#hannahnasexpressionschoice","text":"","title":"hannah.nas.expressions.choice"},{"location":"development/devel/api/#hannahnasexpressionslogic","text":"","title":"hannah.nas.expressions.logic"},{"location":"development/devel/api/#hannahnasexpressionsshapes","text":"","title":"hannah.nas.expressions.shapes"},{"location":"development/devel/api/#hannahnasexpressionsop","text":"","title":"hannah.nas.expressions.op"},{"location":"development/devel/api/#hannahnasexpressions","text":"","title":"hannah.nas.expressions"},{"location":"development/devel/api/#hannahnasexpressionsconditions","text":"","title":"hannah.nas.expressions.conditions"},{"location":"development/devel/api/#hannahnasexpressionsutils","text":"","title":"hannah.nas.expressions.utils"},{"location":"development/devel/api/#hannahnasexpressionstypes","text":"","title":"hannah.nas.expressions.types"},{"location":"development/devel/api/#hannahnasexpressionsmetrics","text":"","title":"hannah.nas.expressions.metrics"},{"location":"development/devel/api/#hannahnasexpressionsplaceholder","text":"","title":"hannah.nas.expressions.placeholder"},{"location":"development/devel/api/#hannahnasexpressionsarithmetic","text":"","title":"hannah.nas.expressions.arithmetic"},{"location":"development/devel/api/#hannahnasops","text":"","title":"hannah.nas.ops"},{"location":"development/devel/api/#hannahnassearchpresamplersimple","text":"","title":"hannah.nas.search.presampler.simple"},{"location":"development/devel/api/#hannahnassearchsearch","text":"","title":"hannah.nas.search.search"},{"location":"development/devel/api/#hannahnassearchweight_sharing","text":"","title":"hannah.nas.search.weight_sharing"},{"location":"development/devel/api/#hannahnassearchutils","text":"","title":"hannah.nas.search.utils"},{"location":"development/devel/api/#hannahnassearchmodel_trainersimple_model_trainer","text":"","title":"hannah.nas.search.model_trainer.simple_model_trainer"},{"location":"development/devel/api/#hannahnassearchsearch_old","text":"","title":"hannah.nas.search.search_old"},{"location":"development/devel/api/#agingevolution-objects","text":"class AgingEvolution () Aging Evolution based multi objective optimization","title":"AgingEvolution Objects"},{"location":"development/devel/api/#next_parameters","text":"def next_parameters () Returns a list of current tasks","title":"next_parameters"},{"location":"development/devel/api/#tell_result","text":"def tell_result ( parameters , metrics ) Tell the result of a task","title":"tell_result"},{"location":"development/devel/api/#hannahnassearchsamplerrandom_sampler","text":"","title":"hannah.nas.search.sampler.random_sampler"},{"location":"development/devel/api/#hannahnassearchsamplerbase_sampler","text":"","title":"hannah.nas.search.sampler.base_sampler"},{"location":"development/devel/api/#sampler-objects","text":"class Sampler ( ABC )","title":"Sampler Objects"},{"location":"development/devel/api/#tell_result_1","text":"def tell_result ( parameters , metrics ) Tell the result of a task","title":"tell_result"},{"location":"development/devel/api/#hannahnassearchsamplermutator","text":"","title":"hannah.nas.search.sampler.mutator"},{"location":"development/devel/api/#hannahnassearchsampleraging_evolution","text":"","title":"hannah.nas.search.sampler.aging_evolution"},{"location":"development/devel/api/#agingevolutionsampler-objects","text":"class AgingEvolutionSampler ( Sampler ) Aging Evolution based multi objective optimization","title":"AgingEvolutionSampler Objects"},{"location":"development/devel/api/#next_parameters_1","text":"def next_parameters () Returns a list of current tasks","title":"next_parameters"},{"location":"development/devel/api/#tell_result_2","text":"def tell_result ( parameters , metrics ) Tell the result of a task","title":"tell_result"},{"location":"development/devel/api/#hannahnassearchsamplerdefined_space_sampler","text":"","title":"hannah.nas.search.sampler.defined_space_sampler"},{"location":"development/devel/api/#hannahnashardware_descriptionultratrail","text":"","title":"hannah.nas.hardware_description.ultratrail"},{"location":"development/devel/api/#hannahnashardware_descriptiondevice","text":"","title":"hannah.nas.hardware_description.device"},{"location":"development/devel/api/#hannahnashardware_description","text":"","title":"hannah.nas.hardware_description"},{"location":"development/devel/api/#hannahnashardware_descriptionmemory_type","text":"","title":"hannah.nas.hardware_description.memory_type"},{"location":"development/devel/api/#hannahnashardware_descriptiondescription","text":"","title":"hannah.nas.hardware_description.description"},{"location":"development/devel/api/#hannahnasfunctional_operatorstorch_conversion","text":"","title":"hannah.nas.functional_operators.torch_conversion"},{"location":"development/devel/api/#hannahnasfunctional_operatorsvisualizer","text":"","title":"hannah.nas.functional_operators.visualizer"},{"location":"development/devel/api/#hannahnasfunctional_operatorsoperators","text":"","title":"hannah.nas.functional_operators.operators"},{"location":"development/devel/api/#self_attention2d","text":"@torch . fx . wrap def self_attention2d ( q , k , v , num_heads , d_model , * , id ) Arguments : q - Tensor, shape [B, h*d, H, W] k - Tensor, shape [B, h*d, H, W] v - Tensor, shape [B, h*d, H, W]","title":"self_attention2d"},{"location":"development/devel/api/#hannahnasfunctional_operatorslazy","text":"","title":"hannah.nas.functional_operators.lazy"},{"location":"development/devel/api/#hannahnasfunctional_operatorsshapes","text":"","title":"hannah.nas.functional_operators.shapes"},{"location":"development/devel/api/#padding_expression_5","text":"def padding_expression ( kernel_size , stride , dilation = 1 ) Symbolically calculate padding such that for a given kernel_size, stride and dilation the padding is such that the output dimension is kept the same(stride=1) or halved(stride=2). Note: If the input dimension is 1 and stride = 2, the calculated padding will result in an output with also dimension 1.","title":"padding_expression"},{"location":"development/devel/api/#parameters_9","text":"kernel_size : Union[int, Expression] stride : Union[int, Expression] dilation : Union[int, Expression], optional description , by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_7","text":"Expression","title":"Returns"},{"location":"development/devel/api/#hannahnasfunctional_operatorsop","text":"","title":"hannah.nas.functional_operators.op"},{"location":"development/devel/api/#bypass-objects","text":"class Bypass ( Op ) Alternative Identity()","title":"Bypass Objects"},{"location":"development/devel/api/#hannahnasfunctional_operators","text":"","title":"hannah.nas.functional_operators"},{"location":"development/devel/api/#hannahnasfunctional_operatorsexecutor","text":"","title":"hannah.nas.functional_operators.executor"},{"location":"development/devel/api/#hannahnasparameterslazy","text":"","title":"hannah.nas.parameters.lazy"},{"location":"development/devel/api/#hannahnasparameters","text":"","title":"hannah.nas.parameters"},{"location":"development/devel/api/#hannahnasparametersiterators","text":"","title":"hannah.nas.parameters.iterators"},{"location":"development/devel/api/#hannahnasparametersparametrize","text":"","title":"hannah.nas.parameters.parametrize"},{"location":"development/devel/api/#hannahnasparametersparameters","text":"","title":"hannah.nas.parameters.parameters"},{"location":"development/devel/api/#hannahnasevalplot","text":"","title":"hannah.nas.eval.plot"},{"location":"development/devel/api/#hannahnasevalprepare","text":"","title":"hannah.nas.eval.prepare"},{"location":"development/devel/api/#prepare_summary","text":"def prepare_summary ( data : Dict [ str , str ], base_dir : str = \".\" , force : bool = False ) -> pd . DataFrame Prepare a summary of one or multiple nas runs Arguments : data Dict[str, str] - A mapping from a short name for a nas run \"e.g KWS\" to a folder containing nas history file e.g. \"trained_models/nas_kws/conv_net_trax\" base_dir str - base directory paths in data mapping are interpreted relative to base directory force bool - force reconstructing of cached results (\"data.pkl\")","title":"prepare_summary"},{"location":"development/devel/api/#hannahnaseval","text":"","title":"hannah.nas.eval"},{"location":"development/devel/api/#hannahnaseval__main__","text":"","title":"hannah.nas.eval.__main__"},{"location":"development/devel/api/#hannahnasevalextract","text":"","title":"hannah.nas.eval.extract"},{"location":"development/devel/api/#hannahnasconfig","text":"","title":"hannah.nas.config"},{"location":"development/devel/api/#scalar-objects","text":"@dataclass class Scalar () Representation of all the options to define a scalar.","title":"Scalar Objects"},{"location":"development/devel/api/#hannahnas","text":"","title":"hannah.nas"},{"location":"development/devel/api/#hannahnasspacesdartsdarts_space","text":"","title":"hannah.nas.spaces.darts.darts_space"},{"location":"development/devel/api/#hannahnasspacesmobilenetmobilenet","text":"","title":"hannah.nas.spaces.mobilenet.mobilenet"},{"location":"development/devel/api/#hannahnastesttest_functional_training","text":"","title":"hannah.nas.test.test_functional_training"},{"location":"development/devel/api/#hannahnastesttest_graph_transformer","text":"","title":"hannah.nas.test.test_graph_transformer"},{"location":"development/devel/api/#hannahnastesttest_description_ultratrail","text":"","title":"hannah.nas.test.test_description_ultratrail"},{"location":"development/devel/api/#hannahnastesttest_op_to_torch_conversion","text":"","title":"hannah.nas.test.test_op_to_torch_conversion"},{"location":"development/devel/api/#hannahnastesttest_onnx_export","text":"","title":"hannah.nas.test.test_onnx_export"},{"location":"development/devel/api/#hannahnastesttest_lazy_torch","text":"","title":"hannah.nas.test.test_lazy_torch"},{"location":"development/devel/api/#hannahnastesttest_add","text":"","title":"hannah.nas.test.test_add"},{"location":"development/devel/api/#hannahnastestnetwork","text":"","title":"hannah.nas.test.network"},{"location":"development/devel/api/#hannahnastesttest_arithmetic","text":"","title":"hannah.nas.test.test_arithmetic"},{"location":"development/devel/api/#test_unimplemeted","text":"@pytest . mark . parametrize ( \"x,y\" , [ ( IntScalarParameter ( 0 , 0 ), 2 ), ( IntScalarParameter ( 0 , 0 ), IntScalarParameter ( 0 , 0 )), ( DefaultInt ( 0 ), 2 ), ], ) def test_unimplemeted ( x , y ) Test that unimplemented methods raise unimplemented errors","title":"test_unimplemeted"},{"location":"development/devel/api/#hannahnastesttest_constraint_model","text":"","title":"hannah.nas.test.test_constraint_model"},{"location":"development/devel/api/#hannahnastesttest_parameters","text":"","title":"hannah.nas.test.test_parameters"},{"location":"development/devel/api/#hannahnastesttest_parametrize","text":"","title":"hannah.nas.test.test_parametrize"},{"location":"development/devel/api/#hannahnastesttest_adjacency","text":"","title":"hannah.nas.test.test_adjacency"},{"location":"development/devel/api/#hannahnastesttest_conditions","text":"","title":"hannah.nas.test.test_conditions"},{"location":"development/devel/api/#hannahnastesttest_z3","text":"","title":"hannah.nas.test.test_z3"},{"location":"development/devel/api/#hannahnastesttest_scoping","text":"","title":"hannah.nas.test.test_scoping"},{"location":"development/devel/api/#hannahnastesttest_functional_ops","text":"","title":"hannah.nas.test.test_functional_ops"},{"location":"development/devel/api/#conv_relu","text":"def conv_relu ( input , out_channels , kernel_size , stride ) Example for a functional block containing conv and relu","title":"conv_relu"},{"location":"development/devel/api/#hannahnastesttest_conv2d","text":"","title":"hannah.nas.test.test_conv2d"},{"location":"development/devel/api/#hannahnastesttest_dataflow","text":"","title":"hannah.nas.test.test_dataflow"},{"location":"development/devel/api/#hannahnastesttest_operators","text":"","title":"hannah.nas.test.test_operators"},{"location":"development/devel/api/#hannahnastesttest_darts_space","text":"","title":"hannah.nas.test.test_darts_space"},{"location":"development/devel/api/#hannahnastesttest_mobilenet","text":"","title":"hannah.nas.test.test_mobilenet"},{"location":"development/devel/api/#hannahnastesttest_functional_executor","text":"","title":"hannah.nas.test.test_functional_executor"},{"location":"development/devel/api/#hannahnastesttest_dfg_removal","text":"","title":"hannah.nas.test.test_dfg_removal"},{"location":"development/devel/api/#hannahnastesttest_repeat","text":"","title":"hannah.nas.test.test_repeat"},{"location":"development/devel/api/#hannahnasutils","text":"","title":"hannah.nas.utils"},{"location":"development/devel/api/#is_pareto","text":"def is_pareto ( costs , maximise = False ) Arguments : costs : An (n_points, n_costs) array Returns : A (n_points, ) boolean array, indicating whether each point is Pareto efficient","title":"is_pareto"},{"location":"development/devel/api/#hannahnas__main__","text":"","title":"hannah.nas.__main__"},{"location":"development/devel/api/#hannahnasconstraintsrandom_walk","text":"","title":"hannah.nas.constraints.random_walk"},{"location":"development/devel/api/#hannahnasconstraintsconstraint_model","text":"","title":"hannah.nas.constraints.constraint_model"},{"location":"development/devel/api/#hannahnasconstraintsdfg_constraint_model","text":"","title":"hannah.nas.constraints.dfg_constraint_model"},{"location":"development/devel/api/#constraintmodel-objects","text":"class ConstraintModel ()","title":"ConstraintModel Objects"},{"location":"development/devel/api/#process_optype","text":"def process_optype ( op : OpType ) Extracts the constraints based on the type of op. New variables are added to self.vars and the constraints are added to the solver.","title":"process_optype"},{"location":"development/devel/api/#parameters_10","text":"op : OpType","title":"Parameters"},{"location":"development/devel/api/#process_tensor","text":"def process_tensor ( tensor : Tensor ) Goes through all axis and extracts the constraints for the respective axis sizes","title":"process_tensor"},{"location":"development/devel/api/#parameters_11","text":"tensor : Tensor","title":"Parameters"},{"location":"development/devel/api/#hannahnasparametrization","text":"","title":"hannah.nas.parametrization"},{"location":"development/devel/api/#scalarparameterstate-objects","text":"@dataclass class ScalarParameterState ( ParameterState )","title":"ScalarParameterState Objects"},{"location":"development/devel/api/#sigma","text":"variance of sampling parameter","title":"sigma"},{"location":"development/devel/api/#hannahnascoreparametrized","text":"","title":"hannah.nas.core.parametrized"},{"location":"development/devel/api/#hannahnascore","text":"","title":"hannah.nas.core"},{"location":"development/devel/api/#hannahnascoreexpression","text":"","title":"hannah.nas.core.expression"},{"location":"development/devel/api/#hannahnasfxtracer","text":"","title":"hannah.nas.fx.tracer"},{"location":"development/devel/api/#inliningtracer-objects","text":"class InliningTracer ( SearchSpaceTracer ) Inline all search space functions, into the graph. This generates a standard pytorch.fx graph module containing only replacing the search space parametrizable functions with their equivalent form torch.functional`","title":"InliningTracer Objects"},{"location":"development/devel/api/#hannahnasexportonnx","text":"","title":"hannah.nas.export.onnx"},{"location":"development/devel/api/#eval_2","text":"def eval ( exp_tree : Any ) -> Any Recursively evaluate expressions organized as a pytree","title":"eval"},{"location":"development/devel/api/#hannahnasexport","text":"","title":"hannah.nas.export"},{"location":"development/devel/api/#hannahnasplotter","text":"","title":"hannah.nas.plotter"},{"location":"development/devel/api/#hannahnasperformance_predictionexamplesgcn_predictor_example","text":"","title":"hannah.nas.performance_prediction.examples.gcn_predictor_example"},{"location":"development/devel/api/#hannahnasperformance_predictionexamplesri_capsule_performance_predictor","text":"","title":"hannah.nas.performance_prediction.examples.ri_capsule_performance_predictor"},{"location":"development/devel/api/#hannahnasperformance_predictionexamplesgaussian_process_predictor_example","text":"","title":"hannah.nas.performance_prediction.examples.gaussian_process_predictor_example"},{"location":"development/devel/api/#hannahnasperformance_predictionexamplesxgb_predictor_example","text":"","title":"hannah.nas.performance_prediction.examples.xgb_predictor_example"},{"location":"development/devel/api/#hannahnasperformance_predictionexamplesgcn_model_example","text":"","title":"hannah.nas.performance_prediction.examples.gcn_model_example"},{"location":"development/devel/api/#hannahnasperformance_predictiongcnpredictor","text":"","title":"hannah.nas.performance_prediction.gcn.predictor"},{"location":"development/devel/api/#predictor-objects","text":"class Predictor ()","title":"Predictor Objects"},{"location":"development/devel/api/#__init___4","text":"def __init__ ( fea_name = \"features\" ) -> None Parent method for different predictor classes.","title":"__init__"},{"location":"development/devel/api/#parameters_12","text":"fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features'","title":"Parameters"},{"location":"development/devel/api/#train_2","text":"def train ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 1 ) Train GCN model","title":"train"},{"location":"development/devel/api/#parameters_13","text":"dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch, by default 1","title":"Parameters"},{"location":"development/devel/api/#predict_1","text":"def predict ( graph ) predict cost of graph","title":"predict"},{"location":"development/devel/api/#parameters_14","text":"graph : dgl.Graph","title":"Parameters"},{"location":"development/devel/api/#returns_8","text":"torch.Tensor predicted cost of given graph. Retrieve float value with .item()","title":"Returns"},{"location":"development/devel/api/#gcnpredictor-objects","text":"class GCNPredictor ( Predictor )","title":"GCNPredictor Objects"},{"location":"development/devel/api/#__init___5","text":"def __init__ ( input_feature_size , hidden_units = [ 128 ], readout = \"mean\" , fea_name = \"features\" ) -> None G(raph)CN based network latency/cost predictor. End-to-end from graph to score.","title":"__init__"},{"location":"development/devel/api/#parameters_15","text":"input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features'","title":"Parameters"},{"location":"development/devel/api/#train_and_fit","text":"def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 0 ) Train GCN model","title":"train_and_fit"},{"location":"development/devel/api/#parameters_16","text":"dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch, by default 1","title":"Parameters"},{"location":"development/devel/api/#predict_2","text":"def predict ( graph ) predict cost of graph","title":"predict"},{"location":"development/devel/api/#parameters_17","text":"graph : dgl.Graph","title":"Parameters"},{"location":"development/devel/api/#returns_9","text":"torch.Tensor predicted cost of given graph. Retrieve float value with .item()","title":"Returns"},{"location":"development/devel/api/#gaussianprocesspredictor-objects","text":"class GaussianProcessPredictor ( Predictor )","title":"GaussianProcessPredictor Objects"},{"location":"development/devel/api/#__init___6","text":"def __init__ ( input_feature_size , hidden_units = 128 , embedding_size = 10 , readout = \"mean\" , fea_name = \"features\" , kernel = \"default\" , alpha = 1e-10 ) -> None Predictor that generates a graph embedding that is used as input for a gaussian process predictor.","title":"__init__"},{"location":"development/devel/api/#parameters_18","text":"input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 embedding_size: int, optional size of output embedding readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' kernel : str, sklearn.gaussian_process.kernels.Kernel, optional The gaussian process kernel to use. input shoudl be either \"default\", or a sklearn Kernel() object by default RBF() + DotProduct() + WhiteKernel()","title":"Parameters"},{"location":"development/devel/api/#train_and_fit_1","text":"def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , validation_dataloader = None , verbose = 1 ) Train GCN model, generate embeddings for training data and fit the predictor with embeddings.","title":"train_and_fit"},{"location":"development/devel/api/#parameters_19","text":"dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional by default 200 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch,by default 1","title":"Parameters"},{"location":"development/devel/api/#returns_10","text":"float score of predictor on TRAINING data, see sklearn doc of chosen predictor for more info","title":"Returns"},{"location":"development/devel/api/#predict_3","text":"def predict ( X , return_std = True ) Predict cost/latency of graphs.","title":"predict"},{"location":"development/devel/api/#parameters_20","text":"X : dgl.DGLGraph, list[DGLGraph], dgl.dataloading.GraphDataLoader Input graph(s) return_std : bool, optional if true, return standard dev. else just mean prediction, by default True","title":"Parameters"},{"location":"development/devel/api/#returns_11","text":"array (,array) prediction(s) , (if return_std: standard deviation(s))","title":"Returns"},{"location":"development/devel/api/#xgbpredictor-objects","text":"class XGBPredictor ( Predictor )","title":"XGBPredictor Objects"},{"location":"development/devel/api/#__init___7","text":"def __init__ ( input_feature_size , hidden_units = 128 , embedding_size = 10 , readout = \"mean\" , fea_name = \"features\" , xgb_param = \"default\" ) -> None Predictor that generates a graph embedding that is used as input for a xgb based predictor.","title":"__init__"},{"location":"development/devel/api/#parameters_21","text":"input_feature_size : [type] length of feature vector of a graph node (graph G with n nodes, each with features of length m, i.e. feature matrix F = n x m) hidden_units : int, list, optional size of hidden layer (layers if list) , by default 128 embedding_size: int, optional size of output embedding readout : str, optional readout function that is used to aggregate node features, by default 'mean' fea_name : str, optional internal name for features in the graph, as in graph.ndata[fea_name], by default 'features' xgb_param : str, dict, optional The xgb_parameter to use. See https://xgboost.readthedocs.io/en/latest/parameter.html","title":"Parameters"},{"location":"development/devel/api/#train_and_fit_2","text":"def train_and_fit ( dataloader , learning_rate = 1e-3 , num_epochs = 200 , num_round = 8000 , validation_dataloader = None , verbose = 1 ) Train GCN model, generate embeddings for training data and fit the predictor with embeddings.","title":"train_and_fit"},{"location":"development/devel/api/#parameters_22","text":"dataloader : GraphDataLoader training data learning_rate : [type], optional by default 1e-3 num_epochs : int, optional Training epochs for the GCN embedding network, by default 200 num_round : int, optional training rounds for xgb booster, by default 800 validation_dataloader : [type], optional if given, use this data to print validation loss, by default None verbose : int if validation_dataloader is given, print validation MSE every epoch,by default 1","title":"Parameters"},{"location":"development/devel/api/#predict_4","text":"def predict ( X ) Predict cost/latency of graphs.","title":"predict"},{"location":"development/devel/api/#parameters_23","text":"X : dgl.DGLGraph, list[DGLGraph], dgl.dataloading.GraphDataLoader Input graph(s) Returns array (,array) prediction(s) , (if return_std: standard deviation(s))","title":"Parameters"},{"location":"development/devel/api/#prepare_dataloader","text":"def prepare_dataloader ( dataset , batch_size = 50 , train_test_split = 1 , subset = 0 , seed = 0 , validation = False ) helper function to construct dataloaders from NASGraphDataset","title":"prepare_dataloader"},{"location":"development/devel/api/#parameters_24","text":"dataset : NASGraphDataset batch_size : int, optional by default 50 train_test_split : float, optional number between 0 and 1, the proportion of the dataset to be used for training, by default 1 subset : int, optional choose only many samples from the dataset. Set 0 for disabling, i.e. whole dataset. by default 0 seed : int, optional set seed for reproduceability validation : bool, optional also output a validation set e.g. for hyperparam tuning","title":"Parameters"},{"location":"development/devel/api/#returns_12","text":"tuple(GraphDataLoader, (GraphDataLoader), GraphDataLoader) training dataloader to be used in CostPredictor.train() and test/validation dataloader if train_test_split > 0, else len(test_dataloader) == 0","title":"Returns"},{"location":"development/devel/api/#hannahnasperformance_predictiongcnmodel","text":"","title":"hannah.nas.performance_prediction.gcn.model"},{"location":"development/devel/api/#hannahnasperformance_prediction","text":"","title":"hannah.nas.performance_prediction"},{"location":"development/devel/api/#hannahnasperformance_predictionfeaturesgraph_conversion","text":"","title":"hannah.nas.performance_prediction.features.graph_conversion"},{"location":"development/devel/api/#hannahnasperformance_predictionfeaturesdataset","text":"","title":"hannah.nas.performance_prediction.features.dataset"},{"location":"development/devel/api/#hannahnasperformance_predictionsimple","text":"","title":"hannah.nas.performance_prediction.simple"},{"location":"development/devel/api/#macpredictor-objects","text":"class MACPredictor () A predictor class that instantiates the model and calculates abstract metrics","title":"MACPredictor Objects"},{"location":"development/devel/api/#gcnpredictor-objects_1","text":"class GCNPredictor () A predictor class that instantiates the model and uses the backends predict function to predict performance metrics","title":"GCNPredictor Objects"},{"location":"development/devel/api/#hannahnasperformance_predictionmlonmcupredictor","text":"","title":"hannah.nas.performance_prediction.mlonmcu.predictor"},{"location":"development/devel/api/#hannahnasperformance_predictionmlonmcu","text":"","title":"hannah.nas.performance_prediction.mlonmcu"},{"location":"development/devel/api/#hannahnasperformance_predictionprotocol","text":"","title":"hannah.nas.performance_prediction.protocol"},{"location":"development/devel/api/#predictor-objects_1","text":"class Predictor ( Protocol )","title":"Predictor Objects"},{"location":"development/devel/api/#predict_5","text":"def predict ( model : ClassifierModule , input : Optional [ InputShape ] = None ) -> Mapping [ str , float ] Pedicts performance metrisc of a model. Performance metrics are returned as a dictionary with the metric name as key and the metric value as floating point value. Arguments : model ClassifierModule - The model to predict the performance of. input __type_, optional_ - Input shape of input . Defaults to None.","title":"predict"},{"location":"development/devel/debugging/","text":"Debugging Hannah supports several debugging tools. Trainer Debugging flags: Most of the lightning trainer debug flags are usable through the hydra configs using trainer.flag name The most important ones are: trainer.fast_dev_run=True This provides a kind of unit test for the model by running one single batch of training, validation and test on the model. trainer.overfit_batches=0.01 This tests if the model can overfit a small percentage (0.01) of the dataset. A good model should be able reach almost 100 percent accuracy very quickly. trainer.limit_*_batches This limits the amount of batches used for training. e.g.: hannah-train trainer.limit_train_batches=0.1 trainer.limit_val_batches=0.1 trainer.limit_test_batches=0.1 Limits the training to 10 percent of available batches for train, validation and test.","title":"Debugging"},{"location":"development/devel/debugging/#debugging","text":"Hannah supports several debugging tools.","title":"Debugging"},{"location":"development/devel/debugging/#trainer-debugging-flags","text":"Most of the lightning trainer debug flags are usable through the hydra configs using trainer.flag name The most important ones are:","title":"Trainer Debugging flags:"},{"location":"development/devel/debugging/#trainerfast_dev_runtrue","text":"This provides a kind of unit test for the model by running one single batch of training, validation and test on the model.","title":"trainer.fast_dev_run=True"},{"location":"development/devel/debugging/#traineroverfit_batches001","text":"This tests if the model can overfit a small percentage (0.01) of the dataset. A good model should be able reach almost 100 percent accuracy very quickly.","title":"trainer.overfit_batches=0.01"},{"location":"development/devel/debugging/#trainerlimit__batches","text":"This limits the amount of batches used for training. e.g.: hannah-train trainer.limit_train_batches=0.1 trainer.limit_val_batches=0.1 trainer.limit_test_batches=0.1 Limits the training to 10 percent of available batches for train, validation and test.","title":"trainer.limit_*_batches"},{"location":"development/devel/overview/","text":"Development Guidelines The main development repository is available at: https://es-git.cs.uni-tuebingen.de/es/ai/hannah/hannah Code Style This project uses pre commit hooks for auto formatting and static code analysis. To enable pre commit hooks run the following command in a poetry shell . pre-commit install Naming Conventions Try to follow (pep8)[https://pep8.org/#naming-conventions] and the rest of pep8 to the best of your abilities. The short summary of pep8 is: Use all lowercase names with underscores for methods/functions/modules/packages. Use CamelCase for Classes and Type Variables Use all upperscore with underscores for constants Logging / Console Messages Try to avoid print for doing outputs whenever possible. Use python module (logging)[https://docs.python.org/3.8/howto/logging.html] instead. Default logglevel for informational messages should be logging.info() . If you want to be able to have more finegrained control, use a dedicated logger for your module e.g.: logger = logging.getLogger('datasets.speech') ... logger.info() logger.debug() Testing / CI Try to provide unit tests for new features, unit tests should be written using pytest . Unit test should be designed to run pretty fast and are run for each push. Tests marked with `@pytest.mark.integration are only triggered when --integration is given on the pytest commandline. These tests are only run on pushs to the main branch. Resolving merge conflicts in poetry.lock If you have changed poetry.lock this can result in merge conflicts. The easiest way to resolve them is: git checkout --theirs poetry.lock poetry lock --no-update Try to avoid running poetry update on feature branches. Branching We apply the following code branch naming convention. main : main development branch should be as stable as possible f/* : feature branches, development branches (will be squash merged after review, and successful passing of merge request pipeline) fix/* bug fix branches, branches containing little self contained bug fixes for existing features (feel free to merge without code review, if you can reasonably assume that they are not breaking anything that is not already broken) pub/* : publication branches, branches containing the version corresponding to a specific, full branch name should contain abbreviation of publication and publication year. Publication branches and main branch are automatically mirrored to the public github repository: https://github.com/ekut-es/hannah","title":"Overview"},{"location":"development/devel/overview/#development-guidelines","text":"The main development repository is available at: https://es-git.cs.uni-tuebingen.de/es/ai/hannah/hannah","title":"Development Guidelines"},{"location":"development/devel/overview/#code-style","text":"This project uses pre commit hooks for auto formatting and static code analysis. To enable pre commit hooks run the following command in a poetry shell . pre-commit install","title":"Code Style"},{"location":"development/devel/overview/#naming-conventions","text":"Try to follow (pep8)[https://pep8.org/#naming-conventions] and the rest of pep8 to the best of your abilities. The short summary of pep8 is: Use all lowercase names with underscores for methods/functions/modules/packages. Use CamelCase for Classes and Type Variables Use all upperscore with underscores for constants","title":"Naming Conventions"},{"location":"development/devel/overview/#logging-console-messages","text":"Try to avoid print for doing outputs whenever possible. Use python module (logging)[https://docs.python.org/3.8/howto/logging.html] instead. Default logglevel for informational messages should be logging.info() . If you want to be able to have more finegrained control, use a dedicated logger for your module e.g.: logger = logging.getLogger('datasets.speech') ... logger.info() logger.debug()","title":"Logging / Console Messages"},{"location":"development/devel/overview/#testing-ci","text":"Try to provide unit tests for new features, unit tests should be written using pytest . Unit test should be designed to run pretty fast and are run for each push. Tests marked with `@pytest.mark.integration are only triggered when --integration is given on the pytest commandline. These tests are only run on pushs to the main branch.","title":"Testing / CI"},{"location":"development/devel/overview/#resolving-merge-conflicts-in-poetrylock","text":"If you have changed poetry.lock this can result in merge conflicts. The easiest way to resolve them is: git checkout --theirs poetry.lock poetry lock --no-update Try to avoid running poetry update on feature branches.","title":"Resolving merge conflicts in poetry.lock"},{"location":"development/devel/overview/#branching","text":"We apply the following code branch naming convention. main : main development branch should be as stable as possible f/* : feature branches, development branches (will be squash merged after review, and successful passing of merge request pipeline) fix/* bug fix branches, branches containing little self contained bug fixes for existing features (feel free to merge without code review, if you can reasonably assume that they are not breaking anything that is not already broken) pub/* : publication branches, branches containing the version corresponding to a specific, full branch name should contain abbreviation of publication and publication year. Publication branches and main branch are automatically mirrored to the public github repository: https://github.com/ekut-es/hannah","title":"Branching"},{"location":"development/devel/profiling/","text":"Profiling Tools To debug training inefficiencies we support the following profiling tools: pytorch autograd profiler: hannah-train profiler=simple trainer.max_epochs=1 profiler=pytorch By default profiles are logged to the trainings output directory and can be visualized using tensorboard. For the example run use: tensorboard --logdir trained_models/test/tc-res8 Simple Profiler: hannah-train profiler=simple trainer.max_epochs=1 profiler=simple Generates a summary of the durations of the individual training steps: Action | Mean duration (s) |Num calls | Total time (s) | Percentage % | ------------------------------------------------------------------------------------------------------------------------------------ Total | - |_ | 29.42 | 100 % | ------------------------------------------------------------------------------------------------------------------------------------ run_training_epoch | 21.692 |1 | 21.692 | 73.733 | run_training_batch | 0.030893 |288 | 8.8971 | 30.241 | optimizer_step_and_closure_0 | 0.030348 |288 | 8.7402 | 29.708 | get_train_batch | 0.022253 |288 | 6.4087 | 21.783 | evaluation_step_and_end | 0.021021 |277 | 5.8228 | 19.792 | validation_step | 0.020913 |277 | 5.7929 | 19.69 | training_step_and_backward | 0.019199 |288 | 5.5292 | 18.794 | model_forward | 0.010312 |288 | 2.9697 | 10.094 | training_step | 0.010076 |288 | 2.9017 | 9.8631 | model_backward | 0.0076062 |288 | 2.1906 | 7.4458 | on_validation_epoch_end | 0.091247 |2 | 0.18249 | 0.6203 | on_train_batch_end | 0.0003854 |288 | 0.111 | 0.37728 | on_train_start | 0.09102 |1 | 0.09102 | 0.30938 | on_train_epoch_end | 0.055033 |1 | 0.055033 | 0.18706 | cache_result | 1.6399e-05 |2290 | 0.037554 | 0.12765 | on_validation_batch_end | 9.3069e-05 |277 | 0.02578 | 0.087628 | on_validation_end | 0.008392 |2 | 0.016784 | 0.05705 | on_train_end | 0.0075968 |1 | 0.0075968 | 0.025822 | on_batch_start | 2.4213e-05 |288 | 0.0069733 | 0.023702 | on_after_backward | 1.5198e-05 |288 | 0.004377 | 0.014877 | on_before_zero_grad | 1.4302e-05 |288 | 0.0041189 | 0.014 | on_train_batch_start | 1.264e-05 |288 | 0.0036403 | 0.012374 | on_batch_end | 1.1853e-05 |288 | 0.0034136 | 0.011603 | training_step_end | 9.123e-06 |288 | 0.0026274 | 0.0089307 | on_validation_batch_start | 8.5754e-06 |277 | 0.0023754 | 0.008074 | validation_step_end | 7.5117e-06 |277 | 0.0020807 | 0.0070725 | on_validation_start | 0.00019774 |2 | 0.00039547 | 0.0013442 | on_epoch_start | 0.00021634 |1 | 0.00021634 | 0.00073533 | on_epoch_end | 1.4326e-05 |3 | 4.2978e-05 | 0.00014608 | on_validation_epoch_start | 1.0761e-05 |2 | 2.1523e-05 | 7.3157e-05 | on_fit_start | 2.0443e-05 |1 | 2.0443e-05 | 6.9486e-05 | on_train_epoch_start | 1.2379e-05 |1 | 1.2379e-05 | 4.2077e-05 | on_before_accelerator_backend_setup | 1.1862e-05 |1 | 1.1862e-05 | 4.0319e-05 | Advanced Profiler hannah-train profiler=simple trainer.max_epochs=1 profiler=advanced Creates python level performance summaries for the individual Training steps: Profile stats for : optimizer_step_and_closure_0 8064 function calls in 8 . 897 seconds Ordered by : cumulative time List reduced from 25 to 20 due to restriction < 20 > ncalls tottime percall cumtime percall filename : lineno ( function ) 288 0 . 001 0 . 000 8 . 897 0 . 031 accelerator . py : 265 ( optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 accelerator . py : 282 ( run_optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 training_type_plugin . py : 159 ( optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 grad_mode . py : 12 ( decorate_context ) 288 0 . 000 0 . 000 8 . 894 0 . 031 adamw . py : 54 ( step ) 288 0 . 000 0 . 000 8 . 893 0 . 031 training_loop . py : 648 ( train_step_and_backward_closure ) 288 0 . 000 0 . 000 8 . 893 0 . 031 training_loop . py : 737 ( training_step_and_backward ) 288 0 . 000 0 . 000 8 . 892 0 . 031 contextlib . py : 108 ( __enter__ ) 288 0 . 000 0 . 000 8 . 892 0 . 031 { built - in method builtins . next } 288 0 . 000 0 . 000 8 . 892 0 . 031 profilers . py : 62 ( profile ) 288 0 . 000 0 . 000 8 . 892 0 . 031 profilers . py : 250 ( start ) 288 8 . 892 0 . 031 8 . 892 0 . 031 { method ' enable ' of ' _lsprof.Profiler ' objects } 288 0 . 000 0 . 000 0 . 001 0 . 000 grad_mode . py : 65 ( __enter__ ) 288 0 . 000 0 . 000 0 . 000 0 . 000 accelerator . py : 104 ( lightning_module ) 288 0 . 000 0 . 000 0 . 000 0 . 000 contextlib . py : 238 ( helper ) 288 0 . 000 0 . 000 0 . 000 0 . 000 training_type_plugin . py : 91 ( lightning_module ) 288 0 . 000 0 . 000 0 . 000 0 . 000 grad_mode . py : 104 ( __enter__ ) 288 0 . 000 0 . 000 0 . 000 0 . 000 contextlib . py : 82 ( __init__ ) 576 0 . 000 0 . 000 0 . 000 0 . 000 { built - in method torch . _C . is_grad_enabled } 288 0 . 000 0 . 000 0 . 000 0 . 000 base . py : 82 ( unwrap_lightning_module )","title":"Profiling"},{"location":"development/devel/profiling/#profiling-tools","text":"To debug training inefficiencies we support the following profiling tools:","title":"Profiling Tools"},{"location":"development/devel/profiling/#pytorch-autograd-profiler","text":"hannah-train profiler=simple trainer.max_epochs=1 profiler=pytorch By default profiles are logged to the trainings output directory and can be visualized using tensorboard. For the example run use: tensorboard --logdir trained_models/test/tc-res8","title":"pytorch autograd profiler:"},{"location":"development/devel/profiling/#simple-profiler","text":"hannah-train profiler=simple trainer.max_epochs=1 profiler=simple Generates a summary of the durations of the individual training steps: Action | Mean duration (s) |Num calls | Total time (s) | Percentage % | ------------------------------------------------------------------------------------------------------------------------------------ Total | - |_ | 29.42 | 100 % | ------------------------------------------------------------------------------------------------------------------------------------ run_training_epoch | 21.692 |1 | 21.692 | 73.733 | run_training_batch | 0.030893 |288 | 8.8971 | 30.241 | optimizer_step_and_closure_0 | 0.030348 |288 | 8.7402 | 29.708 | get_train_batch | 0.022253 |288 | 6.4087 | 21.783 | evaluation_step_and_end | 0.021021 |277 | 5.8228 | 19.792 | validation_step | 0.020913 |277 | 5.7929 | 19.69 | training_step_and_backward | 0.019199 |288 | 5.5292 | 18.794 | model_forward | 0.010312 |288 | 2.9697 | 10.094 | training_step | 0.010076 |288 | 2.9017 | 9.8631 | model_backward | 0.0076062 |288 | 2.1906 | 7.4458 | on_validation_epoch_end | 0.091247 |2 | 0.18249 | 0.6203 | on_train_batch_end | 0.0003854 |288 | 0.111 | 0.37728 | on_train_start | 0.09102 |1 | 0.09102 | 0.30938 | on_train_epoch_end | 0.055033 |1 | 0.055033 | 0.18706 | cache_result | 1.6399e-05 |2290 | 0.037554 | 0.12765 | on_validation_batch_end | 9.3069e-05 |277 | 0.02578 | 0.087628 | on_validation_end | 0.008392 |2 | 0.016784 | 0.05705 | on_train_end | 0.0075968 |1 | 0.0075968 | 0.025822 | on_batch_start | 2.4213e-05 |288 | 0.0069733 | 0.023702 | on_after_backward | 1.5198e-05 |288 | 0.004377 | 0.014877 | on_before_zero_grad | 1.4302e-05 |288 | 0.0041189 | 0.014 | on_train_batch_start | 1.264e-05 |288 | 0.0036403 | 0.012374 | on_batch_end | 1.1853e-05 |288 | 0.0034136 | 0.011603 | training_step_end | 9.123e-06 |288 | 0.0026274 | 0.0089307 | on_validation_batch_start | 8.5754e-06 |277 | 0.0023754 | 0.008074 | validation_step_end | 7.5117e-06 |277 | 0.0020807 | 0.0070725 | on_validation_start | 0.00019774 |2 | 0.00039547 | 0.0013442 | on_epoch_start | 0.00021634 |1 | 0.00021634 | 0.00073533 | on_epoch_end | 1.4326e-05 |3 | 4.2978e-05 | 0.00014608 | on_validation_epoch_start | 1.0761e-05 |2 | 2.1523e-05 | 7.3157e-05 | on_fit_start | 2.0443e-05 |1 | 2.0443e-05 | 6.9486e-05 | on_train_epoch_start | 1.2379e-05 |1 | 1.2379e-05 | 4.2077e-05 | on_before_accelerator_backend_setup | 1.1862e-05 |1 | 1.1862e-05 | 4.0319e-05 |","title":"Simple Profiler:"},{"location":"development/devel/profiling/#advanced-profiler","text":"hannah-train profiler=simple trainer.max_epochs=1 profiler=advanced Creates python level performance summaries for the individual Training steps: Profile stats for : optimizer_step_and_closure_0 8064 function calls in 8 . 897 seconds Ordered by : cumulative time List reduced from 25 to 20 due to restriction < 20 > ncalls tottime percall cumtime percall filename : lineno ( function ) 288 0 . 001 0 . 000 8 . 897 0 . 031 accelerator . py : 265 ( optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 accelerator . py : 282 ( run_optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 training_type_plugin . py : 159 ( optimizer_step ) 288 0 . 000 0 . 000 8 . 896 0 . 031 grad_mode . py : 12 ( decorate_context ) 288 0 . 000 0 . 000 8 . 894 0 . 031 adamw . py : 54 ( step ) 288 0 . 000 0 . 000 8 . 893 0 . 031 training_loop . py : 648 ( train_step_and_backward_closure ) 288 0 . 000 0 . 000 8 . 893 0 . 031 training_loop . py : 737 ( training_step_and_backward ) 288 0 . 000 0 . 000 8 . 892 0 . 031 contextlib . py : 108 ( __enter__ ) 288 0 . 000 0 . 000 8 . 892 0 . 031 { built - in method builtins . next } 288 0 . 000 0 . 000 8 . 892 0 . 031 profilers . py : 62 ( profile ) 288 0 . 000 0 . 000 8 . 892 0 . 031 profilers . py : 250 ( start ) 288 8 . 892 0 . 031 8 . 892 0 . 031 { method ' enable ' of ' _lsprof.Profiler ' objects } 288 0 . 000 0 . 000 0 . 001 0 . 000 grad_mode . py : 65 ( __enter__ ) 288 0 . 000 0 . 000 0 . 000 0 . 000 accelerator . py : 104 ( lightning_module ) 288 0 . 000 0 . 000 0 . 000 0 . 000 contextlib . py : 238 ( helper ) 288 0 . 000 0 . 000 0 . 000 0 . 000 training_type_plugin . py : 91 ( lightning_module ) 288 0 . 000 0 . 000 0 . 000 0 . 000 grad_mode . py : 104 ( __enter__ ) 288 0 . 000 0 . 000 0 . 000 0 . 000 contextlib . py : 82 ( __init__ ) 576 0 . 000 0 . 000 0 . 000 0 . 000 { built - in method torch . _C . is_grad_enabled } 288 0 . 000 0 . 000 0 . 000 0 . 000 base . py : 82 ( unwrap_lightning_module )","title":"Advanced Profiler"},{"location":"evaluation/eval/","text":"Evaluation mode Evaluation of trained models can be run using hannah-eval checkpints=[<list of ckpt files>]","title":"Evaluation"},{"location":"evaluation/eval/#evaluation-mode","text":"Evaluation of trained models can be run using hannah-eval checkpints=[<list of ckpt files>]","title":"Evaluation mode"},{"location":"neural-architecture-search/nas/eval/","text":"Evaluation Note Coming soon","title":"Evaluating Results"},{"location":"neural-architecture-search/nas/eval/#evaluation","text":"Note Coming soon","title":"Evaluation"},{"location":"neural-architecture-search/nas/legacy/","text":"Warning This is the old documentation for the NAS. A lot has changed and a lot will still change in the future so handle with care. Neural architecture search In contrast to hyperparameter optimization neural architecture search, explores new neural network hyperparameters. A aging evolution based neural architecture search has been implemented as a hydra plugin: hannah-train --config-name config_unas To launch multiple configuration jobs in parallel use joblib launcher: hannah-train --config-name config_unas hydra/launcher=joblib Parametrization for neural architecture search need to be given as YAML configuration files at the moment. For an example see: speech_recognition/conf/config_unas.yaml Parametrization The Parametrization contains the following elements: Choice Parameter Choice Parameters select options from a list of parameters. They are configured as a list of options in the parameters. Example: conv_size: [1,3,5,7,9,11] Choice List Parameters Choice List Parameters represent a variable length list of Choices. They are configured with the follwing parameters: min Minimum length of list max Maximum length of list+1 choices List of Choices Example: min : 4 max : 10 choices : - _target_ : \"torch.nn.Conv2d\" size : 3 - _target : \"torch.nn.MaxPool2d\" size : 7 Warning : Mutations for intervall parameters currently always sample randomly from the range of values Intervall Parameters Intervall Parameters represent a Scalar Value from an intervall of Values They are configure with the following parameters: lower lower bound of intervall [lower, upper[ upper upper bound of intervall [lower, upper[ int set to true to generate integers log set to true to generate log scaled distribution Subset Parameter Subset Parameters select a subset of a list of choices. They are configured using the following parameters: choices List of choices to sample from size size of the subset to generate Partition Parameter Partition parameters split the list of choices into a predefined number of partitions. They are configured using the following parameters: choices List of choices to partition partition Number of partitions to generate","title":"NAS (Legacy)"},{"location":"neural-architecture-search/nas/legacy/#neural-architecture-search","text":"In contrast to hyperparameter optimization neural architecture search, explores new neural network hyperparameters. A aging evolution based neural architecture search has been implemented as a hydra plugin: hannah-train --config-name config_unas To launch multiple configuration jobs in parallel use joblib launcher: hannah-train --config-name config_unas hydra/launcher=joblib Parametrization for neural architecture search need to be given as YAML configuration files at the moment. For an example see: speech_recognition/conf/config_unas.yaml","title":"Neural architecture search"},{"location":"neural-architecture-search/nas/legacy/#parametrization","text":"The Parametrization contains the following elements:","title":"Parametrization"},{"location":"neural-architecture-search/nas/legacy/#choice-parameter","text":"Choice Parameters select options from a list of parameters. They are configured as a list of options in the parameters. Example: conv_size: [1,3,5,7,9,11]","title":"Choice Parameter"},{"location":"neural-architecture-search/nas/legacy/#choice-list-parameters","text":"Choice List Parameters represent a variable length list of Choices. They are configured with the follwing parameters: min Minimum length of list max Maximum length of list+1 choices List of Choices Example: min : 4 max : 10 choices : - _target_ : \"torch.nn.Conv2d\" size : 3 - _target : \"torch.nn.MaxPool2d\" size : 7 Warning : Mutations for intervall parameters currently always sample randomly from the range of values","title":"Choice List Parameters"},{"location":"neural-architecture-search/nas/legacy/#intervall-parameters","text":"Intervall Parameters represent a Scalar Value from an intervall of Values They are configure with the following parameters: lower lower bound of intervall [lower, upper[ upper upper bound of intervall [lower, upper[ int set to true to generate integers log set to true to generate log scaled distribution","title":"Intervall Parameters"},{"location":"neural-architecture-search/nas/legacy/#subset-parameter","text":"Subset Parameters select a subset of a list of choices. They are configured using the following parameters: choices List of choices to sample from size size of the subset to generate","title":"Subset Parameter"},{"location":"neural-architecture-search/nas/legacy/#partition-parameter","text":"Partition parameters split the list of choices into a predefined number of partitions. They are configured using the following parameters: choices List of choices to partition partition Number of partitions to generate","title":"Partition Parameter"},{"location":"neural-architecture-search/nas/metrics/","text":"Performance Metrics in Neural Architecture Search Currently there are two main sources of Performance Metrics used in the hannah's NAS subystem. Backend generated metrics. Backends generated metrics are returned by the backend's profile method. Backend generated metrics are usually generated by running the neural networks, either on real target hardware or on accurate simulators. We currently do not enforce accuracy requirements on the reported metrics, but we will consider them as golden reference results for the evaluation and if necessary the training of the performance estimators, so they should be as accurate as possible. Estimators can provide metrics before the neural networks have been trained. Predictors are used in presampling phases of the neural architecture search. Predictors are not and will not be used outside of neural architecture search. There are 2 subclasses of predictors. - Machine Learning based predictors: These predictors provide an interface based on: predict , update , load , train - Analytical predictors, the interface of these methods only contains the: predict The predictor interfaces are defined in hannah.nas.performance_prediction.protcol as python protocols. `","title":"Optimization Metrics"},{"location":"neural-architecture-search/nas/metrics/#performance-metrics-in-neural-architecture-search","text":"Currently there are two main sources of Performance Metrics used in the hannah's NAS subystem. Backend generated metrics. Backends generated metrics are returned by the backend's profile method. Backend generated metrics are usually generated by running the neural networks, either on real target hardware or on accurate simulators. We currently do not enforce accuracy requirements on the reported metrics, but we will consider them as golden reference results for the evaluation and if necessary the training of the performance estimators, so they should be as accurate as possible. Estimators can provide metrics before the neural networks have been trained. Predictors are used in presampling phases of the neural architecture search. Predictors are not and will not be used outside of neural architecture search. There are 2 subclasses of predictors. - Machine Learning based predictors: These predictors provide an interface based on: predict , update , load , train - Analytical predictors, the interface of these methods only contains the: predict The predictor interfaces are defined in hannah.nas.performance_prediction.protcol as python protocols. `","title":"Performance Metrics in Neural Architecture Search"},{"location":"neural-architecture-search/nas/parametrization/","text":"Note Coming soon Parametrization Usage in Search Space Basic Parameter Types IntScalarParameter FloatScalarParameter CategoricalParameter Advanced Parameters Choice SymbolicAttr SymbolicSequence Placeholders Expressions","title":"Parametrization"},{"location":"neural-architecture-search/nas/parametrization/#parametrization","text":"","title":"Parametrization"},{"location":"neural-architecture-search/nas/parametrization/#usage-in-search-space","text":"","title":"Usage in Search Space"},{"location":"neural-architecture-search/nas/parametrization/#basic-parameter-types","text":"","title":"Basic Parameter Types"},{"location":"neural-architecture-search/nas/parametrization/#intscalarparameter","text":"","title":"IntScalarParameter"},{"location":"neural-architecture-search/nas/parametrization/#floatscalarparameter","text":"","title":"FloatScalarParameter"},{"location":"neural-architecture-search/nas/parametrization/#categoricalparameter","text":"","title":"CategoricalParameter"},{"location":"neural-architecture-search/nas/parametrization/#advanced-parameters","text":"","title":"Advanced Parameters"},{"location":"neural-architecture-search/nas/parametrization/#choice","text":"","title":"Choice"},{"location":"neural-architecture-search/nas/parametrization/#symbolicattr","text":"","title":"SymbolicAttr"},{"location":"neural-architecture-search/nas/parametrization/#symbolicsequence","text":"","title":"SymbolicSequence"},{"location":"neural-architecture-search/nas/parametrization/#placeholders","text":"","title":"Placeholders"},{"location":"neural-architecture-search/nas/parametrization/#expressions","text":"","title":"Expressions"},{"location":"neural-architecture-search/nas/search/","text":"Search Note Coming soon","title":"Search"},{"location":"neural-architecture-search/nas/search/#search","text":"Note Coming soon","title":"Search"},{"location":"neural-architecture-search/nas/search_spaces/","text":"Warning The search spaces in HANNAH are currently under construction. If you run into bugs, please contact us. Search Spaces Search spaces in HANNAH are directed graphs (DAGs) where the nodes are Ops or Tensors and the edges indicate data movement. Note Search spaces are not executable themselves but need an Executor which uses the current parametrization state to build a forward . from hannah.nas.functional_operators.operators import Conv2d Basic Building Blocks Ops & Tensors Op nodes represent the operators used in the networks of the search space. Their basic syntax is # usage var_name = Operator ( param0 = val0 , param1 = val1 , ... )( * operands ) Tensor nodes indicate the presence of data in the graph. They do not themselves contain actual values when the search space graph is defined (the actual data is managed by the Executor ). The tensor node defines attributes that the data has at this point in the graph (e.g., shape, axis names, datatypes, ...). from hannah.nas.functional_operators.operators import Conv2d from hannah.nas.functional_operators.op import Tensor input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( \"N\" , \"C\" , \"H\" , \"W\" )) weight = Tensor ( name = 'weight' , shape = ( 32 , 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv = Conv2d ( stride = 2 , dilation = 1 ) # Define operator and parametrization graph = conv ( input , weight ) # Define/create/extend graph graph Conv2d(Conv2d_0) A set of basic operators is implemented in HANNAH, among them Convolution (1D, 2D) Linear BatchNorm Relu Add and more operators will be added in the future. It is also easy to define a new operator, see Custom Ops . Parametrization & Expressions Note For more information about parametrization and expressions, see Parametrization . To build a search space it is not sufficient to feed scalar values to operator parameters. Instead, one can use parameters . from hannah.nas.parameters.parameters import CategoricalParameter , IntScalarParameter input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( \"N\" , \"C\" , \"H\" , \"W\" )) weight = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) # a search space with stride 1 and stride 2 convolutions graph = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight ) graph . parametrization ( flatten = True ) {'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), None: CategoricalParameter(rng = Generator(PCG64), name = stride, id = None, choices = [1, 2], current_value = 1)} As futher explained in Parametrization , parameters are expressions and can be combined to more complex expressions , encoding properties of the search space symbolically. One common use-case is symbolically expressing shapes. Consider for example the following: in_channel = 3 kernel_size = 1 input = Tensor ( name = 'input' , shape = ( 1 , in_channel , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) weight_0 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), in_channel , kernel_size , kernel_size ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_0 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight_0 ) How can we know the output shape of conv_0 , e.g., to put it into the weight tensor of a following convolution, without knowing what value the out_channel parameter has? --> Each node has a method .shape() which returns the shape as an expression and can be used interchangeably with actual values. Those expressions are then only evaluated at sampling and during the forward. print ( \"Input shape: \" , input . shape ()) print ( \"Weight shape: \" , weight_0 . shape ()) print ( \"Convolution output shape:\" , conv_0 . shape ()) Input shape: (1, 3, 32, 32) Weight shape: (IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 3, 1, 1) Convolution output shape: (1, IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), <hannah.nas.expressions.arithmetic.Floor object at 0x7ff93a6ce8c0>, <hannah.nas.expressions.arithmetic.Floor object at 0x7ff93a6ce7a0>) The lazy keyword can be used to evaluate values which might be parameters (but could also be int or else). from hannah.nas.functional_operators.lazy import lazy print ( \"Input shape: \" , [ lazy ( i ) for i in input . shape ()]) print ( \"Weight shape: \" , [ lazy ( i ) for i in weight_0 . shape ()]) print ( \"Convolution output shape:\" , [ lazy ( i ) for i in conv_0 . shape ()]) Input shape: [1, 3, 32, 32] Weight shape: [8, 3, 1, 1] Convolution output shape: [1, 8, 16, 16] When defining an operator, one also has to define a shape function (the default shape function is identity, i.e., output_shape == input_shape ). Tensors return their own shape. Graphs and Hierarchy As seen in the simple examples above, we can chain op and tensor nodes together to create graphs and use parameters to span search spaces. from hannah.nas.functional_operators.operators import Relu input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) weight_0 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_0 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight_0 ) relu_0 = Relu ()( conv_0 ) weight_1 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 32 , max = 64 , name = 'out_channels' ), conv_0 . shape ()[ 1 ], 3 , 3 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_1 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( relu_0 , weight_1 ) relu_1 = Relu ()( conv_1 ) relu_1 . parametrization ( flatten = True ) {'Conv2d_1.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_1.weight.out_channels, min = 32, max = 64, step_size = 1, current_value = 32), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 2), 'Conv2d_1.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_1.stride, choices = [1, 2], current_value = 2)} Nodes have operands for backwards traversal and users for forward traversal. With helper functions like get_nodes one can iterate through all graph nodes. from hannah.nas.functional_operators.op import get_nodes print ( \"Relu Operands: \" , relu_1 . operands ) print ( \"Conv Users: \" , relu_1 . operands [ 0 ] . users ) print ( \" \\n Nodes:\" ) for node in get_nodes ( relu_1 ): print ( 'Node:' , node ) print ( ' \\t Operands: ' , node . operands ) Relu Operands: [Conv2d(Conv2d_1)] Conv Users: [Relu(Relu_1)] Nodes: Node: Relu(Relu_1) Operands: [Conv2d(Conv2d_1)] Node: Conv2d(Conv2d_1) Operands: [Relu(Relu_0), Tensor(Conv2d_1.weight)] Node: Tensor(Conv2d_1.weight) Operands: [] Node: Relu(Relu_0) Operands: [Conv2d(Conv2d_0)] Node: Conv2d(Conv2d_0) Operands: [Tensor(input), Tensor(Conv2d_0.weight)] Node: Tensor(Conv2d_0.weight) Operands: [] Node: Tensor(input) Operands: [] Blocks Creating large graphs with a lot of operators and tensors manually can get tedious and convoluted. Instead, we can define search space graphs in a hierarchical manner by encapsulating them in functions: def conv_relu ( input , kernel_size , out_channels , stride ): in_channels = input . shape ()[ 1 ] weight = Tensor ( name = 'weight' , shape = ( out_channels , in_channels , kernel_size , kernel_size ), axis = ( 'O' , 'I' , 'kH' , 'kW' ), grad = True ) conv = Conv2d ( stride = stride )( input , weight ) relu = Relu ()( conv ) return relu input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size , out_channels = out_channels , stride = stride ) net = conv_relu ( net , kernel_size = kernel_size , out_channels = out_channels , stride = stride ) for n in get_nodes ( net ): print ( n ) Relu(Relu_1) Conv2d(Conv2d_1) Tensor(Conv2d_1.weight) Relu(Relu_0) Conv2d(Conv2d_0) Tensor(Conv2d_0.weight) Tensor(input) net . parametrization ( flatten = True ) {'Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 5), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 1)} Note, how there is just one set of parameters. If defined this way, both blocks share their parameters. To define seperate parameters one can use param.new() input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net . parametrization ( flatten = True ) {'Conv2d_1.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_1.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'Conv2d_1.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_1.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 2), 'Conv2d_1.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_1.stride, choices = [1, 2], current_value = 2)} These function blocks can be nested as desired. def block ( input ): kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) for n in get_nodes ( net ): print ( n ) Relu(Relu_5) Conv2d(Conv2d_5) Tensor(Conv2d_5.weight) Relu(Relu_4) Conv2d(Conv2d_4) Tensor(Conv2d_4.weight) Relu(Relu_3) Conv2d(Conv2d_3) Tensor(Conv2d_3.weight) Relu(Relu_2) Conv2d(Conv2d_2) Tensor(Conv2d_2.weight) Relu(Relu_1) Conv2d(Conv2d_1) Tensor(Conv2d_1.weight) Relu(Relu_0) Conv2d(Conv2d_0) Tensor(Conv2d_0.weight) Tensor(input) Scopes As seen above, while the definition of the graph is made in a hierarchical manner, the actual graph and its node are \"flat\" and do not have any inherent hierarchy. To make the graph more clear and readable one can use scopes with the @scope decorator for blocks. Note that @scope does not have any effect on the inherent structure of the graph but only affects the node id s. from hannah.nas.functional_operators.op import scope @scope def conv_relu ( input , kernel_size , out_channels , stride ): in_channels = input . shape ()[ 1 ] weight = Tensor ( name = 'weight' , shape = ( out_channels , in_channels , kernel_size , kernel_size ), axis = ( 'O' , 'I' , 'kH' , 'kW' ), grad = True ) conv = Conv2d ( stride = stride )( input , weight ) relu = Relu ()( conv ) return relu @scope def block ( input ): kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) for n in get_nodes ( net ): print ( n ) Relu(block_1.conv_relu_2.Relu_0) Conv2d(block_1.conv_relu_2.Conv2d_0) Tensor(block_1.conv_relu_2.Conv2d_0.weight) Relu(block_1.conv_relu_1.Relu_0) Conv2d(block_1.conv_relu_1.Conv2d_0) Tensor(block_1.conv_relu_1.Conv2d_0.weight) Relu(block_1.conv_relu_0.Relu_0) Conv2d(block_1.conv_relu_0.Conv2d_0) Tensor(block_1.conv_relu_0.Conv2d_0.weight) Relu(block_0.conv_relu_2.Relu_0) Conv2d(block_0.conv_relu_2.Conv2d_0) Tensor(block_0.conv_relu_2.Conv2d_0.weight) Relu(block_0.conv_relu_1.Relu_0) Conv2d(block_0.conv_relu_1.Conv2d_0) Tensor(block_0.conv_relu_1.Conv2d_0.weight) Relu(block_0.conv_relu_0.Relu_0) Conv2d(block_0.conv_relu_0.Conv2d_0) Tensor(block_0.conv_relu_0.Conv2d_0.weight) Tensor(input) Choice Ops A choice op is a special node kind that allows to have multiple paths in the graph that exclude each other (or have other specialized behaviour). from hannah.nas.functional_operators.operators import Identity from functools import partial from hannah.nas.functional_operators.op import ChoiceOp @scope def choice_block ( input ): kernel_size = CategoricalParameter ([ 1 , 3 , 5 ], name = 'kernel_size' ) out_channels = IntScalarParameter ( min = 4 , max = 64 , name = 'out_channels' ) stride = CategoricalParameter ([ 1 , 2 ], name = 'stride' ) identity = Identity () optional_conv = partial ( conv_relu , out_channels = out_channels . new (), stride = stride . new (), kernel_size = kernel_size . new ()) net = ChoiceOp ( identity , optional_conv )( input ) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) conv = conv_relu ( input , out_channels = out_channels . new (), stride = stride . new (), kernel_size = kernel_size . new ()) net = choice_block ( conv ) net . parametrization ( flatten = True ) {'choice_block_0.ChoiceOp_0.choice': IntScalarParameter(rng = Generator(PCG64), name = choice, id = choice_block_0.ChoiceOp_0.choice, min = 0, max = 1, step_size = 1, current_value = 0), 'choice_block_0.conv_relu_1.Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = choice_block_0.conv_relu_1.Conv2d_0.stride, choices = [1, 2], current_value = 2), 'choice_block_0.conv_relu_1.Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = choice_block_0.conv_relu_1.Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 5), 'choice_block_0.conv_relu_1.Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = choice_block_0.conv_relu_1.Conv2d_0.weight.out_channels, min = 4, max = 64, step_size = 1, current_value = 4), 'conv_relu_0.Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = conv_relu_0.Conv2d_0.stride, choices = [1, 2], current_value = 2), 'conv_relu_0.Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = conv_relu_0.Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'conv_relu_0.Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = conv_relu_0.Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8)} Note When defining options for a choice node, one can either use ops directly (see Identity() above) or use block functions ( conv_relu ). For block functions, one has to use functools.partial to enable the choice node to perform the respective integration in the graph. During execution, the choice node can be leveraged to define the behaviour (e.g., select one and only one path, execute all paths and return a parametrized sum for differential NAS, ...). Choice nodes can, for example, be used to search over different operator types, different operator patterns, or to implement dynamic depth/a variable amount of layers/blocks. def dynamic_depth ( * exits , switch ): return ChoiceOp ( * exits , switch = switch )() Custom Ops To define custom operators, one can inherit from the Op class. Then, one can override the __call__(self, *operands) class to perform specific actions, e.g., saving certain parameters of the operands as fields of the operator instance that is returned. Don't forget to call super().__call__(*operands) , which performs the integration of the new operator instance into the graph. Then, one has to provide a _forward_implementation(self, *args) , which defines the computation that the operator executes. Lastly, a shape_fun(self) defines the output shape of the operator. Executor The search space graphs are not themselves executable. For that one needs an Executor . The BasicExecutor analyzes the graph to find dependencies and a valid node order (e.g., to execute the results of operands first before they are added in an Add operation) and builds a forward function. It also registers torch parameters and buffers for training.The executor should be usable as a normal torch.nn.Module . One can define custom executors, e.g., for weight sharing NAS or differential NAS. import torch from hannah.nas.functional_operators.executor import BasicExecutor input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) model = BasicExecutor ( net ) model . initialize () x = torch . randn ( input . shape ()) model . forward ( x ) tensor([[[[0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000]], [[0.0255, 0.0000, 0.0000, 0.0000], [0.0000, 0.0152, 0.0000, 0.0000], [0.0000, 0.0000, 0.0898, 0.0000], [0.1132, 0.0894, 0.0094, 0.0138]], [[0.0000, 0.0000, 0.0365, 0.0000], [0.0000, 0.1532, 0.0000, 0.2529], [0.0000, 0.0859, 0.0396, 0.0000], [0.0000, 0.2311, 0.0757, 0.0000]], [[0.0000, 0.1285, 0.1754, 0.0000], [0.1788, 0.1729, 0.1973, 0.1036], [0.1823, 0.2994, 0.2293, 0.2580], [0.0554, 0.2454, 0.1355, 0.3018]], [[0.0000, 0.0234, 0.0000, 0.0000], [0.0725, 0.0212, 0.0615, 0.0960], [0.1040, 0.0960, 0.1613, 0.0927], [0.1025, 0.0846, 0.0000, 0.0424]], [[0.0000, 0.0000, 0.0672, 0.0818], [0.0000, 0.1420, 0.0404, 0.0326], [0.0000, 0.0000, 0.0000, 0.1140], [0.0000, 0.1518, 0.1521, 0.2088]], [[0.0000, 0.0995, 0.1362, 0.0000], [0.0000, 0.1206, 0.0000, 0.0000], [0.0000, 0.1001, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0435]], [[0.0000, 0.0000, 0.0000, 0.0245], [0.0000, 0.0938, 0.0000, 0.0763], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)","title":"Search Spaces"},{"location":"neural-architecture-search/nas/search_spaces/#search-spaces","text":"Search spaces in HANNAH are directed graphs (DAGs) where the nodes are Ops or Tensors and the edges indicate data movement. Note Search spaces are not executable themselves but need an Executor which uses the current parametrization state to build a forward . from hannah.nas.functional_operators.operators import Conv2d","title":"Search Spaces"},{"location":"neural-architecture-search/nas/search_spaces/#basic-building-blocks","text":"","title":"Basic Building Blocks"},{"location":"neural-architecture-search/nas/search_spaces/#ops-tensors","text":"Op nodes represent the operators used in the networks of the search space. Their basic syntax is # usage var_name = Operator ( param0 = val0 , param1 = val1 , ... )( * operands ) Tensor nodes indicate the presence of data in the graph. They do not themselves contain actual values when the search space graph is defined (the actual data is managed by the Executor ). The tensor node defines attributes that the data has at this point in the graph (e.g., shape, axis names, datatypes, ...). from hannah.nas.functional_operators.operators import Conv2d from hannah.nas.functional_operators.op import Tensor input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( \"N\" , \"C\" , \"H\" , \"W\" )) weight = Tensor ( name = 'weight' , shape = ( 32 , 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv = Conv2d ( stride = 2 , dilation = 1 ) # Define operator and parametrization graph = conv ( input , weight ) # Define/create/extend graph graph Conv2d(Conv2d_0) A set of basic operators is implemented in HANNAH, among them Convolution (1D, 2D) Linear BatchNorm Relu Add and more operators will be added in the future. It is also easy to define a new operator, see Custom Ops .","title":"Ops &amp; Tensors"},{"location":"neural-architecture-search/nas/search_spaces/#parametrization-expressions","text":"Note For more information about parametrization and expressions, see Parametrization . To build a search space it is not sufficient to feed scalar values to operator parameters. Instead, one can use parameters . from hannah.nas.parameters.parameters import CategoricalParameter , IntScalarParameter input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( \"N\" , \"C\" , \"H\" , \"W\" )) weight = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) # a search space with stride 1 and stride 2 convolutions graph = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight ) graph . parametrization ( flatten = True ) {'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), None: CategoricalParameter(rng = Generator(PCG64), name = stride, id = None, choices = [1, 2], current_value = 1)} As futher explained in Parametrization , parameters are expressions and can be combined to more complex expressions , encoding properties of the search space symbolically. One common use-case is symbolically expressing shapes. Consider for example the following: in_channel = 3 kernel_size = 1 input = Tensor ( name = 'input' , shape = ( 1 , in_channel , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) weight_0 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), in_channel , kernel_size , kernel_size ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_0 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight_0 ) How can we know the output shape of conv_0 , e.g., to put it into the weight tensor of a following convolution, without knowing what value the out_channel parameter has? --> Each node has a method .shape() which returns the shape as an expression and can be used interchangeably with actual values. Those expressions are then only evaluated at sampling and during the forward. print ( \"Input shape: \" , input . shape ()) print ( \"Weight shape: \" , weight_0 . shape ()) print ( \"Convolution output shape:\" , conv_0 . shape ()) Input shape: (1, 3, 32, 32) Weight shape: (IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 3, 1, 1) Convolution output shape: (1, IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), <hannah.nas.expressions.arithmetic.Floor object at 0x7ff93a6ce8c0>, <hannah.nas.expressions.arithmetic.Floor object at 0x7ff93a6ce7a0>) The lazy keyword can be used to evaluate values which might be parameters (but could also be int or else). from hannah.nas.functional_operators.lazy import lazy print ( \"Input shape: \" , [ lazy ( i ) for i in input . shape ()]) print ( \"Weight shape: \" , [ lazy ( i ) for i in weight_0 . shape ()]) print ( \"Convolution output shape:\" , [ lazy ( i ) for i in conv_0 . shape ()]) Input shape: [1, 3, 32, 32] Weight shape: [8, 3, 1, 1] Convolution output shape: [1, 8, 16, 16] When defining an operator, one also has to define a shape function (the default shape function is identity, i.e., output_shape == input_shape ). Tensors return their own shape.","title":"Parametrization &amp; Expressions"},{"location":"neural-architecture-search/nas/search_spaces/#graphs-and-hierarchy","text":"As seen in the simple examples above, we can chain op and tensor nodes together to create graphs and use parameters to span search spaces. from hannah.nas.functional_operators.operators import Relu input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) weight_0 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 8 , max = 64 , name = 'out_channels' ), 3 , 1 , 1 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_0 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( input , weight_0 ) relu_0 = Relu ()( conv_0 ) weight_1 = Tensor ( name = 'weight' , shape = ( IntScalarParameter ( min = 32 , max = 64 , name = 'out_channels' ), conv_0 . shape ()[ 1 ], 3 , 3 ), axis = ( \"O\" , \"I\" , \"kH\" , \"kW\" )) conv_1 = Conv2d ( stride = CategoricalParameter ( name = 'stride' , choices = [ 1 , 2 ]))( relu_0 , weight_1 ) relu_1 = Relu ()( conv_1 ) relu_1 . parametrization ( flatten = True ) {'Conv2d_1.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_1.weight.out_channels, min = 32, max = 64, step_size = 1, current_value = 32), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 2), 'Conv2d_1.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_1.stride, choices = [1, 2], current_value = 2)} Nodes have operands for backwards traversal and users for forward traversal. With helper functions like get_nodes one can iterate through all graph nodes. from hannah.nas.functional_operators.op import get_nodes print ( \"Relu Operands: \" , relu_1 . operands ) print ( \"Conv Users: \" , relu_1 . operands [ 0 ] . users ) print ( \" \\n Nodes:\" ) for node in get_nodes ( relu_1 ): print ( 'Node:' , node ) print ( ' \\t Operands: ' , node . operands ) Relu Operands: [Conv2d(Conv2d_1)] Conv Users: [Relu(Relu_1)] Nodes: Node: Relu(Relu_1) Operands: [Conv2d(Conv2d_1)] Node: Conv2d(Conv2d_1) Operands: [Relu(Relu_0), Tensor(Conv2d_1.weight)] Node: Tensor(Conv2d_1.weight) Operands: [] Node: Relu(Relu_0) Operands: [Conv2d(Conv2d_0)] Node: Conv2d(Conv2d_0) Operands: [Tensor(input), Tensor(Conv2d_0.weight)] Node: Tensor(Conv2d_0.weight) Operands: [] Node: Tensor(input) Operands: []","title":"Graphs and Hierarchy"},{"location":"neural-architecture-search/nas/search_spaces/#blocks","text":"Creating large graphs with a lot of operators and tensors manually can get tedious and convoluted. Instead, we can define search space graphs in a hierarchical manner by encapsulating them in functions: def conv_relu ( input , kernel_size , out_channels , stride ): in_channels = input . shape ()[ 1 ] weight = Tensor ( name = 'weight' , shape = ( out_channels , in_channels , kernel_size , kernel_size ), axis = ( 'O' , 'I' , 'kH' , 'kW' ), grad = True ) conv = Conv2d ( stride = stride )( input , weight ) relu = Relu ()( conv ) return relu input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size , out_channels = out_channels , stride = stride ) net = conv_relu ( net , kernel_size = kernel_size , out_channels = out_channels , stride = stride ) for n in get_nodes ( net ): print ( n ) Relu(Relu_1) Conv2d(Conv2d_1) Tensor(Conv2d_1.weight) Relu(Relu_0) Conv2d(Conv2d_0) Tensor(Conv2d_0.weight) Tensor(input) net . parametrization ( flatten = True ) {'Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 5), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 1)} Note, how there is just one set of parameters. If defined this way, both blocks share their parameters. To define seperate parameters one can use param.new() input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net . parametrization ( flatten = True ) {'Conv2d_1.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_1.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'Conv2d_1.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_1.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8), 'Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_0.stride, choices = [1, 2], current_value = 2), 'Conv2d_1.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = Conv2d_1.stride, choices = [1, 2], current_value = 2)} These function blocks can be nested as desired. def block ( input ): kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) for n in get_nodes ( net ): print ( n ) Relu(Relu_5) Conv2d(Conv2d_5) Tensor(Conv2d_5.weight) Relu(Relu_4) Conv2d(Conv2d_4) Tensor(Conv2d_4.weight) Relu(Relu_3) Conv2d(Conv2d_3) Tensor(Conv2d_3.weight) Relu(Relu_2) Conv2d(Conv2d_2) Tensor(Conv2d_2.weight) Relu(Relu_1) Conv2d(Conv2d_1) Tensor(Conv2d_1.weight) Relu(Relu_0) Conv2d(Conv2d_0) Tensor(Conv2d_0.weight) Tensor(input)","title":"Blocks"},{"location":"neural-architecture-search/nas/search_spaces/#scopes","text":"As seen above, while the definition of the graph is made in a hierarchical manner, the actual graph and its node are \"flat\" and do not have any inherent hierarchy. To make the graph more clear and readable one can use scopes with the @scope decorator for blocks. Note that @scope does not have any effect on the inherent structure of the graph but only affects the node id s. from hannah.nas.functional_operators.op import scope @scope def conv_relu ( input , kernel_size , out_channels , stride ): in_channels = input . shape ()[ 1 ] weight = Tensor ( name = 'weight' , shape = ( out_channels , in_channels , kernel_size , kernel_size ), axis = ( 'O' , 'I' , 'kH' , 'kW' ), grad = True ) conv = Conv2d ( stride = stride )( input , weight ) relu = Relu ()( conv ) return relu @scope def block ( input ): kernel_size = CategoricalParameter ( name = \"kernel_size\" , choices = [ 1 , 3 , 5 ]) stride = CategoricalParameter ( name = \"stride\" , choices = [ 1 , 2 ]) out_channels = IntScalarParameter ( name = \"out_channels\" , min = 8 , max = 64 ) net = conv_relu ( input , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) net = conv_relu ( net , kernel_size = kernel_size . new (), out_channels = out_channels . new (), stride = stride . new ()) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) for n in get_nodes ( net ): print ( n ) Relu(block_1.conv_relu_2.Relu_0) Conv2d(block_1.conv_relu_2.Conv2d_0) Tensor(block_1.conv_relu_2.Conv2d_0.weight) Relu(block_1.conv_relu_1.Relu_0) Conv2d(block_1.conv_relu_1.Conv2d_0) Tensor(block_1.conv_relu_1.Conv2d_0.weight) Relu(block_1.conv_relu_0.Relu_0) Conv2d(block_1.conv_relu_0.Conv2d_0) Tensor(block_1.conv_relu_0.Conv2d_0.weight) Relu(block_0.conv_relu_2.Relu_0) Conv2d(block_0.conv_relu_2.Conv2d_0) Tensor(block_0.conv_relu_2.Conv2d_0.weight) Relu(block_0.conv_relu_1.Relu_0) Conv2d(block_0.conv_relu_1.Conv2d_0) Tensor(block_0.conv_relu_1.Conv2d_0.weight) Relu(block_0.conv_relu_0.Relu_0) Conv2d(block_0.conv_relu_0.Conv2d_0) Tensor(block_0.conv_relu_0.Conv2d_0.weight) Tensor(input)","title":"Scopes"},{"location":"neural-architecture-search/nas/search_spaces/#choice-ops","text":"A choice op is a special node kind that allows to have multiple paths in the graph that exclude each other (or have other specialized behaviour). from hannah.nas.functional_operators.operators import Identity from functools import partial from hannah.nas.functional_operators.op import ChoiceOp @scope def choice_block ( input ): kernel_size = CategoricalParameter ([ 1 , 3 , 5 ], name = 'kernel_size' ) out_channels = IntScalarParameter ( min = 4 , max = 64 , name = 'out_channels' ) stride = CategoricalParameter ([ 1 , 2 ], name = 'stride' ) identity = Identity () optional_conv = partial ( conv_relu , out_channels = out_channels . new (), stride = stride . new (), kernel_size = kernel_size . new ()) net = ChoiceOp ( identity , optional_conv )( input ) return net input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) conv = conv_relu ( input , out_channels = out_channels . new (), stride = stride . new (), kernel_size = kernel_size . new ()) net = choice_block ( conv ) net . parametrization ( flatten = True ) {'choice_block_0.ChoiceOp_0.choice': IntScalarParameter(rng = Generator(PCG64), name = choice, id = choice_block_0.ChoiceOp_0.choice, min = 0, max = 1, step_size = 1, current_value = 0), 'choice_block_0.conv_relu_1.Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = choice_block_0.conv_relu_1.Conv2d_0.stride, choices = [1, 2], current_value = 2), 'choice_block_0.conv_relu_1.Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = choice_block_0.conv_relu_1.Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 5), 'choice_block_0.conv_relu_1.Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = choice_block_0.conv_relu_1.Conv2d_0.weight.out_channels, min = 4, max = 64, step_size = 1, current_value = 4), 'conv_relu_0.Conv2d_0.stride': CategoricalParameter(rng = Generator(PCG64), name = stride, id = conv_relu_0.Conv2d_0.stride, choices = [1, 2], current_value = 2), 'conv_relu_0.Conv2d_0.weight.kernel_size': CategoricalParameter(rng = Generator(PCG64), name = kernel_size, id = conv_relu_0.Conv2d_0.weight.kernel_size, choices = [1, 3, 5], current_value = 3), 'conv_relu_0.Conv2d_0.weight.out_channels': IntScalarParameter(rng = Generator(PCG64), name = out_channels, id = conv_relu_0.Conv2d_0.weight.out_channels, min = 8, max = 64, step_size = 1, current_value = 8)} Note When defining options for a choice node, one can either use ops directly (see Identity() above) or use block functions ( conv_relu ). For block functions, one has to use functools.partial to enable the choice node to perform the respective integration in the graph. During execution, the choice node can be leveraged to define the behaviour (e.g., select one and only one path, execute all paths and return a parametrized sum for differential NAS, ...). Choice nodes can, for example, be used to search over different operator types, different operator patterns, or to implement dynamic depth/a variable amount of layers/blocks. def dynamic_depth ( * exits , switch ): return ChoiceOp ( * exits , switch = switch )()","title":"Choice Ops"},{"location":"neural-architecture-search/nas/search_spaces/#custom-ops","text":"To define custom operators, one can inherit from the Op class. Then, one can override the __call__(self, *operands) class to perform specific actions, e.g., saving certain parameters of the operands as fields of the operator instance that is returned. Don't forget to call super().__call__(*operands) , which performs the integration of the new operator instance into the graph. Then, one has to provide a _forward_implementation(self, *args) , which defines the computation that the operator executes. Lastly, a shape_fun(self) defines the output shape of the operator.","title":"Custom Ops"},{"location":"neural-architecture-search/nas/search_spaces/#executor","text":"The search space graphs are not themselves executable. For that one needs an Executor . The BasicExecutor analyzes the graph to find dependencies and a valid node order (e.g., to execute the results of operands first before they are added in an Add operation) and builds a forward function. It also registers torch parameters and buffers for training.The executor should be usable as a normal torch.nn.Module . One can define custom executors, e.g., for weight sharing NAS or differential NAS. import torch from hannah.nas.functional_operators.executor import BasicExecutor input = Tensor ( name = 'input' , shape = ( 1 , 3 , 32 , 32 ), axis = ( 'N' , 'C' , 'H' , 'W' )) net = block ( input ) net = block ( net ) model = BasicExecutor ( net ) model . initialize () x = torch . randn ( input . shape ()) model . forward ( x ) tensor([[[[0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000]], [[0.0255, 0.0000, 0.0000, 0.0000], [0.0000, 0.0152, 0.0000, 0.0000], [0.0000, 0.0000, 0.0898, 0.0000], [0.1132, 0.0894, 0.0094, 0.0138]], [[0.0000, 0.0000, 0.0365, 0.0000], [0.0000, 0.1532, 0.0000, 0.2529], [0.0000, 0.0859, 0.0396, 0.0000], [0.0000, 0.2311, 0.0757, 0.0000]], [[0.0000, 0.1285, 0.1754, 0.0000], [0.1788, 0.1729, 0.1973, 0.1036], [0.1823, 0.2994, 0.2293, 0.2580], [0.0554, 0.2454, 0.1355, 0.3018]], [[0.0000, 0.0234, 0.0000, 0.0000], [0.0725, 0.0212, 0.0615, 0.0960], [0.1040, 0.0960, 0.1613, 0.0927], [0.1025, 0.0846, 0.0000, 0.0424]], [[0.0000, 0.0000, 0.0672, 0.0818], [0.0000, 0.1420, 0.0404, 0.0326], [0.0000, 0.0000, 0.0000, 0.1140], [0.0000, 0.1518, 0.1521, 0.2088]], [[0.0000, 0.0995, 0.1362, 0.0000], [0.0000, 0.1206, 0.0000, 0.0000], [0.0000, 0.1001, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0435]], [[0.0000, 0.0000, 0.0000, 0.0245], [0.0000, 0.0938, 0.0000, 0.0763], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)","title":"Executor"},{"location":"neural-architecture-search/nas/usage/","text":"Note Coming soon Introduction Usage NAS CLI CL options Debugging in VSCode","title":"Usage"},{"location":"neural-architecture-search/nas/usage/#introduction","text":"","title":"Introduction"},{"location":"neural-architecture-search/nas/usage/#usage","text":"","title":"Usage"},{"location":"neural-architecture-search/nas/usage/#nas-cli","text":"","title":"NAS CLI"},{"location":"neural-architecture-search/nas/usage/#cl-options","text":"","title":"CL options"},{"location":"neural-architecture-search/nas/usage/#debugging-in-vscode","text":"","title":"Debugging in VSCode"},{"location":"optimization/optimization/hyperparameter/","text":"Hyperparameter Search The hydra based configuration management allows using multiple hydra sweeper plugins for hyperparameter optimization. The optuna sweeper is installed by default. Sweeper plugins support specifying the hyperparameter search space on the commandline. For example the following command optimizes the learning rate and weight decay parameters for the default optimizer ( adamw ): hannah-train scheduler.max_lr='interval(0.00001, 0.001)' optimizer.weight_decay='interval(0.00001, 0.001)' hydra/sweeper=optuna hydra/launcher=joblib -m Parametrization can be given on the commandline as well as using configuration files for a detailed documentation have a look at basic override syntax and extended override syntax . Optuna Options The optuna hyperparameter optimizer has the following options. hannah-train hydra/sweeper = optuna --cfg hydra -p hydra.sweeper # @package hydra.sweeper sampler: _target_: optuna.samplers.TPESampler seed: null consider_prior: true prior_weight: 1 .0 consider_magic_clip: true consider_endpoints: false n_startup_trials: 10 n_ei_candidates: 24 multivariate: false warn_independent_sampling: true _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper direction: minimize storage: null study_name: null n_trials: 20 n_jobs: 2 search_space: {} Try especially increasing the number of trials to run n_trials , and if the sweeper options are dependent on each other try enabling multivariate sampling e.g. sampler.multivariate . State Persistence If you want to save the optimization results for later analyis you can save them to a relational or redis database by setting the study_name and storage options. e.g.: hannah-train experiment_id=\"test_optuna_resume\" hydra/sweeper=optuna hydra.sweeper.study_name=' ${ experiment_id } ' hydra.sweeper.storage='sqlite:/// ${ experiment_id } .sqlite' trainer.max_epochs=15 scheduler.max_lr='interval(0.0001, 0.1)' module.num_workers=4 -m The tuning results can then be visualized using the tools from optuna.visualize : A simple example would be: import optuna storage = \"sqlite:///test_optuna_resume.sqlite\" study_name = \"test_optuna_resume\" def main (): study = optuna . load_study ( study_name , storage ) fig = optuna . visualization . plot_optimization_history ( study ) fig . write_image ( \"history.png\" ) fig . write_image ( \"history.pdf\" ) if __name__ == '__main__' : main () For a slightly extended visualization script have a look at scripts/optuna_visualize_results.py or use optuna-dashboard for a web based visualization. Resuming Trials An additional benefit of exporting the study state to database is that trials can be somewhat resumed at a later time by just restarting the trial with the same parameters. Although this has a view problems: - The hydra sweeper will start it's job numbering from scratch Running on ml-cloud It is possible to use the sqlite backend to store trials on ml-cloud for an example look at scripts/ml_cloud_hyperopt.sh but sqlite might be somewhat unstabble when running multiple workers.","title":"Hyperparameter Optimization"},{"location":"optimization/optimization/hyperparameter/#hyperparameter-search","text":"The hydra based configuration management allows using multiple hydra sweeper plugins for hyperparameter optimization. The optuna sweeper is installed by default. Sweeper plugins support specifying the hyperparameter search space on the commandline. For example the following command optimizes the learning rate and weight decay parameters for the default optimizer ( adamw ): hannah-train scheduler.max_lr='interval(0.00001, 0.001)' optimizer.weight_decay='interval(0.00001, 0.001)' hydra/sweeper=optuna hydra/launcher=joblib -m Parametrization can be given on the commandline as well as using configuration files for a detailed documentation have a look at basic override syntax and extended override syntax .","title":"Hyperparameter Search"},{"location":"optimization/optimization/hyperparameter/#optuna-options","text":"The optuna hyperparameter optimizer has the following options. hannah-train hydra/sweeper = optuna --cfg hydra -p hydra.sweeper # @package hydra.sweeper sampler: _target_: optuna.samplers.TPESampler seed: null consider_prior: true prior_weight: 1 .0 consider_magic_clip: true consider_endpoints: false n_startup_trials: 10 n_ei_candidates: 24 multivariate: false warn_independent_sampling: true _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper direction: minimize storage: null study_name: null n_trials: 20 n_jobs: 2 search_space: {} Try especially increasing the number of trials to run n_trials , and if the sweeper options are dependent on each other try enabling multivariate sampling e.g. sampler.multivariate .","title":"Optuna Options"},{"location":"optimization/optimization/hyperparameter/#state-persistence","text":"If you want to save the optimization results for later analyis you can save them to a relational or redis database by setting the study_name and storage options. e.g.: hannah-train experiment_id=\"test_optuna_resume\" hydra/sweeper=optuna hydra.sweeper.study_name=' ${ experiment_id } ' hydra.sweeper.storage='sqlite:/// ${ experiment_id } .sqlite' trainer.max_epochs=15 scheduler.max_lr='interval(0.0001, 0.1)' module.num_workers=4 -m The tuning results can then be visualized using the tools from optuna.visualize : A simple example would be: import optuna storage = \"sqlite:///test_optuna_resume.sqlite\" study_name = \"test_optuna_resume\" def main (): study = optuna . load_study ( study_name , storage ) fig = optuna . visualization . plot_optimization_history ( study ) fig . write_image ( \"history.png\" ) fig . write_image ( \"history.pdf\" ) if __name__ == '__main__' : main () For a slightly extended visualization script have a look at scripts/optuna_visualize_results.py or use optuna-dashboard for a web based visualization.","title":"State Persistence"},{"location":"optimization/optimization/hyperparameter/#resuming-trials","text":"An additional benefit of exporting the study state to database is that trials can be somewhat resumed at a later time by just restarting the trial with the same parameters. Although this has a view problems: - The hydra sweeper will start it's job numbering from scratch","title":"Resuming Trials"},{"location":"optimization/optimization/hyperparameter/#running-on-ml-cloud","text":"It is possible to use the sqlite backend to store trials on ml-cloud for an example look at scripts/ml_cloud_hyperopt.sh but sqlite might be somewhat unstabble when running multiple workers.","title":"Running on ml-cloud"},{"location":"specific-applications/applications/seizure_detection/","text":"Dataset creation The current used dataset was generated with the 16 channels with the highest variance within the ictal data from the CHB-MIT Scalp EEG Database [1] . If a new preprocessed dataset should be created, the scripts/eeg/eeg_dataset_creator.py file can be used. This takes the edf files from the CHB-MIT dataset as an input and performs a basic preprocessing including a band-pass filtering (0.1 Hz and 50 Hz) to remove DC components as well as the noisy component from the EEG measurement device. It finally creates binary labels for each data fragment. Parameters that can be specified are amongst others: class_ratio default = 5, Ratio of zeros:ones in the final dev and retrain datasets to account for the scarcity of the ictal data data_length default = 1, Number of seconds per data point. For a sample rate of 256, the final data will be of the shape (D,C,256xdata_length) samp_rate default = 256, Sampling rate for the dataset. The data is already sampled at 256 Hz, use this argument only when the rate needed differs from this The dataset creation can be invoked by: python eeg_dataset_creator.py --output_dir \"./data/\" --class_ratio 4 --data_length 0.5 Currently, the 16c_retrain_id preprocessed dataset is configured as the default dataset in HANNAH if someone works with the CHB-MIT dataset. This was generated with a class_ratio of 4, a data_length of 0.5 and a sampling_rate of 256. [1] Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215\u2013e220. Training a base model with the CHB-MIT dataset The training with the preprocessed CHB-MIT dataset can be invoked with: hannah-train dataset=chbmit features=identity ~normalizer The data is normalized directly when loaded and does not need additional normalization during training. ~normalizer is used to turn additional normalization off. Adding +dataset.weighted_loss=True improves the results notably (same applies for the retraining). The trained model can function as a base model which can be fine-tuned for each patient by subsequent retraining. Retraining The main idea of retraining is to account for individual differences within the data. To perform subsequent retraining, for each patient a new model needs to be trained based on the checkpoint of the best model trained on all patients. The prior trained base model is loaded and retrained on patient-specific data. To invoke this training the dataset chbmitrt needs to be used as this specifically loads patient-specific data only. For a single patient, the retraining can be invoked by: hannah-train dataset=chbmitrt trainer.max_epochs=10 model=tc-res8 module.batch_size=8 input_file=/PATH/TO/BASE_MODEL/best.ckpt ~normalizer Alternatively, if the retraining should be performed for all patients, scripts/eeg/run_patient_retraining_complete.py can be used. To execute this script, one can use python run_patient_retraining_complete.py 'model-name' 'dataset_name' A model which has been successfully used for this application is for example the TC-ResNet8 tc-res8 and it is recommended to use 16c_retrain_id as a dataset name, which names the preprocessd CHB-MIT dataset with balanced class samples. During the retraining, for each patient a results folder is generated.","title":"Seizure Detection"},{"location":"specific-applications/applications/seizure_detection/#dataset-creation","text":"The current used dataset was generated with the 16 channels with the highest variance within the ictal data from the CHB-MIT Scalp EEG Database [1] . If a new preprocessed dataset should be created, the scripts/eeg/eeg_dataset_creator.py file can be used. This takes the edf files from the CHB-MIT dataset as an input and performs a basic preprocessing including a band-pass filtering (0.1 Hz and 50 Hz) to remove DC components as well as the noisy component from the EEG measurement device. It finally creates binary labels for each data fragment. Parameters that can be specified are amongst others: class_ratio default = 5, Ratio of zeros:ones in the final dev and retrain datasets to account for the scarcity of the ictal data data_length default = 1, Number of seconds per data point. For a sample rate of 256, the final data will be of the shape (D,C,256xdata_length) samp_rate default = 256, Sampling rate for the dataset. The data is already sampled at 256 Hz, use this argument only when the rate needed differs from this The dataset creation can be invoked by: python eeg_dataset_creator.py --output_dir \"./data/\" --class_ratio 4 --data_length 0.5 Currently, the 16c_retrain_id preprocessed dataset is configured as the default dataset in HANNAH if someone works with the CHB-MIT dataset. This was generated with a class_ratio of 4, a data_length of 0.5 and a sampling_rate of 256. [1] Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215\u2013e220.","title":"Dataset creation"},{"location":"specific-applications/applications/seizure_detection/#training-a-base-model-with-the-chb-mit-dataset","text":"The training with the preprocessed CHB-MIT dataset can be invoked with: hannah-train dataset=chbmit features=identity ~normalizer The data is normalized directly when loaded and does not need additional normalization during training. ~normalizer is used to turn additional normalization off. Adding +dataset.weighted_loss=True improves the results notably (same applies for the retraining). The trained model can function as a base model which can be fine-tuned for each patient by subsequent retraining.","title":"Training a base model with the CHB-MIT dataset"},{"location":"specific-applications/applications/seizure_detection/#retraining","text":"The main idea of retraining is to account for individual differences within the data. To perform subsequent retraining, for each patient a new model needs to be trained based on the checkpoint of the best model trained on all patients. The prior trained base model is loaded and retrained on patient-specific data. To invoke this training the dataset chbmitrt needs to be used as this specifically loads patient-specific data only. For a single patient, the retraining can be invoked by: hannah-train dataset=chbmitrt trainer.max_epochs=10 model=tc-res8 module.batch_size=8 input_file=/PATH/TO/BASE_MODEL/best.ckpt ~normalizer Alternatively, if the retraining should be performed for all patients, scripts/eeg/run_patient_retraining_complete.py can be used. To execute this script, one can use python run_patient_retraining_complete.py 'model-name' 'dataset_name' A model which has been successfully used for this application is for example the TC-ResNet8 tc-res8 and it is recommended to use 16c_retrain_id as a dataset name, which names the preprocessd CHB-MIT dataset with balanced class samples. During the retraining, for each patient a results folder is generated.","title":"Retraining"}]}